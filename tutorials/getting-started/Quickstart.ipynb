{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0408d8b8-3aef-491e-9a17-babc6c6d9c33",
   "metadata": {},
   "source": [
    "# Getting Started with BRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497d4d2-95c9-4f2d-8521-bc8ff6d7ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This documentaiton is both a jupyter notebook and part of the read the docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df09a5f-6bae-41fd-869f-8457e3c5bd38",
   "metadata": {},
   "source": [
    "# Instillation\n",
    "\n",
    "**Here we will explain how to install BRAD** Once we have 100% working version, we will upload the package to PyPI (similar to what we did with HAT) so that it is available in a standard format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701172af-b30e-47e5-a34f-0102ca5084aa",
   "metadata": {},
   "source": [
    "## Install with PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c097c0-9ca5-4800-b17a-c88e2883175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be filled in once we upload to PyPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798084c-9f68-4914-ac37-b33129d1db56",
   "metadata": {},
   "source": [
    "## Local and Development Instillation\n",
    "\n",
    "To install the local version, you can download the code by cloning the [github repository here](https://github.com/Jpickard1/RAG-DEV/tree/main). In the main directory of the repository, you can install the codes locally using the command `pip install -e .` Then, changes you make locally will be available as you chat with BRAD. - Do this @ Marc and Nat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abdfa00-de91-43d9-9289-cf6d4723bed7",
   "metadata": {},
   "source": [
    "# Chatting with BRAD\n",
    "\n",
    "BRAD provides an interface between your data, online bioinformatics databases, a large language model (LLM), and many other tools. Chatting with BRAD allows an LLM of your choice to access to all of these different tools. By default, BRAD currently LLMs hosted by NVIDIA, but BRAD is configurable to use LLMs downloaded locally to your own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05634d5-f7ad-42c6-bd57-ed6cf8f4b0d2",
   "metadata": {},
   "source": [
    "## Choosing your Large Language Model (LLM)\n",
    "\n",
    "Here we show how to specify which LLM to run. Feel free to skip this section if you will use the default model.\n",
    "\n",
    "The llms module of BRAD provides access to the different places we can select LLMs from. Each LLM can have its own requirements and interface for loading, so BRAD provides a simple `load_XXX` interface to load either local or NVIDIA hosted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0f6892-aa51-4d98-aa12-839b2d8cd561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/oliven/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/oliven/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/oliven/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from BRAD import llms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbde14-21b6-4ba9-942d-3ae05624d730",
   "metadata": {},
   "source": [
    "### Local LLMs\n",
    "\n",
    "To run a LLM downloaded to your computer, we use the `load_llama` function. The [Large Language Model Meta AI (llama)](https://arxiv.org/pdf/2307.09288) is an open source LLM that can be downloaded here:\n",
    "* [llama2](https://huggingface.co/docs/transformers/en/model_doc/llama2)\n",
    "* [llama3](https://huggingface.co/docs/transformers/en/model_doc/llama3)\n",
    "\n",
    "When running local llms, you should be mindful of the recommended compute requirements for each model and the available RAM and GPUs on your machine. The `load_llama` function loads a pre-trained Llama model from a given file path, configures it with a specified context length, and maximum number of tokens to generate per query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa667c1-d555-4928-b234-883809a67e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llms.load_llama(\n",
    "    model_path = '<PATH-TO-YOUR-LLAMA.gguf MODEL>', # specify where you downloaded llama\n",
    "    n_ctx      = 4096,                              # specify the maximum context for your model\n",
    "    max_tokens = 1000,                              # the maximum number of tokens (similar to words) your model can generate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fca6611-ec64-4b3d-990d-4cac92646367",
   "metadata": {},
   "source": [
    "### NVIDIA LLMs\n",
    "\n",
    "To run a LLM hosted by NVIDIA, we use the `load_nvidia` function. This method gives access to any of the LLMs that are available here [NVIDIA's Website](https://build.nvidia.com/explore/discover), which includes llama 3 and many more. Using these hosted models can increase the ease of use and reduce the time it takes for BRAD to respond to each query. NVIDIA provides free usage for at least the first 1000 queries to BRAD.\n",
    "\n",
    "The `load_nvidia` function loads a pre-trained NVIDIA model using the provided model name and API key. If the API key is not supplied as an argument, the function will prompt the user to input it securely. See [NVIDIA's documentation](https://build.nvidia.com/explore/discover) to find your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663c10a8-85e0-4c73-9a7c-19bf01854769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your NVIDIA API key:  ······································································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliven/.local/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/_statics.py:319: UserWarning: Model mistral_7b is deprecated. Using mistralai/mistral-7b-instruct-v0.2 instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = llms.load_nvidia()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1f53b-e3a1-48a5-bdb7-babaa0ac2bcf",
   "metadata": {},
   "source": [
    "## Running BRAD\n",
    "\n",
    "With our model selected, run the following commands to chat with BRAD. If you didn't use the above section to select a particular LLM and want the default option, run the command `brad.chat()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42424e60-5900-4998-9283-84be3745ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BRAD import brad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c364aea9-3bff-4119-91f2-c102e6700e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to RAG! The chat log from this conversation will be saved to ~/BRAD/2024-06-05-12:37:56.821301.json. How can I help?\n",
      "\n",
      "Would you like to use a database with BRAD [Y/N]?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Jun  5 12:37:59 2024 INFO Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "/home/oliven/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Wed Jun  5 12:38:00 2024 INFO Use pytorch device_name: cpu\n",
      "Wed Jun  5 12:38:00 2024 INFO Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Wed Jun  5 12:38:00 2024 INFO Collection DB_cosine_cSize_700_cOver_200 is not created.\n",
      "\u001b[32m2024-06-05 12:38:03 INFO semantic_router.utils.logger local\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input >>  tell me about myod\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RAG >> 1: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliven/.local/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Myod is a transcription factor that plays a central role in the differentiation of skeletal muscles. It forms heterodimers with E-protein sub-family of bHLH proteins and binds directly to the regulatory elements of genes expressed in skeletal muscle using its basic regions as sequence-specific DNA-binding domains. Myod expression is initiated by signaling from surrounding cells and the combined deletion of Myf5 and Pax3 leads to its absence. Myod regulates the activity of its own expression through a feed-forward mechanism. The regulation of Myod expression and its role in muscle specification is complex and not fully understood.\n",
      "['', 'MYOD', 'MYOD', 'MYF5', 'PAX3', 'MYOD', 'MYOD']\n",
      "\n",
      " would you search Gene Ontology for these terms [Y/N]?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gonto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_498459/2128865677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/RAG-DEV/BRAD/brad.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(model_path, persist_directory, llm, ragvectordb, embeddings_model)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# Query database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mchatstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchatstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m# Log and reset these values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RAG-DEV/BRAD/rag.py\u001b[0m in \u001b[0;36mqueryDocs\u001b[0;34m(chatstatus)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# update and return the chatstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;31m# chatstatus['output'], chatstatus['process'] = res['output_text'], res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mchatstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneOntology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchatstatus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchatstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchatstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RAG-DEV/BRAD/gene_ontology.py\u001b[0m in \u001b[0;36mgeneOntology\u001b[0;34m(goQuery, chatstatus)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mchatstatus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'process'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'search'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgo\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgo\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Y'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mgo_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgonto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgoSearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mchatstatus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'process'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GO'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgo_process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mchatstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gonto' is not defined"
     ]
    }
   ],
   "source": [
    "brad.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16554c0-e33e-4d05-a08f-c20aa601fb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de8f265c-aa61-4c30-a3fc-aba98ed6ecc6",
   "metadata": {},
   "source": [
    "# Anatomy of BRAD\n",
    "- help, force, set, configs, logs, modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bcbb43-502f-4306-98d1-849bcb729229",
   "metadata": {},
   "source": [
    "## Helpful Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139759b0-2cdb-4b67-aec9-2e5bc568a588",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
