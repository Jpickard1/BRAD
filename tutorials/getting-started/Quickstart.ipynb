{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0408d8b8-3aef-491e-9a17-babc6c6d9c33",
   "metadata": {},
   "source": [
    "# Getting Started with BRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497d4d2-95c9-4f2d-8521-bc8ff6d7ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This documentaiton is both a jupyter notebook and part of the read the docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df09a5f-6bae-41fd-869f-8457e3c5bd38",
   "metadata": {},
   "source": [
    "# Instillation\n",
    "\n",
    "**Here we will explain how to install BRAD** Once we have 100% working version, we will upload the package to PyPI (similar to what we did with HAT) so that it is available in a standard format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701172af-b30e-47e5-a34f-0102ca5084aa",
   "metadata": {},
   "source": [
    "## Install with PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c097c0-9ca5-4800-b17a-c88e2883175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be filled in once we upload to PyPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d798084c-9f68-4914-ac37-b33129d1db56",
   "metadata": {},
   "source": [
    "## Local and Development Instillation\n",
    "\n",
    "To install the local version, you can download the code by cloning the [github repository here](https://github.com/Jpickard1/RAG-DEV/tree/main). In the main directory of the repository, you can install the codes locally using the command `pip install -e .` Then, changes you make locally will be available as you chat with BRAD. - Do this @ Marc and Nat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abdfa00-de91-43d9-9289-cf6d4723bed7",
   "metadata": {},
   "source": [
    "# Chatting with BRAD\n",
    "\n",
    "BRAD provides an interface between your data, online bioinformatics databases, a large language model (LLM), and many other tools. Chatting with BRAD allows an LLM of your choice to access to all of these different tools. By default, BRAD currently LLMs hosted by NVIDIA, but BRAD is configurable to use LLMs downloaded locally to your own computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05634d5-f7ad-42c6-bd57-ed6cf8f4b0d2",
   "metadata": {},
   "source": [
    "## Choosing your Large Language Model (LLM)\n",
    "\n",
    "Here we show how to specify which LLM to run. Feel free to skip this section if you will use the default model.\n",
    "\n",
    "The llms module of BRAD provides access to the different places we can select LLMs from. Each LLM can have its own requirements and interface for loading, so BRAD provides a simple `load_XXX` interface to load either local or NVIDIA hosted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e0f6892-aa51-4d98-aa12-839b2d8cd561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BRAD import llms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbde14-21b6-4ba9-942d-3ae05624d730",
   "metadata": {},
   "source": [
    "### Local LLMs\n",
    "\n",
    "To run a LLM downloaded to your computer, we use the `load_llama` function. The [Large Language Model Meta AI (llama)](https://arxiv.org/pdf/2307.09288) is an open source LLM that can be downloaded here:\n",
    "* [llama2](https://huggingface.co/docs/transformers/en/model_doc/llama2)\n",
    "* [llama3](https://huggingface.co/docs/transformers/en/model_doc/llama3)\n",
    "\n",
    "When running local llms, you should be mindful of the recommended compute requirements for each model and the available RAM and GPUs on your machine. The `load_llama` function loads a pre-trained Llama model from a given file path, configures it with a specified context length, and maximum number of tokens to generate per query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa667c1-d555-4928-b234-883809a67e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llms.load_llama(\n",
    "    model_path = '<PATH-TO-YOUR-LLAMA.gguf MODEL>', # specify where you downloaded llama\n",
    "    n_ctx      = 4096,                              # specify the maximum context for your model\n",
    "    max_tokens = 1000,                              # the maximum number of tokens (similar to words) your model can generate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fca6611-ec64-4b3d-990d-4cac92646367",
   "metadata": {},
   "source": [
    "### NVIDIA LLMs\n",
    "\n",
    "To run a LLM hosted by NVIDIA, we use the `load_nvidia` function. This method gives access to any of the LLMs that are available here [NVIDIA's Website](https://build.nvidia.com/explore/discover), which includes llama 3 and many more. Using these hosted models can increase the ease of use and reduce the time it takes for BRAD to respond to each query. NVIDIA provides free usage for at least the first 1000 queries to BRAD.\n",
    "\n",
    "The `load_nvidia` function loads a pre-trained NVIDIA model using the provided model name and API key. If the API key is not supplied as an argument, the function will prompt the user to input it securely. See [NVIDIA's documentation](https://build.nvidia.com/explore/discover) to find your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "663c10a8-85e0-4c73-9a7c-19bf01854769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your NVIDIA API key:  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpic/.local/lib/python3.11/site-packages/langchain_nvidia_ai_endpoints/_statics.py:313: UserWarning: Model mistral_7b is deprecated. Using mistralai/mistral-7b-instruct-v0.2 instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = llms.load_nvidia()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1f53b-e3a1-48a5-bdb7-babaa0ac2bcf",
   "metadata": {},
   "source": [
    "## Running BRAD\n",
    "\n",
    "With our model selected, run the following commands to chat with BRAD. If you didn't use the above section to select a particular LLM and want the default option, run the command `brad.chat()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42424e60-5900-4998-9283-84be3745ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BRAD import brad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c364aea9-3bff-4119-91f2-c102e6700e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to RAG! The chat log from this conversation will be saved to /home/jpic/BRAD/2024-06-13-13:04:49.731669.json. How can I help?\n",
      "\n",
      "Would you like to use a database with BRAD [Y/N]?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thu Jun 13 13:05:00 2024 INFO local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input >>  who are you and what can you do for me?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Thu Jun 13 13:05:11 2024 INFO RAG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RAG >> 1: \n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mCurrent conversation: \n",
      "\n",
      "\n",
      "New Input: \n",
      "Context: You are BRAD (Bioinformatic Retrieval Augmented Data), a chatbot specializing in biology,\n",
      "bioinformatics, genetics, and data science. You can be connected to a text database to augment your answers\n",
      "based on the literature with Retrieval Augmented Generation, or you can use several additional modules including\n",
      "searching the web for new articles, searching Gene Ontology or Enrichr bioinformatics databases, running snakemake\n",
      "and matlab pipelines, or analyzing your own codes. Please answer the following questions to the best of your\n",
      "ability.\n",
      "\n",
      "Prompt: who are you and what can you do for me?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " I am BRAD, a chatbot specialized in biology, bioinformatics, genetics, and data science. I can augment my answers by connecting to a text database using Retrieval Augmented Generation (RAG). Additionally, I can search the web for new articles, access Gene Ontology or Enrichr bioinformatics databases, run Snakemake and MATLAB pipelines, or analyze my own codes to help answer your questions to the best of my ability.\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input >>  bye\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for chatting today! I hope to talk soon, and don't forget that a record of this conversation is available at: /home/jpic/BRAD/2024-06-13-13:04:49.731669.json\n"
     ]
    }
   ],
   "source": [
    "brad.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16554c0-e33e-4d05-a08f-c20aa601fb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de8f265c-aa61-4c30-a3fc-aba98ed6ecc6",
   "metadata": {},
   "source": [
    "# Anatomy of BRAD\n",
    "- help, force, set, configs, logs, modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bcbb43-502f-4306-98d1-849bcb729229",
   "metadata": {},
   "source": [
    "## Helpful Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139759b0-2cdb-4b67-aec9-2e5bc568a588",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
