{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cd77758-f458-43e0-bae1-68b33efa3c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machoi/.local/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/machoi/.local/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import subprocess\n",
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from semantic_router.layer import RouteLayer\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "#BERTscore\n",
    "import bert_score\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "from bert_score import BERTScorer\n",
    "from BRAD import brad\n",
    "from BRAD import rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a07c3-15a6-446d-84cf-5d0143f90466",
   "metadata": {},
   "source": [
    "## Single Document Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3516f15-d53c-448c-b9c7-6171fac70589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92829391-d5c6-4c49-8adc-d312e4d01254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tue Jun 18 01:27:06 2024 INFO Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "/home/machoi/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Tue Jun 18 01:27:08 2024 INFO Use pytorch device_name: cpu\n",
      "Tue Jun 18 01:27:08 2024 INFO Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Tue Jun 18 01:27:09 2024 INFO Collection DB_cosine_cSize_700_cOver_200 is not created.\n"
     ]
    }
   ],
   "source": [
    "path = '/nfs/turbo/umms-indikar/shared/projects/RAG/databases/EXP2/'\n",
    "vectordb, embeddings_model = brad.load_literature_db(persist_directory='/nfs/turbo/umms-indikar/shared/projects/RAG/databases/EXP2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93337a8e-4af8-403a-93f5-bbbcab2201f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Chroma in module langchain_chroma.vectorstores object:\n",
      "\n",
      "class Chroma(langchain_core.vectorstores.VectorStore)\n",
      " |  Chroma(collection_name: 'str' = 'langchain', embedding_function: 'Optional[Embeddings]' = None, persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, collection_metadata: 'Optional[Dict]' = None, client: 'Optional[chromadb.ClientAPI]' = None, relevance_score_fn: 'Optional[Callable[[float], float]]' = None, create_collection_if_not_exists: 'Optional[bool]' = True) -> 'None'\n",
      " |  \n",
      " |  `ChromaDB` vector store.\n",
      " |  \n",
      " |  To use, you should have the ``chromadb`` python package installed.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |              from langchain_chroma import Chroma\n",
      " |              from langchain_openai import OpenAIEmbeddings\n",
      " |  \n",
      " |              embeddings = OpenAIEmbeddings()\n",
      " |              vectorstore = Chroma(\"langchain_store\", embeddings)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Chroma\n",
      " |      langchain_core.vectorstores.VectorStore\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, collection_name: 'str' = 'langchain', embedding_function: 'Optional[Embeddings]' = None, persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, collection_metadata: 'Optional[Dict]' = None, client: 'Optional[chromadb.ClientAPI]' = None, relevance_score_fn: 'Optional[Callable[[float], float]]' = None, create_collection_if_not_exists: 'Optional[bool]' = True) -> 'None'\n",
      " |      Initialize with a Chroma client.\n",
      " |  \n",
      " |  add_images(self, uris: 'List[str]', metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more images through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          uris List[str]: File path to the image.\n",
      " |          metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n",
      " |          ids (Optional[List[str]], optional): Optional list of IDs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added images.\n",
      " |  \n",
      " |  add_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more texts through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts (Iterable[str]): Texts to add to the vectorstore.\n",
      " |          metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n",
      " |          ids (Optional[List[str]], optional): Optional list of IDs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  delete(self, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'None'\n",
      " |      Delete by vector IDs.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: List of ids to delete.\n",
      " |  \n",
      " |  delete_collection(self) -> 'None'\n",
      " |      Delete the collection.\n",
      " |  \n",
      " |  encode_image(self, uri: 'str') -> 'str'\n",
      " |      Get base64 string from image URI.\n",
      " |  \n",
      " |  get(self, ids: 'Optional[OneOrMany[ID]]' = None, where: 'Optional[Where]' = None, limit: 'Optional[int]' = None, offset: 'Optional[int]' = None, where_document: 'Optional[WhereDocument]' = None, include: 'Optional[List[str]]' = None) -> 'Dict[str, Any]'\n",
      " |      Gets the collection.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: The ids of the embeddings to get. Optional.\n",
      " |          where: A Where type dict used to filter results by.\n",
      " |                 E.g. `{\"color\" : \"red\", \"price\": 4.20}`. Optional.\n",
      " |          limit: The number of documents to return. Optional.\n",
      " |          offset: The offset to start returning results from.\n",
      " |                  Useful for paging results with limit. Optional.\n",
      " |          where_document: A WhereDocument type dict used to filter by the documents.\n",
      " |                          E.g. `{$contains: \"hello\"}`. Optional.\n",
      " |          include: A list of what to include in the results.\n",
      " |                   Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`.\n",
      " |                   Ids are always included.\n",
      " |                   Defaults to `[\"metadatas\", \"documents\"]`. Optional.\n",
      " |  \n",
      " |  max_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  max_marginal_relevance_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          embedding: Embedding to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  similarity_search(self, query: 'str', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Run similarity search with Chroma.\n",
      " |      \n",
      " |      Args:\n",
      " |          query (str): Query text to search for.\n",
      " |          k (int): Number of results to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[Document]: List of documents most similar to the query text.\n",
      " |  \n",
      " |  similarity_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to embedding vector.\n",
      " |      Args:\n",
      " |          embedding (List[float]): Embedding to look up documents similar to.\n",
      " |          k (int): Number of Documents to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      Returns:\n",
      " |          List of Documents most similar to the query vector.\n",
      " |  \n",
      " |  similarity_search_by_vector_with_relevance_scores(self, embedding: 'List[float]', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs most similar to embedding vector and similarity score.\n",
      " |      \n",
      " |      Args:\n",
      " |          embedding (List[float]): Embedding to look up documents similar to.\n",
      " |          k (int): Number of Documents to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[Tuple[Document, float]]: List of documents most similar to\n",
      " |          the query text and cosine distance in float for each.\n",
      " |          Lower score represents more similarity.\n",
      " |  \n",
      " |  similarity_search_with_score(self, query: 'str', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Run similarity search with Chroma with distance.\n",
      " |      \n",
      " |      Args:\n",
      " |          query (str): Query text to search for.\n",
      " |          k (int): Number of results to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[Tuple[Document, float]]: List of documents most similar to\n",
      " |          the query text and cosine distance in float for each.\n",
      " |          Lower score represents more similarity.\n",
      " |  \n",
      " |  update_document(self, document_id: 'str', document: 'Document') -> 'None'\n",
      " |      Update a document in the collection.\n",
      " |      \n",
      " |      Args:\n",
      " |          document_id (str): ID of the document to update.\n",
      " |          document (Document): Document to update.\n",
      " |  \n",
      " |  update_documents(self, ids: 'List[str]', documents: 'List[Document]') -> 'None'\n",
      " |      Update a document in the collection.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (List[str]): List of ids of the document to update.\n",
      " |          documents (List[Document]): List of documents to update.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_documents(documents: 'List[Document]', embedding: 'Optional[Embeddings]' = None, ids: 'Optional[List[str]]' = None, collection_name: 'str' = 'langchain', persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, client: 'Optional[chromadb.ClientAPI]' = None, collection_metadata: 'Optional[Dict]' = None, **kwargs: 'Any') -> 'Chroma' from abc.ABCMeta\n",
      " |      Create a Chroma vectorstore from a list of documents.\n",
      " |      \n",
      " |      If a persist_directory is specified, the collection will be persisted there.\n",
      " |      Otherwise, the data will be ephemeral in-memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          collection_name (str): Name of the collection to create.\n",
      " |          persist_directory (Optional[str]): Directory to persist the collection.\n",
      " |          ids (Optional[List[str]]): List of document IDs. Defaults to None.\n",
      " |          documents (List[Document]): List of documents to add to the vectorstore.\n",
      " |          embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n",
      " |          client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n",
      " |          collection_metadata (Optional[Dict]): Collection configurations.\n",
      " |                                                Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Chroma: Chroma vectorstore.\n",
      " |  \n",
      " |  from_texts(texts: 'List[str]', embedding: 'Optional[Embeddings]' = None, metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, collection_name: 'str' = 'langchain', persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, client: 'Optional[chromadb.ClientAPI]' = None, collection_metadata: 'Optional[Dict]' = None, **kwargs: 'Any') -> 'Chroma' from abc.ABCMeta\n",
      " |      Create a Chroma vectorstore from a raw documents.\n",
      " |      \n",
      " |      If a persist_directory is specified, the collection will be persisted there.\n",
      " |      Otherwise, the data will be ephemeral in-memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts (List[str]): List of texts to add to the collection.\n",
      " |          collection_name (str): Name of the collection to create.\n",
      " |          persist_directory (Optional[str]): Directory to persist the collection.\n",
      " |          embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n",
      " |          metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n",
      " |          ids (Optional[List[str]]): List of document IDs. Defaults to None.\n",
      " |          client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n",
      " |          collection_metadata (Optional[Dict]): Collection configurations.\n",
      " |                                                Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Chroma: Chroma vectorstore.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  embeddings\n",
      " |      Access the query embedding object if available.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  async aadd_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more documents through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (List[Document]: Documents to add to the vectorstore.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  async aadd_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[List[dict]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more texts through the embeddings and add to the vectorstore.\n",
      " |  \n",
      " |  add_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more documents through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (List[Document]: Documents to add to the vectorstore.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  async adelete(self, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Optional[bool]'\n",
      " |      Delete by vector ID or other criteria.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: List of ids to delete.\n",
      " |          **kwargs: Other keyword arguments that subclasses might use.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Optional[bool]: True if deletion is successful,\n",
      " |          False otherwise, None if not implemented.\n",
      " |  \n",
      " |  async amax_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      \n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  async amax_marginal_relevance_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |  \n",
      " |  as_retriever(self, **kwargs: 'Any') -> 'VectorStoreRetriever'\n",
      " |      Return VectorStoreRetriever initialized from this VectorStore.\n",
      " |      \n",
      " |      Args:\n",
      " |          search_type (Optional[str]): Defines the type of search that\n",
      " |              the Retriever should perform.\n",
      " |              Can be \"similarity\" (default), \"mmr\", or\n",
      " |              \"similarity_score_threshold\".\n",
      " |          search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
      " |              search function. Can include things like:\n",
      " |                  k: Amount of documents to return (Default: 4)\n",
      " |                  score_threshold: Minimum relevance threshold\n",
      " |                      for similarity_score_threshold\n",
      " |                  fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
      " |                  lambda_mult: Diversity of results returned by MMR;\n",
      " |                      1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
      " |                  filter: Filter by document metadata\n",
      " |      \n",
      " |      Returns:\n",
      " |          VectorStoreRetriever: Retriever class for VectorStore.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # Retrieve more documents with higher diversity\n",
      " |          # Useful if your dataset has many similar documents\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"mmr\",\n",
      " |              search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
      " |          )\n",
      " |      \n",
      " |          # Fetch more documents for the MMR algorithm to consider\n",
      " |          # But only return the top 5\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"mmr\",\n",
      " |              search_kwargs={'k': 5, 'fetch_k': 50}\n",
      " |          )\n",
      " |      \n",
      " |          # Only retrieve documents that have a relevance score\n",
      " |          # Above a certain threshold\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"similarity_score_threshold\",\n",
      " |              search_kwargs={'score_threshold': 0.8}\n",
      " |          )\n",
      " |      \n",
      " |          # Only get the single most similar document from the dataset\n",
      " |          docsearch.as_retriever(search_kwargs={'k': 1})\n",
      " |      \n",
      " |          # Use a filter to only retrieve documents from a specific paper\n",
      " |          docsearch.as_retriever(\n",
      " |              search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
      " |          )\n",
      " |  \n",
      " |  async asearch(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query using specified search type.\n",
      " |  \n",
      " |  async asimilarity_search(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query.\n",
      " |  \n",
      " |  async asimilarity_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to embedding vector.\n",
      " |  \n",
      " |  async asimilarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs and relevance scores in the range [0, 1], asynchronously.\n",
      " |      \n",
      " |      0 is dissimilar, 1 is most similar.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: input text\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
      " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
      " |                  filter the resulting set of retrieved docs\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Tuples of (doc, similarity_score)\n",
      " |  \n",
      " |  async asimilarity_search_with_score(self, *args: 'Any', **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Run similarity search with distance asynchronously.\n",
      " |  \n",
      " |  search(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query using specified search type.\n",
      " |  \n",
      " |  similarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs and relevance scores in the range [0, 1].\n",
      " |      \n",
      " |      0 is dissimilar, 1 is most similar.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: input text\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
      " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
      " |                  filter the resulting set of retrieved docs\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Tuples of (doc, similarity_score)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  async afrom_documents(documents: 'List[Document]', embedding: 'Embeddings', **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from documents and embeddings.\n",
      " |  \n",
      " |  async afrom_texts(texts: 'List[str]', embedding: 'Embeddings', metadatas: 'Optional[List[dict]]' = None, **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from texts and embeddings.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "317ec1b9-e072-4576-b32f-14cf1119359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(vectordb.get()['ids'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935bda08-5a22-4878-9b8d-2355423842f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f64588-a3e8-43e5-91e7-2ff25d0a2e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\n",
      "['high-spatial-resolution multi-omics sequencing via deterministic barcoding in tissue', 'understanding transcriptional networks regulating initiation of cutaneous wound healing', 'transcriptional control of wound repair', 'optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming', 'a predictive computational framework for direct reprogramming between human cell types', 'pederson - 2021 - a layperson encounter, on the “modified” rna world', 'long-term association of a transcription factor with its chromatin binding site can stabilize gene expression and cell fate commitment', 'a comprehensive library of human transcription', 'in vivo cellular reprogramming the next generation', 'transcription factors orchestrate dynamic interplay between genome topology and gene regulation during cell reprogramming', 'elite and stochastic models for induced pluripotent stem cell generation', 'qiu et al. - 2022 - mapping transcriptomic vector fields of single cel', 'ismara automated modeling of genomic signals as a democracy of regulatory motifs', 'a decade of transcription factor‑mediated reprogramming to pluripotency', 'converting cell fates generating hematopoietic stem cells de novo via transcription factor reprogramming', 'nuclear reprogramming in cells', 'leveling waddington the emergence of direct programming and the loss of cell fate hierarchies', 'transcription-driven genome organization a model for chromosome structure and the regulation of gene expression tested through simulations', 'a comprehensive library of human transcription factors for cell fate engineering', 'the myod family and myogenesis redundancy, networks, and thresholds', 'ctcf making the right connections', 'conserved transcription factors promote cell fate stability', 'a modular master regulator landscape controls', 'transcription factories gene expression in unions', 'reprogramming- emerging strategies to rejuvenate aging', 'dynamic condensates activate transcription', 'activation of muscle-specific genes in pigment, nerve, fat, liver, and fibroblast cell lines by forced expression of myod', 'enabling direct fate conversion with network biology', 'eguchi et al. - 2022 - transdire data-driven direct reprogramming by a p', 'molecular interaction networks to select factors for cell conversion', 'direct cell reprogramming', 'updated perspectives on direct vascular cellular reprogramming', 'the organization of replication and transcription', 'balsalobre and drouin - 2022 - pioneer factors as master regulators of the epigen', 'direct lineage reprogramming strategies, mechanisms, and applications', 'similar active genes cluster in specialized transcription factories', 'computational methods for direct cell conversion', 'predicting three-dimensional genome structure from transcriptional activity', 'hematopoietic stem cells count', 'cell fate determination by transcription factors', 'direct cell reprogramming for tissue engineering', 'cooperative transcription factor induction', 'transcription factories genome organization and gene regulation', 'szklarczyk2023string', 'pioneer transcription factors in cell reprogramming', 'ahmad and henikoff - 2021 - the h3.3k27m oncohistone antagonizes reprogramming', 'active genes dynamically colocalize to shared sites of ongoing transcription', 'a computer-guided design tool to increase the efficiency of cellular conversions', 'direct cell-fate conversion of somatic cells toward regenerative medicine and industries', 'genome-wide myod binding', 'challenges for computational stem cell biology a discussion for the field', 'induction of pluripotent stem cells from adult human fibroblasts by defined factors', 'the circuitry of a master switch', 'the human transcription factors', 'transcription factors and 3d genome conformation in cell-fate decisions', 'a transient disruption of fibroblastic', 'responsiveness to perturbations is a hallmark of transcription factors that maintain cell identity', 'luck2020reference', 'cell identity reprogrammed', 'computational stem cell biology open questions and guiding principles']\n"
     ]
    }
   ],
   "source": [
    "source_title = []\n",
    "path = \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\"\n",
    "print(path)\n",
    "for metadata in vectordb.get()['metadatas']:\n",
    "    source_title.append(metadata['source'].removeprefix(path).removesuffix(\".pdf\"))\n",
    "no_repeat_source_title = set(source_title)\n",
    "post_process_title = [x.lower() for x in no_repeat_source_title]\n",
    "print(post_process_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f52dabae-3985-47fb-86f1-cc356e6cc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into two methods?\n",
    "def get_all_sources(prompt, path):\n",
    "    prompt = prompt.lower()\n",
    "    metadata_full = vectordb.get()['metadatas']\n",
    "    source_list = [item['source'] for item in metadata_full]   \n",
    "    real_source_list = [((item.replace(path, '')).removesuffix('.pdf')).lower() for item in source_list]\n",
    "    db = pd.DataFrame({'id' :vectordb.get()['ids'] , 'metadatas' : real_source_list})\n",
    "    filtered_df = db[db['metadatas'].apply(lambda x: x in prompt)]\n",
    "    return real_source_list, filtered_df['id'].to_list()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fb96b6-db3c-4034-b001-26ed30fb735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tue Jun 18 01:02:32 2024 INFO Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "Tue Jun 18 01:02:32 2024 INFO Use pytorch device_name: cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_match' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3646009/2729889946.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdb_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"new_DB_cosine_cSize_%d_cOver_%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtitle_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_id_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbest_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtitle_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_id_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectordb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_id_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'documents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_match' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = 'Summarize the human transcription factors paper'\n",
    "path = \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\"\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n",
    "db_name = \"new_DB_cosine_cSize_%d_cOver_%d\" % (700, 200)\n",
    "title_list, real_id_list = get_all_sources(prompt, path)\n",
    "best_title, best_score = best_match(prompt, title_list)\n",
    "title_list, real_id_list = get_all_sources(best_title, path)\n",
    "text = vectordb.get(ids=real_id_list)['documents']\n",
    "print(len(text))\n",
    "#newdb = Chroma(persist_directory='/nfs/turbo/umms-indikar/shared/projects/RAG/databases/new_EXP3/', embedding_function=embeddings_model, collection_name=db_name)\n",
    "#print(len(newdb.get()['documents']))\n",
    "#newdb.add_texts(text)\n",
    "#print(len(newdb.get()['documents']))\n",
    "\n",
    "#things to do - hook this up with RAG (shouldnt be too bad)\n",
    "#determine how to sift thru the prompt or go thru paper titles that are not well done\n",
    "#ask about which paper db to use on turbo\n",
    "#where to save this new db\n",
    "#sometimes there is a weird error that occurs - maybe split up lower text stuff but idk how that effects runtime\n",
    "#how to remove docs from database / make new one everytime since it keeps adding to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cada8-4087-4e9b-b522-c30b7695e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "#Given the prompt, find the title and corresponding score that is the best match\n",
    "def best_match(prompt, title_list):\n",
    "    sentence_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    unique_title_list = list(set(title_list))\n",
    "    query_embedding = sentence_model.encode(prompt)\n",
    "    passage_embedding = sentence_model.encode(unique_title_list)\n",
    "\n",
    "    save_title = \"\"\n",
    "    save_score = 0\n",
    "\n",
    "    for score, title in zip(util.cos_sim(query_embedding, passage_embedding)[0], unique_title_list):\n",
    "        if score > save_score:\n",
    "            save_score = score\n",
    "            save_title = title\n",
    "    print(f\"The best match is {save_title} with a score of {save_score}\")\n",
    "    return save_title, save_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d32cc-16b0-4dce-a271-f3d0d7e9974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27458348-b815-4917-9d68-383f37dfabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "unique_title_list = list(set(title_list))\n",
    "query_embedding = sentence_model.encode(prompt)\n",
    "passage_embedding = sentence_model.encode(unique_title_list)\n",
    "\n",
    "save_title = \"\"\n",
    "save_score = 0\n",
    "\n",
    "for score, title in zip(util.cos_sim(query_embedding, passage_embedding)[0], unique_title_list):\n",
    "    if score > save_score:\n",
    "        save_score = score\n",
    "        save_title = title\n",
    "print(f\"The best match is {save_title} with a score of {save_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935dbf8-b82c-4b89-95c6-70ff9a5e6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list, real_id_list = get_all_sources(save_title, \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\")\n",
    "text = vectordb.get(ids=real_id_list)['documents']\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6664ad69-7ab1-41e0-bbd4-149b0721dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def restrictedDB(prompt, vectordb, path):\n",
    "    #path = \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\"\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n",
    "    db_name = \"new_DB_cosine_cSize_%d_cOver_%d\" % (700, 200)\n",
    "    title_list, real_id_list = get_all_sources(prompt, path)\n",
    "    best_title, best_score = best_match(prompt, title_list)\n",
    "    title_list, real_id_list = get_all_sources(best_title, path)\n",
    "    text = vectordb.get(ids=real_id_list)['documents']\n",
    "    newdb = Chroma(persist_directory='/nfs/turbo/umms-indikar/shared/projects/RAG/databases/new_EXP3/', embedding_function=embeddings_model, collection_name=db_name)\n",
    "    newdb.add_texts(text)\n",
    "    return best_score, newdb\n",
    "\n",
    "\n",
    "#Given the prompt, find the title and corresponding score that is the best match\n",
    "def best_match(prompt, title_list):\n",
    "    sentence_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    unique_title_list = list(set(title_list))\n",
    "    query_embedding = sentence_model.encode(prompt)\n",
    "    passage_embedding = sentence_model.encode(unique_title_list)\n",
    "\n",
    "    save_title = \"\"\n",
    "    #adjust the save_score to be the threshold cutoff - set to 0.75 but maybe thats too high\n",
    "    save_score = 0.75\n",
    "\n",
    "    for score, title in zip(util.cos_sim(query_embedding, passage_embedding)[0], unique_title_list):\n",
    "        if score > save_score:\n",
    "            save_score = score\n",
    "            save_title = title\n",
    "    print(f\"The best match is {save_title} with a score of {save_score}\")\n",
    "    return save_title, save_score\n",
    "\n",
    "#Split into two methods?\n",
    "def get_all_sources(prompt, path):\n",
    "    prompt = prompt.lower()\n",
    "    metadata_full = vectordb.get()['metadatas']\n",
    "    source_list = [item['source'] for item in metadata_full]   \n",
    "    real_source_list = [((item.replace(path, '')).removesuffix('.pdf')).lower() for item in source_list]\n",
    "    db = pd.DataFrame({'id' :vectordb.get()['ids'] , 'metadatas' : real_source_list})\n",
    "    filtered_df = db[db['metadatas'].apply(lambda x: x in prompt)]\n",
    "    return real_source_list, filtered_df['id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ef5a074-377a-41fd-b22e-ff0846a4e099",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tue Jun 18 01:44:10 2024 INFO Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "/home/machoi/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Tue Jun 18 01:44:11 2024 INFO Use pytorch device_name: cpu\n",
      "Tue Jun 18 01:44:11 2024 INFO Collection DB_cosine_cSize_700_cOver_200 is not created.\n",
      "Tue Jun 18 01:44:12 2024 INFO Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "Tue Jun 18 01:44:13 2024 INFO Use pytorch device_name: cpu\n",
      "Tue Jun 18 01:44:15 2024 INFO Load pretrained SentenceTransformer: multi-qa-MiniLM-L6-cos-v1\n",
      "Tue Jun 18 01:44:16 2024 INFO Use pytorch device_name: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf65b6fff5a4fa8a4041b3491d212c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792903ab2f434e6d865ee7bca6a30c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best match is the human transcription factors with a score of 0.8029918670654297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tue Jun 18 01:44:18 2024 INFO Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Tue Jun 18 01:44:18 2024 INFO Collection new_DB_cosine_cSize_700_cOver_200 is not created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8030)\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Summarize the human transcription factors paper'\n",
    "path = \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\"\n",
    "#path = '/nfs/turbo/umms-indikar/shared/projects/RAG/databases/EXP2/'\n",
    "vectordb, embeddings_model = brad.load_literature_db(persist_directory='/nfs/turbo/umms-indikar/shared/projects/RAG/databases/EXP2/')\n",
    "best_score, text = restrictedDB(prompt, vectordb, path)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5bad5e-82f6-42ed-963f-42903cec1a1c",
   "metadata": {},
   "source": [
    "## PageRanker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59771ae9-3b83-4e8b-8465-e7a8fe157415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeats(vectordb):\n",
    "    df = pd.DataFrame({'id' :vectordb.get()['ids'] , 'documents' : vectordb.get()['documents']})\n",
    "    repeated_ids = df[df.duplicated(subset='documents', keep='last')]['id'].tolist()\n",
    "    vectordb.delete(repeated_ids)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f598f70f-3fe9-410c-8fb5-25093d074c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6aea9a41-a9bd-427b-b27b-05b2afccb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectordb.get()['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a560acc-7478-46e1-8ce6-92243d1bc920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given the prompt, find the title and corresponding score that is the best match\n",
    "def adj_matrix_builder(prompt, vectordb):\n",
    "    dimension = len(vectordb.get()['documents'])+1\n",
    "    adj_matrix = np.zeros([dimension, dimension])\n",
    "    \n",
    "    sentence_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    query_embedding = sentence_model.encode(prompt)\n",
    "    passage_embedding = sentence_model.encode(vectordb.get()['documents'])\n",
    "    pos = 1\n",
    "    for score in zip(util.cos_sim(query_embedding, passage_embedding)[0]):\n",
    "        adj_matrix[0][pos] = score[0]\n",
    "        adj_matrix[pos][0] = score[0]\n",
    "        pos += 1\n",
    "    return adj_matrix\n",
    "\n",
    "# Normalize columns of A\n",
    "def normalize_adjacency_matrix(A):\n",
    "    col_sums = A.sum(axis=0)\n",
    "    return A / col_sums[np.newaxis, :]\n",
    "\n",
    "#weighted pagerank algorithm\n",
    "def pagerank_weighted(A, alpha=0.85, tol=1e-6, max_iter=100):\n",
    "    n = A.shape[0]\n",
    "    A_normalized = normalize_adjacency_matrix(A)\n",
    "    v = np.ones(n) / n  # Initial PageRank vector\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        v_next = alpha * A_normalized.dot(v) + (1 - alpha) / n\n",
    "        if np.linalg.norm(v_next - v, 1) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    return v\n",
    "\n",
    "#reranker\n",
    "\n",
    "def pagerank_rerank(prompt, vectordb):\n",
    "    adj_matrix = adj_matrix_builder(prompt, vectordb)\n",
    "    pagerank_scores = pagerank_weighted(A = adj_matrix)\n",
    "    top_rank_scores = sorted(range(len(pagerank_scores)), key=lambda i: pagerank_scores[i], reverse=True)[1:11]\n",
    "    df = pd.DataFrame({'id' :vectordb.get()['ids'] , 'documents' : vectordb.get()['documents'], 'metadatas' : vectordb.get()['metadatas']})\n",
    "    reranked_df = df.iloc[top_rank_scores]\n",
    "    return reranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dd5fa7a4-44de-4e92-b211-9f39d1acd0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tue Jun 18 02:46:42 2024 INFO Load pretrained SentenceTransformer: multi-qa-MiniLM-L6-cos-v1\n",
      "Tue Jun 18 02:46:43 2024 INFO Use pytorch device_name: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc02064c534148749ae6b2131c4c3af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9d5808b5114c86bb24fb62417d4847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.59468655e-01 6.08485365e-05 7.72999648e-05 ... 5.36480820e-05\n",
      " 4.53528530e-05 6.73818630e-05]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'tell me about cell reprogramming'\n",
    "rank_scores = pagerank_rerank(prompt, vectordb)\n",
    "print(rank_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "021ff3be-56f7-4a25-8862-c0c9993e5941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 4, 'source': '/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/Transcription factories gene expression in unions.pdf'}\n",
      "{'page': 17, 'source': '/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/Transcription Factories Genome Organization and Gene Regulation.pdf'}\n",
      "{'page': 28, 'source': '/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/Updated Perspectives on Direct Vascular Cellular Reprogramming.pdf'}\n",
      "{'page': 25, 'source': '/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/Optimal-Transport Analysis of Single-Cell Gene Expression Identifies Developmental Trajectories in Reprogramming.pdf'}\n",
      "{'page': 17, 'source': '/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/Direct cell-fate conversion of somatic cells Toward regenerative medicine and industries.pdf'}\n",
      "{'page': 7, 'source': '/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/In Vivo Cellular Reprogramming The Next Generation.pdf'}\n",
      "{'page': 3, 'source': '/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/Hematopoietic Stem Cells Count.pdf'}\n",
      "{'page': 3, 'source': '/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/Induction of Pluripotent Stem Cells from Adult Human Fibroblasts by Defined Factors.pdf'}\n",
      "{'page': 5, 'source': '/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/Direct cell reprogramming.pdf'}\n",
      "{'page': 3, 'source': '/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/A predictive computational framework for direct reprogramming between human cell types.pdf'}\n"
     ]
    }
   ],
   "source": [
    "#first entry is always the prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "08771f10-5301-4695-a331-086ba5a30623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given the prompt, find the title and corresponding score that is the best match\n",
    "def adj_matrix_builder(prompt, vectordb):\n",
    "    dimension = len(vectordb.get()['documents'])+1\n",
    "    adj_matrix = np.zeros([dimension, dimension])\n",
    "    \n",
    "    sentence_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    query_embedding = sentence_model.encode(prompt)\n",
    "    passage_embedding = sentence_model.encode(vectordb.get()['documents'])\n",
    "    pos = 1\n",
    "    for score in zip(util.cos_sim(query_embedding, passage_embedding)[0]):\n",
    "        adj_matrix[0][pos] = score[0]\n",
    "        adj_matrix[pos][0] = score[0]\n",
    "        pos += 1\n",
    "    return adj_matrix\n",
    "\n",
    "# Normalize columns of A\n",
    "def normalize_adjacency_matrix(A):\n",
    "    col_sums = A.sum(axis=0)\n",
    "    return A / col_sums[np.newaxis, :]\n",
    "\n",
    "#weighted pagerank algorithm\n",
    "def pagerank_weighted(A, alpha=0.85, tol=1e-6, max_iter=100):\n",
    "    n = A.shape[0]\n",
    "    A_normalized = normalize_adjacency_matrix(A)\n",
    "    v = np.ones(n) / n  # Initial PageRank vector\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        v_next = alpha * A_normalized.dot(v) + (1 - alpha) / n\n",
    "        if np.linalg.norm(v_next - v, 1) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    return v\n",
    "\n",
    "#reranker\n",
    "\n",
    "def pagerank_rerank(prompt, vectordb):\n",
    "    adj_matrix = adj_matrix_builder(prompt, vectordb)\n",
    "    pagerank_scores = pagerank_weighted(A = adj_matrix)\n",
    "    top_rank_scores = sorted(range(len(pagerank_scores)), key=lambda i: pagerank_scores[i], reverse=True)[1:11]\n",
    "    df = pd.DataFrame({'id' :vectordb.get()['ids'] , 'documents' : vectordb.get()['documents'], 'metadatas' : vectordb.get()['metadatas']})\n",
    "    reranked_df = df.iloc[top_rank_scores]\n",
    "    return reranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "08a2b227-4d2c-407b-b442-06a2296909a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I THINK THIS FRAMEWORK WORKS BETTER\n",
    "\n",
    "\n",
    "#Given the prompt, find the title and corresponding score that is the best match\n",
    "def adj_matrix_builder(docs, scores):\n",
    "    dimension = len(docs)+1\n",
    "    adj_matrix = np.zeros([dimension, dimension])\n",
    "    pos = 1\n",
    "    for score in scores:\n",
    "        adj_matrix[0][pos] = score\n",
    "        adj_matrix[pos][0] = score\n",
    "        pos += 1\n",
    "    return adj_matrix\n",
    "\n",
    "# Normalize columns of A\n",
    "def normalize_adjacency_matrix(A):\n",
    "    col_sums = A.sum(axis=0)\n",
    "    return A / col_sums[np.newaxis, :]\n",
    "\n",
    "#weighted pagerank algorithm\n",
    "def pagerank_weighted(A, alpha=0.85, tol=1e-6, max_iter=100):\n",
    "    n = A.shape[0]\n",
    "    A_normalized = normalize_adjacency_matrix(A)\n",
    "    v = np.ones(n) / n  # Initial PageRank vector\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        v_next = alpha * A_normalized.dot(v) + (1 - alpha) / n\n",
    "        if np.linalg.norm(v_next - v, 1) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    return v\n",
    "\n",
    "#reranker\n",
    "\n",
    "def pagerank_rerank(docs, scores):\n",
    "    adj_matrix = adj_matrix_builder(docs, scores)\n",
    "    pagerank_scores = pagerank_weighted(A = adj_matrix)\n",
    "    top_rank_scores = sorted(range(len(pagerank_scores)), key=lambda i: pagerank_scores[i], reverse=True)[1:11]\n",
    "    reranked_docs = [docs[i] for i in top_rank_scores]\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ebb2d09-8418-4de0-9a5f-156da3521770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      id  \\\n",
      "0   0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "0   0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "0   0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "0   0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "0   0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "..                                   ...   \n",
      "0   0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "0   0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "0   0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "0   0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "0   0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "\n",
      "                                            documents  \n",
      "0   lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...  \n",
      "0   lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...  \n",
      "0   lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...  \n",
      "0   lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...  \n",
      "0   lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...  \n",
      "..                                                ...  \n",
      "0   lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...  \n",
      "0   lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...  \n",
      "0   lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...  \n",
      "0   lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...  \n",
      "0   lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...  \n",
      "\n",
      "[9054 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "docs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2458e3d-57c2-4889-9aa1-a031b51ea279",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
