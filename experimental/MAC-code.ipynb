{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cd77758-f458-43e0-bae1-68b33efa3c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import subprocess\n",
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from semantic_router.layer import RouteLayer\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#BERTscore\n",
    "import bert_score\n",
    "import logging\n",
    "import transformers\n",
    "transformers.tokenization_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.configuration_utils.logger.setLevel(logging.ERROR)\n",
    "transformers.modeling_utils.logger.setLevel(logging.ERROR)\n",
    "from bert_score import BERTScorer\n",
    "from BRAD import brad\n",
    "from BRAD import rag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720a07c3-15a6-446d-84cf-5d0143f90466",
   "metadata": {},
   "source": [
    "## Single Document Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3516f15-d53c-448c-b9c7-6171fac70589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92829391-d5c6-4c49-8adc-d312e4d01254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Jun 19 09:34:53 2024 INFO Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "/home/machoi/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Wed Jun 19 09:35:07 2024 INFO Use pytorch device_name: cpu\n",
      "Wed Jun 19 09:35:08 2024 INFO Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "Wed Jun 19 09:35:09 2024 INFO Collection DB_cosine_cSize_700_cOver_200 is not created.\n"
     ]
    }
   ],
   "source": [
    "path = '/nfs/turbo/umms-indikar/shared/projects/RAG/databases/EXP2/'\n",
    "vectordb, embeddings_model = brad.load_literature_db(persist_directory='/nfs/turbo/umms-indikar/shared/projects/RAG/databases/EXP2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93337a8e-4af8-403a-93f5-bbbcab2201f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Chroma in module langchain_chroma.vectorstores object:\n",
      "\n",
      "class Chroma(langchain_core.vectorstores.VectorStore)\n",
      " |  Chroma(collection_name: 'str' = 'langchain', embedding_function: 'Optional[Embeddings]' = None, persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, collection_metadata: 'Optional[Dict]' = None, client: 'Optional[chromadb.ClientAPI]' = None, relevance_score_fn: 'Optional[Callable[[float], float]]' = None, create_collection_if_not_exists: 'Optional[bool]' = True) -> 'None'\n",
      " |  \n",
      " |  `ChromaDB` vector store.\n",
      " |  \n",
      " |  To use, you should have the ``chromadb`` python package installed.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |              from langchain_chroma import Chroma\n",
      " |              from langchain_openai import OpenAIEmbeddings\n",
      " |  \n",
      " |              embeddings = OpenAIEmbeddings()\n",
      " |              vectorstore = Chroma(\"langchain_store\", embeddings)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Chroma\n",
      " |      langchain_core.vectorstores.VectorStore\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, collection_name: 'str' = 'langchain', embedding_function: 'Optional[Embeddings]' = None, persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, collection_metadata: 'Optional[Dict]' = None, client: 'Optional[chromadb.ClientAPI]' = None, relevance_score_fn: 'Optional[Callable[[float], float]]' = None, create_collection_if_not_exists: 'Optional[bool]' = True) -> 'None'\n",
      " |      Initialize with a Chroma client.\n",
      " |  \n",
      " |  add_images(self, uris: 'List[str]', metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more images through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          uris List[str]: File path to the image.\n",
      " |          metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n",
      " |          ids (Optional[List[str]], optional): Optional list of IDs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added images.\n",
      " |  \n",
      " |  add_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more texts through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts (Iterable[str]): Texts to add to the vectorstore.\n",
      " |          metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n",
      " |          ids (Optional[List[str]], optional): Optional list of IDs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  delete(self, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'None'\n",
      " |      Delete by vector IDs.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: List of ids to delete.\n",
      " |  \n",
      " |  delete_collection(self) -> 'None'\n",
      " |      Delete the collection.\n",
      " |  \n",
      " |  encode_image(self, uri: 'str') -> 'str'\n",
      " |      Get base64 string from image URI.\n",
      " |  \n",
      " |  get(self, ids: 'Optional[OneOrMany[ID]]' = None, where: 'Optional[Where]' = None, limit: 'Optional[int]' = None, offset: 'Optional[int]' = None, where_document: 'Optional[WhereDocument]' = None, include: 'Optional[List[str]]' = None) -> 'Dict[str, Any]'\n",
      " |      Gets the collection.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: The ids of the embeddings to get. Optional.\n",
      " |          where: A Where type dict used to filter results by.\n",
      " |                 E.g. `{\"color\" : \"red\", \"price\": 4.20}`. Optional.\n",
      " |          limit: The number of documents to return. Optional.\n",
      " |          offset: The offset to start returning results from.\n",
      " |                  Useful for paging results with limit. Optional.\n",
      " |          where_document: A WhereDocument type dict used to filter by the documents.\n",
      " |                          E.g. `{$contains: \"hello\"}`. Optional.\n",
      " |          include: A list of what to include in the results.\n",
      " |                   Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`.\n",
      " |                   Ids are always included.\n",
      " |                   Defaults to `[\"metadatas\", \"documents\"]`. Optional.\n",
      " |  \n",
      " |  max_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  max_marginal_relevance_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          embedding: Embedding to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  similarity_search(self, query: 'str', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Run similarity search with Chroma.\n",
      " |      \n",
      " |      Args:\n",
      " |          query (str): Query text to search for.\n",
      " |          k (int): Number of results to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[Document]: List of documents most similar to the query text.\n",
      " |  \n",
      " |  similarity_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to embedding vector.\n",
      " |      Args:\n",
      " |          embedding (List[float]): Embedding to look up documents similar to.\n",
      " |          k (int): Number of Documents to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      Returns:\n",
      " |          List of Documents most similar to the query vector.\n",
      " |  \n",
      " |  similarity_search_by_vector_with_relevance_scores(self, embedding: 'List[float]', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs most similar to embedding vector and similarity score.\n",
      " |      \n",
      " |      Args:\n",
      " |          embedding (List[float]): Embedding to look up documents similar to.\n",
      " |          k (int): Number of Documents to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[Tuple[Document, float]]: List of documents most similar to\n",
      " |          the query text and cosine distance in float for each.\n",
      " |          Lower score represents more similarity.\n",
      " |  \n",
      " |  similarity_search_with_score(self, query: 'str', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Run similarity search with Chroma with distance.\n",
      " |      \n",
      " |      Args:\n",
      " |          query (str): Query text to search for.\n",
      " |          k (int): Number of results to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[Tuple[Document, float]]: List of documents most similar to\n",
      " |          the query text and cosine distance in float for each.\n",
      " |          Lower score represents more similarity.\n",
      " |  \n",
      " |  update_document(self, document_id: 'str', document: 'Document') -> 'None'\n",
      " |      Update a document in the collection.\n",
      " |      \n",
      " |      Args:\n",
      " |          document_id (str): ID of the document to update.\n",
      " |          document (Document): Document to update.\n",
      " |  \n",
      " |  update_documents(self, ids: 'List[str]', documents: 'List[Document]') -> 'None'\n",
      " |      Update a document in the collection.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (List[str]): List of ids of the document to update.\n",
      " |          documents (List[Document]): List of documents to update.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_documents(documents: 'List[Document]', embedding: 'Optional[Embeddings]' = None, ids: 'Optional[List[str]]' = None, collection_name: 'str' = 'langchain', persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, client: 'Optional[chromadb.ClientAPI]' = None, collection_metadata: 'Optional[Dict]' = None, **kwargs: 'Any') -> 'Chroma' from abc.ABCMeta\n",
      " |      Create a Chroma vectorstore from a list of documents.\n",
      " |      \n",
      " |      If a persist_directory is specified, the collection will be persisted there.\n",
      " |      Otherwise, the data will be ephemeral in-memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          collection_name (str): Name of the collection to create.\n",
      " |          persist_directory (Optional[str]): Directory to persist the collection.\n",
      " |          ids (Optional[List[str]]): List of document IDs. Defaults to None.\n",
      " |          documents (List[Document]): List of documents to add to the vectorstore.\n",
      " |          embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n",
      " |          client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n",
      " |          collection_metadata (Optional[Dict]): Collection configurations.\n",
      " |                                                Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Chroma: Chroma vectorstore.\n",
      " |  \n",
      " |  from_texts(texts: 'List[str]', embedding: 'Optional[Embeddings]' = None, metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, collection_name: 'str' = 'langchain', persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, client: 'Optional[chromadb.ClientAPI]' = None, collection_metadata: 'Optional[Dict]' = None, **kwargs: 'Any') -> 'Chroma' from abc.ABCMeta\n",
      " |      Create a Chroma vectorstore from a raw documents.\n",
      " |      \n",
      " |      If a persist_directory is specified, the collection will be persisted there.\n",
      " |      Otherwise, the data will be ephemeral in-memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts (List[str]): List of texts to add to the collection.\n",
      " |          collection_name (str): Name of the collection to create.\n",
      " |          persist_directory (Optional[str]): Directory to persist the collection.\n",
      " |          embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n",
      " |          metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n",
      " |          ids (Optional[List[str]]): List of document IDs. Defaults to None.\n",
      " |          client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n",
      " |          collection_metadata (Optional[Dict]): Collection configurations.\n",
      " |                                                Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Chroma: Chroma vectorstore.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  embeddings\n",
      " |      Access the query embedding object if available.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  async aadd_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more documents through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (List[Document]: Documents to add to the vectorstore.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  async aadd_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[List[dict]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more texts through the embeddings and add to the vectorstore.\n",
      " |  \n",
      " |  add_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more documents through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (List[Document]: Documents to add to the vectorstore.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  async adelete(self, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Optional[bool]'\n",
      " |      Delete by vector ID or other criteria.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: List of ids to delete.\n",
      " |          **kwargs: Other keyword arguments that subclasses might use.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Optional[bool]: True if deletion is successful,\n",
      " |          False otherwise, None if not implemented.\n",
      " |  \n",
      " |  async amax_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      \n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  async amax_marginal_relevance_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |  \n",
      " |  as_retriever(self, **kwargs: 'Any') -> 'VectorStoreRetriever'\n",
      " |      Return VectorStoreRetriever initialized from this VectorStore.\n",
      " |      \n",
      " |      Args:\n",
      " |          search_type (Optional[str]): Defines the type of search that\n",
      " |              the Retriever should perform.\n",
      " |              Can be \"similarity\" (default), \"mmr\", or\n",
      " |              \"similarity_score_threshold\".\n",
      " |          search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
      " |              search function. Can include things like:\n",
      " |                  k: Amount of documents to return (Default: 4)\n",
      " |                  score_threshold: Minimum relevance threshold\n",
      " |                      for similarity_score_threshold\n",
      " |                  fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
      " |                  lambda_mult: Diversity of results returned by MMR;\n",
      " |                      1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
      " |                  filter: Filter by document metadata\n",
      " |      \n",
      " |      Returns:\n",
      " |          VectorStoreRetriever: Retriever class for VectorStore.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # Retrieve more documents with higher diversity\n",
      " |          # Useful if your dataset has many similar documents\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"mmr\",\n",
      " |              search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
      " |          )\n",
      " |      \n",
      " |          # Fetch more documents for the MMR algorithm to consider\n",
      " |          # But only return the top 5\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"mmr\",\n",
      " |              search_kwargs={'k': 5, 'fetch_k': 50}\n",
      " |          )\n",
      " |      \n",
      " |          # Only retrieve documents that have a relevance score\n",
      " |          # Above a certain threshold\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"similarity_score_threshold\",\n",
      " |              search_kwargs={'score_threshold': 0.8}\n",
      " |          )\n",
      " |      \n",
      " |          # Only get the single most similar document from the dataset\n",
      " |          docsearch.as_retriever(search_kwargs={'k': 1})\n",
      " |      \n",
      " |          # Use a filter to only retrieve documents from a specific paper\n",
      " |          docsearch.as_retriever(\n",
      " |              search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
      " |          )\n",
      " |  \n",
      " |  async asearch(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query using specified search type.\n",
      " |  \n",
      " |  async asimilarity_search(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query.\n",
      " |  \n",
      " |  async asimilarity_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to embedding vector.\n",
      " |  \n",
      " |  async asimilarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs and relevance scores in the range [0, 1], asynchronously.\n",
      " |      \n",
      " |      0 is dissimilar, 1 is most similar.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: input text\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
      " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
      " |                  filter the resulting set of retrieved docs\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Tuples of (doc, similarity_score)\n",
      " |  \n",
      " |  async asimilarity_search_with_score(self, *args: 'Any', **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Run similarity search with distance asynchronously.\n",
      " |  \n",
      " |  search(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query using specified search type.\n",
      " |  \n",
      " |  similarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs and relevance scores in the range [0, 1].\n",
      " |      \n",
      " |      0 is dissimilar, 1 is most similar.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: input text\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
      " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
      " |                  filter the resulting set of retrieved docs\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Tuples of (doc, similarity_score)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  async afrom_documents(documents: 'List[Document]', embedding: 'Embeddings', **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from documents and embeddings.\n",
      " |  \n",
      " |  async afrom_texts(texts: 'List[str]', embedding: 'Embeddings', metadatas: 'Optional[List[dict]]' = None, **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from texts and embeddings.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "317ec1b9-e072-4576-b32f-14cf1119359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = list(vectordb.get()['ids'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935bda08-5a22-4878-9b8d-2355423842f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47f64588-a3e8-43e5-91e7-2ff25d0a2e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\n",
      "['a transient disruption of fibroblastic', 'szklarczyk2023string', 'in vivo cellular reprogramming the next generation', 'direct cell reprogramming', 'ismara automated modeling of genomic signals as a democracy of regulatory motifs', 'similar active genes cluster in specialized transcription factories', 'cooperative transcription factor induction', 'updated perspectives on direct vascular cellular reprogramming', 'elite and stochastic models for induced pluripotent stem cell generation', 'transcription factors orchestrate dynamic interplay between genome topology and gene regulation during cell reprogramming', 'the circuitry of a master switch', 'a comprehensive library of human transcription factors for cell fate engineering', 'qiu et al. - 2022 - mapping transcriptomic vector fields of single cel', 'ahmad and henikoff - 2021 - the h3.3k27m oncohistone antagonizes reprogramming', 'conserved transcription factors promote cell fate stability', 'pioneer transcription factors in cell reprogramming', 'the human transcription factors', 'cell fate determination by transcription factors', 'activation of muscle-specific genes in pigment, nerve, fat, liver, and fibroblast cell lines by forced expression of myod', 'cell identity reprogrammed', 'transcription-driven genome organization a model for chromosome structure and the regulation of gene expression tested through simulations', 'reprogramming- emerging strategies to rejuvenate aging', 'computational stem cell biology open questions and guiding principles', 'a decade of transcription factor‑mediated reprogramming to pluripotency', 'molecular interaction networks to select factors for cell conversion', 'transcriptional control of wound repair', 'a modular master regulator landscape controls', 'transcription factories gene expression in unions', 'active genes dynamically colocalize to shared sites of ongoing transcription', 'challenges for computational stem cell biology a discussion for the field', 'responsiveness to perturbations is a hallmark of transcription factors that maintain cell identity', 'high-spatial-resolution multi-omics sequencing via deterministic barcoding in tissue', 'genome-wide myod binding', 'a predictive computational framework for direct reprogramming between human cell types', 'the organization of replication and transcription', 'direct cell-fate conversion of somatic cells toward regenerative medicine and industries', 'ctcf making the right connections', 'leveling waddington the emergence of direct programming and the loss of cell fate hierarchies', 'luck2020reference', 'a computer-guided design tool to increase the efficiency of cellular conversions', 'dynamic condensates activate transcription', 'optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming', 'direct cell reprogramming for tissue engineering', 'transcription factories genome organization and gene regulation', 'a comprehensive library of human transcription', 'enabling direct fate conversion with network biology', 'direct lineage reprogramming strategies, mechanisms, and applications', 'hematopoietic stem cells count', 'the myod family and myogenesis redundancy, networks, and thresholds', 'transcription factors and 3d genome conformation in cell-fate decisions', 'balsalobre and drouin - 2022 - pioneer factors as master regulators of the epigen', 'predicting three-dimensional genome structure from transcriptional activity', 'understanding transcriptional networks regulating initiation of cutaneous wound healing', 'eguchi et al. - 2022 - transdire data-driven direct reprogramming by a p', 'induction of pluripotent stem cells from adult human fibroblasts by defined factors', 'long-term association of a transcription factor with its chromatin binding site can stabilize gene expression and cell fate commitment', 'computational methods for direct cell conversion', 'pederson - 2021 - a layperson encounter, on the “modified” rna world', 'converting cell fates generating hematopoietic stem cells de novo via transcription factor reprogramming', 'nuclear reprogramming in cells']\n"
     ]
    }
   ],
   "source": [
    "source_title = []\n",
    "path = \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\"\n",
    "print(path)\n",
    "for metadata in vectordb.get()['metadatas']:\n",
    "    source_title.append(metadata['source'].removeprefix(path).removesuffix(\".pdf\"))\n",
    "no_repeat_source_title = set(source_title)\n",
    "post_process_title = [x.lower() for x in no_repeat_source_title]\n",
    "print(post_process_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f52dabae-3985-47fb-86f1-cc356e6cc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into two methods?\n",
    "def get_all_sources(prompt, path):\n",
    "    prompt = prompt.lower()\n",
    "    metadata_full = vectordb.get()['metadatas']\n",
    "    source_list = [item['source'] for item in metadata_full]   \n",
    "    real_source_list = [((item.replace(path, '')).removesuffix('.pdf')).lower() for item in source_list]\n",
    "    db = pd.DataFrame({'id' :vectordb.get()['ids'] , 'metadatas' : real_source_list})\n",
    "    filtered_df = db[db['metadatas'].apply(lambda x: x in prompt)]\n",
    "    return real_source_list, filtered_df['id'].to_list()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25fb96b6-db3c-4034-b001-26ed30fb735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Jun 19 09:35:12 2024 INFO Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "Wed Jun 19 09:35:14 2024 INFO Use pytorch device_name: cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_match' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43247/2729889946.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdb_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"new_DB_cosine_cSize_%d_cOver_%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtitle_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_id_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbest_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtitle_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_id_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_sources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectordb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal_id_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'documents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_match' is not defined"
     ]
    }
   ],
   "source": [
    "prompt = 'Summarize the human transcription factors paper'\n",
    "path = \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\"\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n",
    "db_name = \"new_DB_cosine_cSize_%d_cOver_%d\" % (700, 200)\n",
    "title_list, real_id_list = get_all_sources(prompt, path)\n",
    "best_title, best_score = best_match(prompt, title_list)\n",
    "title_list, real_id_list = get_all_sources(best_title, path)\n",
    "text = vectordb.get(ids=real_id_list)['documents']\n",
    "print(len(text))\n",
    "#newdb = Chroma(persist_directory='/nfs/turbo/umms-indikar/shared/projects/RAG/databases/new_EXP3/', embedding_function=embeddings_model, collection_name=db_name)\n",
    "#print(len(newdb.get()['documents']))\n",
    "#newdb.add_texts(text)\n",
    "#print(len(newdb.get()['documents']))\n",
    "\n",
    "#things to do - hook this up with RAG (shouldnt be too bad)\n",
    "#determine how to sift thru the prompt or go thru paper titles that are not well done\n",
    "#ask about which paper db to use on turbo\n",
    "#where to save this new db\n",
    "#sometimes there is a weird error that occurs - maybe split up lower text stuff but idk how that effects runtime\n",
    "#how to remove docs from database / make new one everytime since it keeps adding to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621cada8-4087-4e9b-b522-c30b7695e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "#Given the prompt, find the title and corresponding score that is the best match\n",
    "def best_match(prompt, title_list):\n",
    "    sentence_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    unique_title_list = list(set(title_list))\n",
    "    query_embedding = sentence_model.encode(prompt)\n",
    "    passage_embedding = sentence_model.encode(unique_title_list)\n",
    "\n",
    "    save_title = \"\"\n",
    "    save_score = 0\n",
    "\n",
    "    for score, title in zip(util.cos_sim(query_embedding, passage_embedding)[0], unique_title_list):\n",
    "        if score > save_score:\n",
    "            save_score = score\n",
    "            save_title = title\n",
    "    print(f\"The best match is {save_title} with a score of {save_score}\")\n",
    "    return save_title, save_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1d32cc-16b0-4dce-a271-f3d0d7e9974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27458348-b815-4917-9d68-383f37dfabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "unique_title_list = list(set(title_list))\n",
    "query_embedding = sentence_model.encode(prompt)\n",
    "passage_embedding = sentence_model.encode(unique_title_list)\n",
    "\n",
    "save_title = \"\"\n",
    "save_score = 0\n",
    "\n",
    "for score, title in zip(util.cos_sim(query_embedding, passage_embedding)[0], unique_title_list):\n",
    "    if score > save_score:\n",
    "        save_score = score\n",
    "        save_title = title\n",
    "print(f\"The best match is {save_title} with a score of {save_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935dbf8-b82c-4b89-95c6-70ff9a5e6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list, real_id_list = get_all_sources(save_title, \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\")\n",
    "text = vectordb.get(ids=real_id_list)['documents']\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664ad69-7ab1-41e0-bbd4-149b0721dc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def restrictedDB(prompt, vectordb, path):\n",
    "    #path = \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\"\n",
    "    embeddings_model = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n",
    "    db_name = \"new_DB_cosine_cSize_%d_cOver_%d\" % (700, 200)\n",
    "    title_list, real_id_list = get_all_sources(prompt, path)\n",
    "    best_title, best_score = best_match(prompt, title_list)\n",
    "    title_list, real_id_list = get_all_sources(best_title, path)\n",
    "    text = vectordb.get(ids=real_id_list)['documents']\n",
    "    newdb = Chroma(persist_directory='/nfs/turbo/umms-indikar/shared/projects/RAG/databases/new_EXP3/', embedding_function=embeddings_model, collection_name=db_name)\n",
    "    newdb.add_texts(text)\n",
    "    return best_score, newdb\n",
    "\n",
    "\n",
    "#Given the prompt, find the title and corresponding score that is the best match\n",
    "def best_match(prompt, title_list):\n",
    "    sentence_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    unique_title_list = list(set(title_list))\n",
    "    query_embedding = sentence_model.encode(prompt)\n",
    "    passage_embedding = sentence_model.encode(unique_title_list)\n",
    "\n",
    "    save_title = \"\"\n",
    "    #adjust the save_score to be the threshold cutoff - set to 0.75 but maybe thats too high\n",
    "    save_score = 0.75\n",
    "\n",
    "    for score, title in zip(util.cos_sim(query_embedding, passage_embedding)[0], unique_title_list):\n",
    "        if score > save_score:\n",
    "            save_score = score\n",
    "            save_title = title\n",
    "    print(f\"The best match is {save_title} with a score of {save_score}\")\n",
    "    return save_title, save_score\n",
    "\n",
    "#Split into two methods?\n",
    "def get_all_sources(prompt, path):\n",
    "    prompt = prompt.lower()\n",
    "    metadata_full = vectordb.get()['metadatas']\n",
    "    source_list = [item['source'] for item in metadata_full]   \n",
    "    real_source_list = [((item.replace(path, '')).removesuffix('.pdf')).lower() for item in source_list]\n",
    "    db = pd.DataFrame({'id' :vectordb.get()['ids'] , 'metadatas' : real_source_list})\n",
    "    filtered_df = db[db['metadatas'].apply(lambda x: x in prompt)]\n",
    "    return real_source_list, filtered_df['id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5a074-377a-41fd-b22e-ff0846a4e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Summarize the human transcription factors paper'\n",
    "path = \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\"\n",
    "#path = '/nfs/turbo/umms-indikar/shared/projects/RAG/databases/EXP2/'\n",
    "vectordb, embeddings_model = brad.load_literature_db(persist_directory='/nfs/turbo/umms-indikar/shared/projects/RAG/databases/EXP2/')\n",
    "best_score, text = restrictedDB(prompt, vectordb, path)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5bad5e-82f6-42ed-963f-42903cec1a1c",
   "metadata": {},
   "source": [
    "## PageRanker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59771ae9-3b83-4e8b-8465-e7a8fe157415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeats(vectordb):\n",
    "    df = pd.DataFrame({'id' :vectordb.get()['ids'] , 'documents' : vectordb.get()['documents']})\n",
    "    repeated_ids = df[df.duplicated(subset='documents', keep='last')]['id'].tolist()\n",
    "    vectordb.delete(repeated_ids)\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598f70f-3fe9-410c-8fb5-25093d074c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_repeats(vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea9a41-a9bd-427b-b27b-05b2afccb882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectordb.get()['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a560acc-7478-46e1-8ce6-92243d1bc920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given the prompt, find the title and corresponding score that is the best match\n",
    "def adj_matrix_builder(vectordb):\n",
    "    dimension = len(vectordb.get()['documents'])\n",
    "    adj_matrix = np.zeros([dimension, dimension])\n",
    "    \n",
    "    sentence_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    passage_embedding = sentence_model.encode(vectordb.get()['documents'])\n",
    "    cosine_similarities = cosine_similarity(passage_embedding[0:10])\n",
    "    print(cosine_similarities)\n",
    "    return cosine_similarities\n",
    "\n",
    "# Normalize columns of A\n",
    "def normalize_adjacency_matrix(A):\n",
    "    col_sums = A.sum(axis=0)\n",
    "    return A / col_sums[np.newaxis, :]\n",
    "\n",
    "#weighted pagerank algorithm\n",
    "def pagerank_weighted(A, alpha=0.85, tol=1e-6, max_iter=100):\n",
    "    n = A.shape[0]\n",
    "    A_normalized = normalize_adjacency_matrix(A)\n",
    "    v = np.ones(n) / n  # Initial PageRank vector\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        v_next = alpha * A_normalized.dot(v) + (1 - alpha) / n\n",
    "        if np.linalg.norm(v_next - v, 1) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    return v\n",
    "\n",
    "#reranker\n",
    "\n",
    "def pagerank_rerank(vectordb):\n",
    "    adj_matrix = adj_matrix_builder(vectordb)\n",
    "    pagerank_scores = pagerank_weighted(A = adj_matrix)\n",
    "    top_rank_scores = sorted(range(len(pagerank_scores)), key=lambda i: pagerank_scores[i], reverse=True)[1:11]\n",
    "    df = pd.DataFrame({'id' :vectordb.get()['ids'] , 'documents' : vectordb.get()['documents'], 'metadatas' : vectordb.get()['metadatas']})\n",
    "    reranked_df = df.iloc[top_rank_scores]\n",
    "    return reranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd5fa7a4-44de-4e92-b211-9f39d1acd0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Jun 19 10:10:42 2024 INFO Load pretrained SentenceTransformer: multi-qa-MiniLM-L6-cos-v1\n",
      "Wed Jun 19 10:10:43 2024 INFO Use pytorch device_name: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8439daaae87d4cfab663a9d3a95ba1fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/283 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[ 3.42600904e-02 -2.65194345e-02  4.13578190e-03 -6.21633492e-02\n",
      " -2.41281725e-02 -4.93638031e-02  2.85513680e-02  7.13860393e-02\n",
      " -2.77028400e-02 -3.23414840e-02  1.54855046e-02 -7.62076303e-02\n",
      " -2.67160474e-03 -8.98527075e-03 -1.44219518e-01 -3.77658121e-02\n",
      " -1.72684994e-02  5.09255640e-02 -1.00878596e-01  4.22701798e-02\n",
      "  3.55484076e-02  2.83368211e-02  6.34182319e-02  1.38540920e-02\n",
      "  4.28031944e-02 -2.33840593e-03 -5.75252585e-02 -3.45930345e-02\n",
      " -3.44563276e-02 -4.67588045e-02  3.03957816e-02  7.35652298e-02\n",
      " -1.88741330e-02  3.93743366e-02  9.98095945e-02  6.60319487e-03\n",
      "  1.13752242e-02 -3.43076847e-02  5.02356282e-03  6.19645230e-02\n",
      "  1.43595808e-03 -5.39184175e-02  1.55541557e-03  7.00846761e-02\n",
      " -1.05156219e-02  5.57598844e-02 -6.67627007e-02  2.79081725e-02\n",
      " -4.93292846e-02  2.93017998e-02 -2.24598031e-02 -1.59579709e-01\n",
      " -1.07039087e-01  7.74903372e-02 -5.58888307e-03  3.52224871e-03\n",
      " -3.88910174e-02 -8.47253352e-02  1.96575955e-03 -4.22144346e-02\n",
      " -8.96326266e-03  4.80527803e-02  6.06459379e-02  6.50148392e-02\n",
      " -5.83539046e-02 -4.21044342e-02  8.45701918e-02 -2.08761226e-02\n",
      " -4.41055838e-03 -1.42148975e-02  1.22068252e-03 -5.50714359e-02\n",
      " -5.55474088e-02  8.95126835e-02  5.27813509e-02 -8.78408668e-04\n",
      "  9.89418551e-02  1.20901233e-02  6.53334782e-02 -7.05374181e-02\n",
      " -1.72009282e-02  6.02795966e-02  1.10234842e-01  3.38722132e-02\n",
      "  7.98005518e-03 -3.46322660e-04 -4.09705564e-02  1.25981951e-02\n",
      " -1.65861938e-02 -3.03619839e-02 -4.57951473e-03 -6.17944784e-02\n",
      " -6.68502413e-03  4.60215621e-02 -8.34419355e-02  2.36760546e-02\n",
      " -4.77545373e-02 -9.53170136e-02  4.66393642e-02  2.34658248e-03\n",
      " -5.92933334e-02 -1.14397161e-01  9.48165357e-02  3.04480642e-02\n",
      " -1.21943451e-01 -1.27895370e-01  2.08842326e-02 -2.66152974e-02\n",
      "  3.08340248e-02  3.45666222e-02 -2.56028213e-02 -8.21714383e-03\n",
      "  4.08445671e-02  4.71116006e-02 -1.92162301e-02 -1.06288688e-02\n",
      "  6.00894764e-02  7.29859918e-02  5.13058826e-02 -2.16428116e-02\n",
      " -5.66692315e-02 -8.11468512e-02  4.05867286e-02 -3.27727869e-02\n",
      " -6.70279041e-02 -1.90966520e-02 -9.66326669e-02 -7.40647111e-31\n",
      "  5.13327308e-02 -4.27769348e-02  2.94460673e-02  7.70593807e-02\n",
      "  2.02536937e-02  9.13530996e-04 -2.31409371e-02 -3.59489769e-02\n",
      " -5.71170226e-02 -7.68868923e-02 -1.21183135e-01 -4.47408967e-02\n",
      " -2.82229576e-02  1.25806078e-01  3.35216559e-02 -3.76469195e-02\n",
      "  6.82903966e-03  2.84867361e-02 -5.79052009e-02 -2.06561945e-02\n",
      " -6.24787528e-03  7.66132697e-02  7.43661448e-02  6.19850531e-02\n",
      "  7.17235282e-02 -2.22003367e-02 -9.58528444e-02 -5.07474318e-03\n",
      " -1.69059839e-02  1.91011559e-02  4.34046611e-02 -5.46794198e-02\n",
      " -7.31241331e-02 -8.69261250e-02 -2.45979405e-03  6.57068864e-02\n",
      " -4.40621860e-02 -8.46584886e-02 -3.13223451e-02 -4.34847549e-02\n",
      "  2.34227367e-02 -5.61920833e-03  1.36634838e-02 -4.07926515e-02\n",
      "  1.33454323e-01  2.65368372e-02  2.27467343e-03 -1.12652294e-02\n",
      "  6.42101690e-02 -1.36845047e-02  5.57231065e-03 -2.58372556e-02\n",
      "  4.20599021e-02 -6.61668554e-02  4.45712619e-02  1.13843054e-01\n",
      "  9.74881742e-03  3.42138708e-02 -3.57031301e-02  6.52349368e-02\n",
      "  6.51767999e-02  8.37470219e-02  1.00872599e-01  6.08743401e-03\n",
      "  6.53432086e-02  1.29668405e-02 -5.17854095e-02 -1.63664613e-02\n",
      " -6.08371058e-03  3.06405313e-03  2.20731217e-02 -7.14566410e-02\n",
      "  7.03582168e-02 -6.21744292e-03  8.09969567e-03 -2.10654307e-02\n",
      "  6.89808354e-02 -1.11444086e-01 -1.62329972e-02 -1.01146020e-01\n",
      "  3.29566449e-02  4.69800690e-03 -1.10454317e-02 -9.11143571e-02\n",
      " -7.28407875e-02 -5.57312332e-02  3.22400848e-03 -2.44901888e-02\n",
      " -5.31150922e-02 -2.27594506e-02 -3.48518007e-02 -1.58783235e-02\n",
      " -1.30928084e-01 -1.99741703e-02 -8.74009803e-02 -9.68296206e-33\n",
      " -4.63070124e-02 -1.75871495e-02  3.18040513e-03  5.36127239e-02\n",
      " -2.19627619e-02  4.67303880e-02 -2.18017977e-02  5.54330610e-02\n",
      "  4.03851829e-02 -2.29172409e-02  5.22562787e-02 -9.86932311e-03\n",
      " -5.80757633e-02 -6.76681399e-02  4.58497833e-03 -2.09098626e-02\n",
      " -1.06684966e-02 -1.57819614e-02 -3.46637368e-02  4.48258333e-02\n",
      " -1.63453426e-02  2.48237476e-02  1.11366529e-03 -2.49117538e-02\n",
      "  4.19655209e-03 -6.59903511e-02 -4.37788898e-03  7.72692114e-02\n",
      " -1.83471851e-02  1.95218939e-02  3.23219150e-02 -1.80193111e-02\n",
      "  3.40439714e-02  1.48713579e-02  3.24439853e-02 -6.00847453e-02\n",
      "  1.03194378e-01 -1.62406601e-02 -1.88494436e-02 -7.00788274e-02\n",
      "  2.84999982e-03 -3.79684642e-02  5.17212925e-03  2.81314310e-02\n",
      "  3.27897295e-02  2.43451688e-02 -8.64975248e-03 -6.80772513e-02\n",
      "  2.85569988e-02  9.80757177e-03 -2.32572388e-02 -2.38723941e-02\n",
      " -1.01670530e-02  3.18629667e-02  2.47607604e-02  6.72445819e-02\n",
      "  1.81963183e-02 -5.95123507e-02 -2.33194772e-02 -2.48928126e-02\n",
      " -6.58483356e-02 -6.96641812e-03 -8.84514209e-03  4.58680093e-02\n",
      "  5.92715256e-02  3.37421708e-02  2.12121885e-02 -3.24273761e-03\n",
      " -4.00957763e-02 -9.30852722e-03  1.57235209e-02  1.06662855e-01\n",
      " -1.93225443e-02 -1.04248680e-01 -3.42467278e-02 -1.06970310e-01\n",
      " -1.83978528e-02 -2.05367953e-02 -2.88721751e-02 -6.28458634e-02\n",
      " -8.35709367e-03 -6.73308745e-02 -8.85734241e-03  4.81099859e-02\n",
      "  4.42524068e-02 -5.95180131e-03  8.06987211e-02  4.64988984e-02\n",
      " -1.43575519e-02  5.32886088e-02  3.50593887e-02 -5.62686957e-02\n",
      " -2.11793021e-03  3.17751467e-02  3.58335264e-02 -4.16337654e-33\n",
      "  1.05537847e-01 -4.87347599e-03 -7.79593596e-03 -3.70836332e-02\n",
      "  2.92168688e-02 -2.95099914e-02 -4.05120365e-02  5.33069596e-02\n",
      "  2.81547126e-03 -2.30692271e-02 -5.70301749e-02  7.36315995e-02\n",
      " -4.87144329e-02 -1.60479248e-02  1.10607974e-01  2.09197719e-02\n",
      " -6.84310943e-02  5.91976307e-02 -3.48663107e-02  1.78408772e-02\n",
      " -6.42407686e-03  1.75018050e-02  1.59148145e-02  4.66372855e-02\n",
      "  9.20023471e-02 -2.11079735e-02 -1.49334706e-02 -7.40211681e-02\n",
      " -2.26881374e-02 -4.53696996e-02  1.52992764e-02 -1.77011508e-02\n",
      "  1.72270872e-02  3.47990240e-03 -1.32151902e-01  8.82211551e-02\n",
      "  4.55469489e-02 -5.91417067e-02  3.02613042e-02 -1.00358054e-02\n",
      "  1.19872764e-02 -1.83147211e-02  6.68098629e-02  2.87250355e-02\n",
      "  5.53128831e-02  4.23418917e-02 -2.32481137e-02  3.06411181e-02\n",
      " -1.34245986e-02 -3.02810371e-02 -2.51795147e-02 -4.12921188e-03\n",
      " -5.41503467e-02  2.50727724e-04 -1.05072975e-01  4.44655977e-02\n",
      " -1.91316381e-02  1.68137439e-02  3.76179330e-02  5.36682606e-02\n",
      "  7.29644001e-02 -3.68557125e-02  4.05977108e-02  1.52972871e-02]\n",
      "<class 'numpy.ndarray'>\n",
      "[[ 3.42600904e-02 -2.65194345e-02  4.13578190e-03 ... -3.68557125e-02\n",
      "   4.05977108e-02  1.52972871e-02]\n",
      " [-6.32728040e-02 -5.63978627e-02 -3.63026969e-02 ...  6.34608278e-03\n",
      "   5.37550151e-02  2.43272446e-02]\n",
      " [ 4.75836387e-05  3.75942066e-02 -4.14297581e-02 ...  1.24578113e-02\n",
      "   3.51196975e-02 -1.05824433e-01]\n",
      " ...\n",
      " [-4.57842909e-02  1.14695705e-01 -2.63791848e-02 ... -8.95160213e-02\n",
      "   6.89394847e-02  1.04193063e-02]\n",
      " [ 2.96822656e-03 -4.75173593e-02 -1.15894191e-02 ...  5.19799814e-02\n",
      "   6.71446649e-03  1.39544150e-02]\n",
      " [-4.13333811e-02  7.36171799e-03 -6.48237318e-02 ...  8.70315954e-02\n",
      "   1.11585418e-02 -6.05779737e-02]]\n",
      "[[1.         0.40780178 0.2239058  0.34967136 0.27000853 0.37449303\n",
      "  0.11974596 0.471384   0.28080007 0.41771033]\n",
      " [0.40780178 0.9999999  0.29097712 0.43835777 0.14606073 0.26247755\n",
      "  0.29910707 0.48260686 0.34905434 0.4715516 ]\n",
      " [0.2239058  0.29097712 1.0000005  0.35702574 0.05815138 0.29308593\n",
      "  0.130587   0.2925227  0.23897788 0.30013344]\n",
      " [0.34967136 0.43835777 0.35702574 0.9999999  0.12500063 0.2443494\n",
      "  0.19293565 0.50623006 0.35507986 0.38875586]\n",
      " [0.27000853 0.14606073 0.05815138 0.12500063 0.9999999  0.15680732\n",
      "  0.28346938 0.21009412 0.21672143 0.25006104]\n",
      " [0.37449303 0.26247755 0.29308593 0.2443494  0.15680732 1.0000004\n",
      "  0.22511247 0.23897696 0.3399314  0.40085596]\n",
      " [0.11974596 0.29910707 0.130587   0.19293565 0.28346938 0.22511247\n",
      "  1.         0.28216627 0.40895784 0.29408607]\n",
      " [0.471384   0.48260686 0.2925227  0.50623006 0.21009412 0.23897696\n",
      "  0.28216627 1.0000002  0.37226495 0.44979596]\n",
      " [0.28080007 0.34905434 0.23897788 0.35507986 0.21672143 0.3399314\n",
      "  0.40895784 0.37226495 0.9999999  0.5538584 ]\n",
      " [0.41771033 0.4715516  0.30013344 0.38875586 0.25006104 0.40085596\n",
      "  0.29408607 0.44979596 0.5538584  1.0000001 ]]\n",
      "                                     id  \\\n",
      "7  0034189a-1ec3-43d2-9472-3355edce9a66   \n",
      "1  00051c4b-22c7-4a78-9228-3c0c80f49a9f   \n",
      "8  003f1050-35fa-4c4f-bde4-4fe66dcdee26   \n",
      "3  001b09f1-1682-4f8c-a7fe-54794dcac1c8   \n",
      "0  0003eff7-c988-466f-97bd-85df69ca723a   \n",
      "5  002fef87-d134-4dbd-b35d-66072145e37b   \n",
      "6  00315294-0300-435f-9a20-c352c35238fd   \n",
      "2  001051b3-d967-40be-bf97-15f77c4b7935   \n",
      "4  002de69a-4c66-43a5-a4d0-b5c9198a8ce2   \n",
      "\n",
      "                                           documents  \\\n",
      "7  ﬁbroblasts to myoblasts. Cell 51, 987-1000.\\nD...   \n",
      "1  virally deliv-\\nering Ascl1 to Mu ¨ller glia i...   \n",
      "8  and Cooke used three other\\nendothelial-speciﬁ...   \n",
      "3  stage of differentiation, \\nwhen the globin ge...   \n",
      "0  lOE(A)\\nFIG.1.MapsoftheparentandMyoDretrovirus...   \n",
      "5  0123456789();: increasingly affordable and mor...   \n",
      "6  made meaningful differences in the lives of pa...   \n",
      "2  assign a p value to the whole-chromosome score...   \n",
      "4  set of devices for ﬂow barcoding a tissue slid...   \n",
      "\n",
      "                                           metadatas  \n",
      "7  {'page': 9, 'source': '/nfs/turbo/umms-indikar...  \n",
      "1  {'page': 5, 'source': '/nfs/turbo/umms-indikar...  \n",
      "8  {'page': 6, 'source': '/nfs/turbo/umms-indikar...  \n",
      "3  {'page': 6, 'source': '/nfs/turbo/umms-indikar...  \n",
      "0  {'page': 0, 'source': '/nfs/turbo/umms-indikar...  \n",
      "5  {'page': 11, 'source': '/nfs/turbo/umms-indika...  \n",
      "6  {'page': 8, 'source': '/nfs/turbo/umms-indikar...  \n",
      "2  {'page': 24, 'source': '/nfs/turbo/umms-indika...  \n",
      "4  {'page': 19, 'source': '/nfs/turbo/umms-indika...  \n"
     ]
    }
   ],
   "source": [
    "rank_scores = pagerank_rerank(vectordb)\n",
    "print(rank_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ff3be-56f7-4a25-8862-c0c9993e5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first entry is always the prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08771f10-5301-4695-a331-086ba5a30623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given the prompt, find the title and corresponding score that is the best match\n",
    "def adj_matrix_builder(vectordb):\n",
    "    dimension = len(vectordb.get()['documents'])\n",
    "    adj_matrix = np.zeros([dimension, dimension])\n",
    "    \n",
    "    sentence_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "    passage_embedding = sentence_model.encode(vectordb.get()['documents'])\n",
    "    print(type(passage_embedding))\n",
    "    print(passage_embedding)\n",
    "    pos = 1\n",
    "    for score in zip(util.cos_sim(query_embedding, passage_embedding)[0]):\n",
    "        adj_matrix[0][pos] = score[0]\n",
    "        adj_matrix[pos][0] = score[0]\n",
    "        pos += 1\n",
    "    return adj_matrix\n",
    "\n",
    "# Normalize columns of A\n",
    "def normalize_adjacency_matrix(A):\n",
    "    col_sums = A.sum(axis=0)\n",
    "    return A / col_sums[np.newaxis, :]\n",
    "\n",
    "#weighted pagerank algorithm\n",
    "def pagerank_weighted(A, alpha=0.85, tol=1e-6, max_iter=100):\n",
    "    n = A.shape[0]\n",
    "    A_normalized = normalize_adjacency_matrix(A)\n",
    "    v = np.ones(n) / n  # Initial PageRank vector\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        v_next = alpha * A_normalized.dot(v) + (1 - alpha) / n\n",
    "        if np.linalg.norm(v_next - v, 1) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    return v\n",
    "\n",
    "#reranker\n",
    "\n",
    "def pagerank_rerank(prompt, vectordb):\n",
    "    adj_matrix = adj_matrix_builder(prompt, vectordb)\n",
    "    pagerank_scores = pagerank_weighted(A = adj_matrix)\n",
    "    top_rank_scores = sorted(range(len(pagerank_scores)), key=lambda i: pagerank_scores[i], reverse=True)[1:11]\n",
    "    df = pd.DataFrame({'id' :vectordb.get()['ids'] , 'documents' : vectordb.get()['documents'], 'metadatas' : vectordb.get()['metadatas']})\n",
    "    reranked_df = df.iloc[top_rank_scores]\n",
    "    return reranked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a2b227-4d2c-407b-b442-06a2296909a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I THINK THIS FRAMEWORK WORKS BETTER\n",
    "\n",
    "\n",
    "#Given the prompt, find the title and corresponding score that is the best match\n",
    "def adj_matrix_builder(docs, scores):\n",
    "    dimension = len(docs)\n",
    "    adj_matrix = np.zeros([dimension, dimension])\n",
    "    pos = 1\n",
    "    for score in scores:\n",
    "        adj_matrix[0][pos] = score\n",
    "        adj_matrix[pos][0] = score\n",
    "        pos += 1\n",
    "    return adj_matrix\n",
    "\n",
    "# Normalize columns of A\n",
    "def normalize_adjacency_matrix(A):\n",
    "    col_sums = A.sum(axis=0)\n",
    "    return A / col_sums[np.newaxis, :]\n",
    "\n",
    "#weighted pagerank algorithm\n",
    "def pagerank_weighted(A, alpha=0.85, tol=1e-6, max_iter=100):\n",
    "    n = A.shape[0]\n",
    "    A_normalized = normalize_adjacency_matrix(A)\n",
    "    v = np.ones(n) / n  # Initial PageRank vector\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        v_next = alpha * A_normalized.dot(v) + (1 - alpha) / n\n",
    "        if np.linalg.norm(v_next - v, 1) < tol:\n",
    "            break\n",
    "        v = v_next\n",
    "\n",
    "    return v\n",
    "\n",
    "#reranker\n",
    "\n",
    "def pagerank_rerank(docs, scores):\n",
    "    adj_matrix = adj_matrix_builder(docs, scores)\n",
    "    pagerank_scores = pagerank_weighted(A = adj_matrix)\n",
    "    top_rank_scores = sorted(range(len(pagerank_scores)), key=lambda i: pagerank_scores[i], reverse=True)[1:11]\n",
    "    reranked_docs = [docs[i] for i in top_rank_scores]\n",
    "    return reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb2d09-8418-4de0-9a5f-156da3521770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2458e3d-57c2-4889-9aa1-a031b51ea279",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/nfs/turbo/umms-indikar/shared/projects/RAG/papers/EXP2/\"\n",
    "metadata_full = vectordb.get()['metadatas']\n",
    "source_list = [item['source'] for item in metadata_full]   \n",
    "real_source_list = [((item.replace(path, '')).removesuffix('.pdf')).lower() for item in source_list]\n",
    "df = pd.DataFrame({'id' :vectordb.get()['ids'] , 'documents' : vectordb.get()['documents'], 'metadatas' : real_source_list})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331132ee-2e45-4b72-b4a4-9e3a6f366d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "brad.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82c6636-4b4b-4c04-80d0-cc3ea5911036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairwise Cosine Similarities:\n",
      "[[1.         0.97463185 0.95941195]\n",
      " [0.97463185 1.         0.99819089]\n",
      " [0.95941195 0.99819089 1.        ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6d80e54-c93c-4b6e-b094-f4c7995cb437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to RAG! The chat log from this conversation will be saved to /home/machoi/BRAD/2024-06-19-10:27:27.230997.json. How can I help?\n",
      "\n",
      "Would you like to use a database with BRAD [Y/N]?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Wed Jun 19 10:27:29 2024 INFO Load pretrained SentenceTransformer: BAAI/bge-base-en-v1.5\n",
      "/home/machoi/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Wed Jun 19 10:27:30 2024 INFO Use pytorch device_name: cpu\n",
      "Wed Jun 19 10:27:30 2024 INFO Collection DB_cosine_cSize_700_cOver_200 is not created.\n",
      "\u001b[32m2024-06-19 10:27:31 INFO semantic_router.utils.logger local\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'callMatlab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43247/2128865677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/RAG2/RAG-DEV/BRAD/brad.py\u001b[0m in \u001b[0;36mchat\u001b[0;34m(model_path, persist_directory, llm, ragvectordb, embeddings_model)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;31m# Initialize all modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m     \u001b[0mmodule_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetModules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RAG2/RAG-DEV/BRAD/brad.py\u001b[0m in \u001b[0;36mgetModules\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;34m'SNS'\u001b[0m    \u001b[0;34m:\u001b[0m \u001b[0mcallSnsV3\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# seaborn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;34m'RAG'\u001b[0m    \u001b[0;34m:\u001b[0m \u001b[0mqueryDocs\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# standard rag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;34m'MATLAB'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcallMatlab\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0;31m# matlab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;34m'PYTHON'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcallPython\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;34m'SNAKE'\u001b[0m  \u001b[0;34m:\u001b[0m \u001b[0mcallSnakemake\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# snakemake,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'callMatlab' is not defined"
     ]
    }
   ],
   "source": [
    "brad.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ccb38-002f-4c2e-afa5-88ec92a45e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
