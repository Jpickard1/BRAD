{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a610eb5c-6dd3-4b6b-b78b-4b0a0379ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21fda6a1-2890-4eb2-914a-899d5bc01d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e9da18e-a458-479a-bb29-4bb9528fd383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpic/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'instruction': [\n",
    "        'Given this dataframe, make a scatter plot of Age and Weight with x label Age, y label Weight, marker size of 10 and highlight each point according to its city.',\n",
    "        'Create a scatter plot with Age on x-axis and Weight on y-axis. Label the x-axis as Age and the y-axis as Weight. Use marker size 10 and color points by city.'\n",
    "    ],\n",
    "    'output': [\n",
    "        \"{'plot_type': 'scatter', 'x': 'Age', 'y': 'Weight', 'xlabel': 'Age', 'ylabel': 'Weight', 'marker_size': '10', 'hue': 'city'}\",\n",
    "        \"{'plot_type': 'scatter', 'x': 'Age', 'y': 'Weight', 'xlabel': 'Age', 'ylabel': 'Weight', 'marker_size': '10', 'hue': 'city'}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"translate English to JSON: {ex}\" for ex in examples['instruction']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['output'], max_length=512, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4198f995-081e-41b7-abbd-98b9007371b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efe334a034349029ca1413aadff5ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpic/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3921: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c10194-175b-4e96-a18a-2f1c0b141f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59fbe788-cc13-4605-a9ea-0068826715a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 45 at dim 1 (got 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2165\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2162\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2164\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2166\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/data_loader.py:454\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 454\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.10-anaconda/2023.03/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.10-anaconda/2023.03/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.10-anaconda/2023.03/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/data/data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[0;34m(features, return_tensors)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/data/data_collator.py:158\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    156\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mstack([f[k] \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]))\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m             batch[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 45 at dim 1 (got 58)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec3f10c-10f7-4daa-940d-b2d6352cdbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f88cd579b4e41e99c38097e54448770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [f\"translate English to JSON: {ex}\" for ex in examples['instruction']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding='max_length', truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['output'], max_length=512, padding='max_length', truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d34aebc-c66b-4b45-a887-563c9016a619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input lengths:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column train not in the dataset. Current columns in the dataset: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Check lengths of tokenized inputs and labels\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized input lengths:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenized_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(input_ids))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenized label lengths:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2861\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2861\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:2845\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2843\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2844\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2845\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2846\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2847\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2848\u001b[0m )\n\u001b[1;32m   2849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:584\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    582\u001b[0m         _raise_bad_key_type(key)\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 584\u001b[0m     \u001b[43m_check_valid_column_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/formatting/formatting.py:521\u001b[0m, in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_column_key\u001b[39m(key: \u001b[38;5;28mstr\u001b[39m, columns: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:\n\u001b[0;32m--> 521\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. Current columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['instruction', 'output', 'input_ids', 'attention_mask', 'labels']\""
     ]
    }
   ],
   "source": [
    "# Check lengths of tokenized inputs and labels\n",
    "print(\"Tokenized input lengths:\")\n",
    "for input_ids in tokenized_dataset[\"train\"][\"input_ids\"]:\n",
    "    print(len(input_ids))\n",
    "\n",
    "print(\"Tokenized label lengths:\")\n",
    "for label_ids in tokenized_dataset[\"train\"][\"labels\"]:\n",
    "    print(len(label_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5fec7ea-def4-4054-ad21-57f96da8c031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d7e8d3774e24a60bfa1c940cea74366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'instruction': [\n",
    "        'Given this dataframe, make a scatter plot of Age and Weight with x label Age, y label Weight, marker size of 10 and highlight each point according to its city.',\n",
    "        'Create a scatter plot with Age on x-axis and Weight on y-axis. Label the x-axis as Age and the y-axis as Weight. Use marker size 10 and color points by city.'\n",
    "    ],\n",
    "    'output': [\n",
    "        \"{'plot_type': 'scatter', 'x': 'Age', 'y': 'Weight', 'xlabel': 'Age', 'ylabel': 'Weight', 'marker_size': '10', 'hue': 'city'}\",\n",
    "        \"{'plot_type': 'scatter', 'x': 'Age', 'y': 'Weight', 'xlabel': 'Age', 'ylabel': 'Weight', 'marker_size': '10', 'hue': 'city'}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to dataset\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_function(examples):\n",
    "    inputs = [f\"translate English to JSON: {ex}\" for ex in examples['instruction']]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, padding='max_length', truncation=True)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['output'], max_length=512, padding='max_length', truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07cd1ce6-462d-41be-a0f0-5306c14dc064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input lengths:\n",
      "512\n",
      "512\n",
      "Tokenized label lengths:\n",
      "512\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# Check lengths of tokenized inputs and labels\n",
    "print(\"Tokenized input lengths:\")\n",
    "for input_ids in tokenized_dataset[\"input_ids\"]:\n",
    "    print(len(input_ids))\n",
    "\n",
    "print(\"Tokenized label lengths:\")\n",
    "for label_ids in tokenized_dataset[\"labels\"]:\n",
    "    print(len(label_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e000e60e-cacd-412c-94d0-7772a2baa0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26a884de-a6ee-4a4e-bbc5-6a06571722b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.278948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.205039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>11.154114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=11.72619883219401, metrics={'train_runtime': 64.4474, 'train_samples_per_second': 0.047, 'train_steps_per_second': 0.047, 'total_flos': 406025404416.0, 'train_loss': 11.72619883219401, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eee0fcfe-8eb1-47c9-8720-195ef2f26a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 11.15411376953125, 'eval_runtime': 0.3522, 'eval_samples_per_second': 2.839, 'eval_steps_per_second': 2.839, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate()\n",
    "print(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16ff56ce-6529-48dc-b973-e9c71869c60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction: Given this dataframe, create a bar chart of Sales and Month with x label Month, y label Sales, and color by Region.\n",
      "Generated JSON: Fill in the following JSON object based on the instruction: The JSON object is 'x': None, 'y': None, 'hue': None, 'xlabel': None, 'ylabel': None, 'title': None. Based on the instruction, provide values for 'x' (variable for the x-axis), 'y' (variable for the y-axis), 'hue' (\n",
      "\n",
      "Instruction: Generate a line plot with Date on the x-axis and Temperature on the y-axis. Label the x-axis as Date and the y-axis as Temperature. Use different colors for each City.\n",
      "Generated JSON: 'x': None, 'y': None, 'hue': None, 'ylabel': None, 'title': None. Based on the instruction, provide values for 'x' (variable for the x-axis), 'y' (variable for the y-axis), 'hue' (variable for color differentiation), 'xlabel' (label for the x-axis), 'yl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "new_instructions = [\n",
    "    'Given this dataframe, create a bar chart of Sales and Month with x label Month, y label Sales, and color by Region.',\n",
    "    'Generate a line plot with Date on the x-axis and Temperature on the y-axis. Label the x-axis as Date and the y-axis as Temperature. Use different colors for each City.'\n",
    "]\n",
    "\n",
    "# Encode the new inputs\n",
    "# new_inputs = [f\"Fill in the following JSON object based on the instruction: The JSON object is {'x': None, 'y': None, 'hue': None, 'xlabel': None, 'ylabel': None, 'title': None}. Based on the instruction, provide values for 'x' (variable for the x-axis), 'y' (variable for the y-axis), 'hue' (variable for color differentiation), 'xlabel' (label for the x-axis), 'ylabel' (label for the y-axis), and 'title' (title of the plot).: {instruction}\" for instruction in new_instructions]\n",
    "new_inputs = [f\"Fill in the following JSON object based on the instruction: The JSON object is {{'x': None, 'y': None, 'hue': None, 'xlabel': None, 'ylabel': None, 'title': None}}. Based on the instruction, provide values for 'x' (variable for the x-axis), 'y' (variable for the y-axis), 'hue' (variable for color differentiation), 'xlabel' (label for the x-axis), 'ylabel' (label for the y-axis), and 'title' (title of the plot).: {instruction}\" for instruction in new_instructions]\n",
    "# new_inputs = [f\"construct a python dictionary with plotting arguments from the instruction: {instruction}\" for instruction in new_instructions]\n",
    "tokenized_inputs = tokenizer(new_inputs, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "\n",
    "# Move inputs to the correct device\n",
    "tokenized_inputs = {key: value.to(device) for key, value in tokenized_inputs.items()}\n",
    "\n",
    "# Generate outputs\n",
    "outputs = model.generate(**tokenized_inputs, max_length=512, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode the outputs\n",
    "decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "for input_text, output_text in zip(new_instructions, decoded_outputs):\n",
    "    print(f\"Instruction: {input_text}\")\n",
    "    print(f\"Generated JSON: {output_text}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b161733-c6bb-46fc-b8b3-5adb63688727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32428073-c83a-449b-93e0-344678814d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d30b28b7-d8f9-4910-b035-19cc2d90bf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function boxplot in module seaborn.categorical:\n",
      "\n",
      "boxplot(data=None, *, x=None, y=None, hue=None, order=None, hue_order=None, orient=None, color=None, palette=None, saturation=0.75, width=0.8, dodge=True, fliersize=5, linewidth=None, whis=1.5, ax=None, **kwargs)\n",
      "    Draw a box plot to show distributions with respect to categories.\n",
      "    \n",
      "    A box plot (or box-and-whisker plot) shows the distribution of quantitative\n",
      "    data in a way that facilitates comparisons between variables or across\n",
      "    levels of a categorical variable. The box shows the quartiles of the\n",
      "    dataset while the whiskers extend to show the rest of the distribution,\n",
      "    except for points that are determined to be \"outliers\" using a method\n",
      "    that is a function of the inter-quartile range.\n",
      "    \n",
      "    .. note::\n",
      "        This function always treats one of the variables as categorical and\n",
      "        draws data at ordinal positions (0, 1, ... n) on the relevant axis,\n",
      "        even when the data has a numeric or date type.\n",
      "    \n",
      "    See the :ref:`tutorial <categorical_tutorial>` for more information.    \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data : DataFrame, array, or list of arrays, optional\n",
      "        Dataset for plotting. If ``x`` and ``y`` are absent, this is\n",
      "        interpreted as wide-form. Otherwise it is expected to be long-form.    \n",
      "    x, y, hue : names of variables in ``data`` or vector data, optional\n",
      "        Inputs for plotting long-form data. See examples for interpretation.    \n",
      "    order, hue_order : lists of strings, optional\n",
      "        Order to plot the categorical levels in; otherwise the levels are\n",
      "        inferred from the data objects.    \n",
      "    orient : \"v\" | \"h\", optional\n",
      "        Orientation of the plot (vertical or horizontal). This is usually\n",
      "        inferred based on the type of the input variables, but it can be used\n",
      "        to resolve ambiguity when both `x` and `y` are numeric or when\n",
      "        plotting wide-form data.    \n",
      "    color : matplotlib color, optional\n",
      "        Single color for the elements in the plot.    \n",
      "    palette : palette name, list, or dict\n",
      "        Colors to use for the different levels of the ``hue`` variable. Should\n",
      "        be something that can be interpreted by :func:`color_palette`, or a\n",
      "        dictionary mapping hue levels to matplotlib colors.    \n",
      "    saturation : float, optional\n",
      "        Proportion of the original saturation to draw colors at. Large patches\n",
      "        often look better with slightly desaturated colors, but set this to\n",
      "        `1` if you want the plot colors to perfectly match the input color.    \n",
      "    width : float, optional\n",
      "        Width of a full element when not using hue nesting, or width of all the\n",
      "        elements for one level of the major grouping variable.    \n",
      "    dodge : bool, optional\n",
      "        When hue nesting is used, whether elements should be shifted along the\n",
      "        categorical axis.    \n",
      "    fliersize : float, optional\n",
      "        Size of the markers used to indicate outlier observations.\n",
      "    linewidth : float, optional\n",
      "        Width of the gray lines that frame the plot elements.    \n",
      "    whis : float, optional\n",
      "        Maximum length of the plot whiskers as proportion of the\n",
      "        interquartile range. Whiskers extend to the furthest datapoint\n",
      "        within that range. More extreme points are marked as outliers.\n",
      "    ax : matplotlib Axes, optional\n",
      "        Axes object to draw the plot onto, otherwise uses the current Axes.    \n",
      "    kwargs : key, value mappings\n",
      "        Other keyword arguments are passed through to\n",
      "        :meth:`matplotlib.axes.Axes.boxplot`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    ax : matplotlib Axes\n",
      "        Returns the Axes object with the plot drawn onto it.    \n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    violinplot : A combination of boxplot and kernel density estimation.    \n",
      "    stripplot : A scatterplot where one variable is categorical. Can be used\n",
      "                in conjunction with other plots to show each observation.    \n",
      "    swarmplot : A categorical scatterplot where the points do not overlap. Can\n",
      "                be used with other plots to show each observation.    \n",
      "    catplot : Combine a categorical plot with a :class:`FacetGrid`.    \n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    .. include:: ../docstrings/boxplot.rst\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = help(sns.boxplot)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09112492-9edb-45c7-9606-5e7c27c750e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da6ec82-e0e6-4f62-a896-7a338fa1238c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10161e-82fa-47af-8cf3-dac72372fdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1cd7e9-7f94-4b48-9ce6-5315de981b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef700786-79a2-445b-8f02-04719ee70db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a41ba61-31d1-436b-b52f-aecda1699694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ba69381-399e-4f97-8bd6-d7c8a9c74733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 195 tensors from /nfs/turbo/umms-indikar/shared/projects/RAG/models/Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  3072, 32064,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:              blk.0.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:            blk.0.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:         blk.0.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:            blk.0.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:           blk.1.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:            blk.1.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.1.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:            blk.1.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:         blk.1.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:          blk.10.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:           blk.10.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:             blk.10.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:           blk.10.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:        blk.10.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:           blk.10.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.11.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.11.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:             blk.11.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:           blk.11.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:        blk.11.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:           blk.11.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:          blk.12.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:           blk.12.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.12.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:           blk.12.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:        blk.12.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.12.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:          blk.13.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.13.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.13.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.13.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:        blk.13.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:           blk.13.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.14.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.14.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:             blk.14.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:           blk.14.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:        blk.14.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:           blk.14.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:          blk.15.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:           blk.15.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.15.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.15.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:        blk.15.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.15.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:          blk.16.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.16.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.16.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:           blk.16.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:        blk.16.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:           blk.16.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.17.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.17.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:             blk.17.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:           blk.17.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:        blk.17.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:           blk.17.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:          blk.18.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:           blk.18.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.18.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.18.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:        blk.18.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.18.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:          blk.19.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.19.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.19.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:           blk.19.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:        blk.19.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:           blk.19.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:           blk.2.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:            blk.2.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.2.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:            blk.2.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:         blk.2.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:            blk.2.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:          blk.20.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:           blk.20.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.20.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:           blk.20.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:        blk.20.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.20.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:        blk.21.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.3.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:            blk.3.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:              blk.3.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:            blk.3.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:         blk.3.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:            blk.3.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.4.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:            blk.4.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:              blk.4.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:            blk.4.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:         blk.4.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:            blk.4.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:           blk.5.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:            blk.5.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:              blk.5.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:            blk.5.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:         blk.5.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:            blk.5.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.6.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:            blk.6.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:              blk.6.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:            blk.6.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:         blk.6.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:            blk.6.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:           blk.7.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.7.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.7.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.7.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:         blk.7.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:            blk.7.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.8.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:            blk.8.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:              blk.8.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:            blk.8.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:         blk.8.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:            blk.8.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.9.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:            blk.9.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:              blk.9.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:            blk.9.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:         blk.9.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:            blk.9.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:                    output.weight q6_K     [  3072, 32064,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:          blk.21.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:           blk.21.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:             blk.21.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:           blk.21.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:           blk.21.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:          blk.22.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:           blk.22.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:             blk.22.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:        blk.22.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:           blk.22.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:          blk.23.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:           blk.23.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:             blk.23.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:           blk.23.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:        blk.23.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:           blk.23.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:          blk.24.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.24.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.24.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.24.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:        blk.24.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:           blk.24.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:          blk.25.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:           blk.25.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.25.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:           blk.25.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:        blk.25.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:           blk.25.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:          blk.26.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:           blk.26.ffn_down.weight q4_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:             blk.26.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:           blk.26.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:        blk.26.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.26.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:          blk.27.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:           blk.27.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.27.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.27.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:        blk.27.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:           blk.27.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:          blk.28.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:           blk.28.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:             blk.28.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.28.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:        blk.28.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:           blk.28.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:          blk.29.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:           blk.29.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:             blk.29.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:           blk.29.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:        blk.29.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.29.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:          blk.30.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:           blk.30.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.30.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.30.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:        blk.30.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:           blk.30.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:          blk.31.attn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:           blk.31.ffn_down.weight q6_K     [  8192,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:             blk.31.ffn_up.weight q4_K     [  3072, 16384,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:           blk.31.ffn_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:        blk.31.attn_output.weight q4_K     [  3072,  3072,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:           blk.31.attn_qkv.weight q5_K     [  3072,  9216,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:               output_norm.weight f32      [  3072,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "error loading model: unknown model architecture: 'phi3'\n",
      "llama_load_model_from_file: failed to load model\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/nfs/turbo/umms-indikar/shared/projects/RAG/models/Phi-3-mini-4k-instruct-q4.gguf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# path to GGUF file\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# The max sequence length to use - note that longer sequence lengths require much more resources\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The number of CPU threads to use, tailor to your system and the resulting performance\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mn_gpu_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:962\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, main_gpu, tensor_split, vocab_only, use_mmap, use_mlock, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, mul_mat_q, logits_all, embedding, offload_kqv, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format \u001b[38;5;241m=\u001b[39m chat_format\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler \u001b[38;5;241m=\u001b[39m chat_handler\n\u001b[0;32m--> 962\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_ctx()\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_nl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_nl()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:2266\u001b[0m, in \u001b[0;36mLlama.n_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mn_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the vocabulary size.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:251\u001b[0m, in \u001b[0;36m_LlamaModel.n_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mn_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llama_cpp\u001b[38;5;241m.\u001b[39mllama_n_vocab(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "  model_path   = \"/nfs/turbo/umms-indikar/shared/projects/RAG/models/Phi-3-mini-4k-instruct-q4.gguf\",  # path to GGUF file\n",
    "  n_ctx        = 4096,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "  n_threads    = 8, # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "  n_gpu_layers = 0, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aabe070f-5d60-4e27-8339-d85138a95889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: llama-cpp-python in /home/jpic/.local/lib/python3.10/site-packages (0.2.24)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/jpic/.local/lib/python3.10/site-packages (from llama-cpp-python) (4.11.0)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /home/jpic/.local/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/jpic/.local/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "! CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7cd92f6-513d-48dd-952a-542e7d758d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiohttp==3.9.5\n",
      "aiosignal==1.3.1\n",
      "alabaster @ file:///home/ktietz/src/ci/alabaster_1611921544520/work\n",
      "anaconda-client==1.11.1\n",
      "anaconda-navigator==2.4.0\n",
      "anaconda-project @ file:///opt/conda/conda-bld/anaconda-project_1660339890420/work\n",
      "anndata==0.10.7\n",
      "annotated-types==0.6.0\n",
      "anyio @ file:///tmp/build/80754af9/anyio_1644481695334/work/dist\n",
      "appdirs==1.4.4\n",
      "argon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work\n",
      "argon2-cffi-bindings @ file:///tmp/build/80754af9/argon2-cffi-bindings_1644553347904/work\n",
      "array_api_compat==1.6\n",
      "arrow @ file:///croot/arrow_1676588132104/work\n",
      "arxivscraper==0.0.5\n",
      "asgiref==3.8.1\n",
      "astroid @ file:///croot/astroid_1676904296642/work\n",
      "astropy @ file:///opt/conda/conda-bld/astropy_1657786094003/work\n",
      "asttokens @ file:///opt/conda/conda-bld/asttokens_1646925590279/work\n",
      "async-timeout==4.0.3\n",
      "atomicwrites==1.4.0\n",
      "attrs==23.2.0\n",
      "Automat @ file:///tmp/build/80754af9/automat_1600298431173/work\n",
      "autopep8 @ file:///opt/conda/conda-bld/autopep8_1650463822033/work\n",
      "Babel @ file:///croot/babel_1671781930836/work\n",
      "backcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work\n",
      "backoff==2.2.1\n",
      "backports.functools-lru-cache @ file:///tmp/build/80754af9/backports.functools_lru_cache_1618170165463/work\n",
      "backports.tempfile @ file:///home/linux1/recipes/ci/backports.tempfile_1610991236607/work\n",
      "backports.weakref==1.0.post1\n",
      "bcrypt==4.1.3\n",
      "beautifulsoup4==4.12.3\n",
      "binaryornot @ file:///tmp/build/80754af9/binaryornot_1617751525010/work\n",
      "bio==1.7.0\n",
      "biopython==1.83\n",
      "bioservices==1.11.2\n",
      "biothings-client==0.3.1\n",
      "black @ file:///opt/conda/conda-bld/black_1660237809219/work\n",
      "bleach @ file:///opt/conda/conda-bld/bleach_1641577558959/work\n",
      "blis==0.7.11\n",
      "bokeh @ file:///tmp/abs_34854e1f-d7d3-4f22-85d9-1075588e4ecdga64o0qg/croots/recipe/bokeh_1658136654619/work\n",
      "boto3==1.34.105\n",
      "botocore==1.34.105\n",
      "Bottleneck @ file:///opt/conda/conda-bld/bottleneck_1657175564434/work\n",
      "brotlipy==0.7.0\n",
      "bs4==0.0.2\n",
      "build==1.2.1\n",
      "cachetools==5.3.3\n",
      "catalogue==2.0.10\n",
      "cattrs==23.2.3\n",
      "certifi==2024.2.2\n",
      "cffi @ file:///croot/cffi_1670423208954/work\n",
      "chardet @ file:///home/builder/ci_310/chardet_1640804867535/work\n",
      "charset-normalizer==2.1.1\n",
      "chart-studio==1.1.0\n",
      "chroma-hnswlib==0.7.3\n",
      "chromadb==0.5.0\n",
      "click @ file:///tmp/build/80754af9/click_1646056706450/work\n",
      "cloudpathlib==0.16.0\n",
      "cloudpickle @ file:///tmp/build/80754af9/cloudpickle_1632508026186/work\n",
      "clyent==1.2.2\n",
      "cohere==5.5.0\n",
      "colorama @ file:///croot/colorama_1672386526460/work\n",
      "colorcet @ file:///croot/colorcet_1668084511401/work\n",
      "coloredlogs==15.0.1\n",
      "colorlog==6.8.2\n",
      "colorlover==0.3.0\n",
      "comm==0.2.2\n",
      "conda==23.1.0\n",
      "conda-build==3.23.3\n",
      "conda-content-trust @ file:///tmp/abs_5952f1c8-355c-4855-ad2e-538535021ba5h26t22e5/croots/recipe/conda-content-trust_1658126371814/work\n",
      "conda-pack @ file:///tmp/build/80754af9/conda-pack_1611163042455/work\n",
      "conda-package-handling @ file:///croot/conda-package-handling_1672865015732/work\n",
      "conda-repo-cli==1.0.27\n",
      "conda-token @ file:///Users/paulyim/miniconda3/envs/c3i/conda-bld/conda-token_1662660369760/work\n",
      "conda-verify==3.4.2\n",
      "conda_package_streaming @ file:///croot/conda-package-streaming_1670508151586/work\n",
      "confection==0.1.4\n",
      "constantly==15.1.0\n",
      "contourpy @ file:///opt/conda/conda-bld/contourpy_1663827406301/work\n",
      "cookiecutter @ file:///opt/conda/conda-bld/cookiecutter_1649151442564/work\n",
      "cryptography @ file:///croot/cryptography_1677533068310/work\n",
      "cssselect==1.2.0\n",
      "cufflinks==0.17.3\n",
      "cycler @ file:///tmp/build/80754af9/cycler_1637851556182/work\n",
      "cymem==2.0.8\n",
      "Cython==3.0.10\n",
      "cytoolz @ file:///croot/cytoolz_1667465931118/work\n",
      "daal4py==2023.0.2\n",
      "dask @ file:///tmp/abs_994957d9-ec12-411f-b953-c010f9d489d10hj3gz4k/croots/recipe/dask-core_1658513209934/work\n",
      "dataclasses-json==0.6.6\n",
      "datashader @ file:///croot/datashader_1676022884151/work\n",
      "datashape==0.5.4\n",
      "debugpy @ file:///home/builder/ci_310/debugpy_1640789504635/work\n",
      "decorator @ file:///opt/conda/conda-bld/decorator_1643638310831/work\n",
      "defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work\n",
      "Deprecated==1.2.14\n",
      "diff-match-patch @ file:///Users/ktietz/demo/mc3/conda-bld/diff-match-patch_1630511840874/work\n",
      "dill @ file:///croot/dill_1667919592856/work\n",
      "dirtyjson==1.0.8\n",
      "diskcache==5.6.3\n",
      "distlib==0.3.8\n",
      "distributed @ file:///tmp/abs_593da390-bd12-4acc-ba49-4c9993cbe8abgqg_w3rb/croots/recipe/distributed_1658520746481/work\n",
      "distro==1.9.0\n",
      "dnspython==2.6.1\n",
      "docstring-to-markdown @ file:///croot/docstring-to-markdown_1673447639232/work\n",
      "docutils @ file:///opt/conda/conda-bld/docutils_1657175430858/work\n",
      "easydev==0.13.2\n",
      "email_validator==2.1.1\n",
      "emoji==2.11.1\n",
      "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\n",
      "entrypoints @ file:///tmp/build/80754af9/entrypoints_1649908313000/work\n",
      "et-xmlfile==1.1.0\n",
      "exceptiongroup==1.2.1\n",
      "executing @ file:///opt/conda/conda-bld/executing_1646925071911/work\n",
      "faiss-cpu==1.8.0\n",
      "fake-useragent==1.5.1\n",
      "Faker==25.2.0\n",
      "fastapi==0.111.0\n",
      "fastapi-cli==0.0.3\n",
      "fastavro==1.9.4\n",
      "fastjsonschema @ file:///opt/conda/conda-bld/python-fastjsonschema_1661371079312/work\n",
      "filelock==3.14.0\n",
      "filetype==1.2.0\n",
      "filterpy==1.4.5\n",
      "flake8 @ file:///croot/flake8_1674581792275/work\n",
      "Flask @ file:///croot/flask_1671217343254/work\n",
      "flatbuffers==24.3.25\n",
      "flit_core @ file:///opt/conda/conda-bld/flit-core_1644941570762/work/source/flit_core\n",
      "fonttools==4.25.0\n",
      "frozenlist==1.4.1\n",
      "fsspec==2024.3.1\n",
      "future @ file:///croot/future_1677599870788/work\n",
      "gensim @ file:///croot/gensim_1674852436045/work\n",
      "gevent==24.2.1\n",
      "gget==0.28.4\n",
      "glob2 @ file:///home/linux1/recipes/ci/glob2_1610991677669/work\n",
      "gmpy2 @ file:///tmp/build/80754af9/gmpy2_1645455533097/work\n",
      "google-auth==2.29.0\n",
      "googleapis-common-protos==1.63.0\n",
      "gprofiler-official==1.0.0\n",
      "GPy==1.13.1\n",
      "greenlet @ file:///croot/greenlet_1670513226095/work\n",
      "grequests==0.7.0\n",
      "grpcio==1.63.0\n",
      "h11==0.14.0\n",
      "h5py @ file:///tmp/abs_4aewd3wzey/croots/recipe/h5py_1659091371897/work\n",
      "HeapDict @ file:///Users/ktietz/demo/mc3/conda-bld/heapdict_1630598515714/work\n",
      "holoviews @ file:///croot/holoviews_1676372888382/work\n",
      "httpcore==1.0.5\n",
      "httptools==0.6.1\n",
      "httpx==0.27.0\n",
      "httpx-sse==0.4.0\n",
      "huggingface-hub==0.23.0\n",
      "humanfriendly==10.0\n",
      "hvplot @ file:///croot/hvplot_1670508910345/work\n",
      "hyperlink @ file:///tmp/build/80754af9/hyperlink_1610130746837/work\n",
      "idna @ file:///croot/idna_1666125576474/work\n",
      "imagecodecs @ file:///croot/imagecodecs_1677576717595/work\n",
      "imageio @ file:///croot/imageio_1677879560821/work\n",
      "imagesize @ file:///opt/conda/conda-bld/imagesize_1657179498843/work\n",
      "imbalanced-learn @ file:///croot/imbalanced-learn_1677191565042/work\n",
      "importlib-metadata==7.0.0\n",
      "importlib_resources==6.4.0\n",
      "incremental @ file:///tmp/build/80754af9/incremental_1636629750599/work\n",
      "inflection==0.5.1\n",
      "iniconfig @ file:///home/linux1/recipes/ci/iniconfig_1610983019677/work\n",
      "intake @ file:///croot/intake_1676619859896/work\n",
      "intervaltree @ file:///Users/ktietz/demo/mc3/conda-bld/intervaltree_1630511889664/work\n",
      "ipykernel @ file:///croot/ipykernel_1671488378391/work\n",
      "ipython @ file:///croot/ipython_1676582224036/work\n",
      "ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\n",
      "ipywidgets==8.1.2\n",
      "isort @ file:///tmp/build/80754af9/isort_1628603791788/work\n",
      "itemadapter @ file:///tmp/build/80754af9/itemadapter_1626442940632/work\n",
      "itemloaders @ file:///opt/conda/conda-bld/itemloaders_1646805235997/work\n",
      "itsdangerous @ file:///tmp/build/80754af9/itsdangerous_1621432558163/work\n",
      "jedi @ file:///tmp/build/80754af9/jedi_1644315229345/work\n",
      "jeepney @ file:///tmp/build/80754af9/jeepney_1627537048313/work\n",
      "jellyfish @ file:///tmp/build/80754af9/jellyfish_1647944422765/work\n",
      "Jinja2 @ file:///croot/jinja2_1666908132255/work\n",
      "jinja2-time @ file:///opt/conda/conda-bld/jinja2-time_1649251842261/work\n",
      "jmespath @ file:///Users/ktietz/demo/mc3/conda-bld/jmespath_1630583964805/work\n",
      "joblib @ file:///croot/joblib_1666298844297/work\n",
      "json5 @ file:///tmp/build/80754af9/json5_1624432770122/work\n",
      "jsonpatch==1.33\n",
      "jsonpath-python==1.0.6\n",
      "jsonpointer==2.4\n",
      "jsonschema @ file:///croot/jsonschema_1676558650973/work\n",
      "jupyter @ file:///tmp/abs_33h4eoipez/croots/recipe/jupyter_1659349046347/work\n",
      "jupyter-console @ file:///croot/jupyter_console_1677674649642/work\n",
      "jupyter-server @ file:///croot/jupyter_server_1671707632269/work\n",
      "jupyter_client @ file:///opt/conda/conda-bld/jupyter_client_1661848916004/work\n",
      "jupyter_core @ file:///croot/jupyter_core_1676538566912/work\n",
      "jupyterlab @ file:///croot/jupyterlab_1675354114448/work\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\n",
      "jupyterlab_server @ file:///croot/jupyterlab_server_1677143054853/work\n",
      "jupyterlab_widgets==3.0.10\n",
      "keyring @ file:///home/builder/ci_310/keyring_1640884834928/work\n",
      "kiwisolver @ file:///croot/kiwisolver_1672387140495/work\n",
      "kubernetes==29.0.0\n",
      "langchain==0.1.20\n",
      "langchain-community==0.0.38\n",
      "langchain-core==0.1.52\n",
      "langchain-openai==0.1.7\n",
      "langchain-text-splitters==0.0.1\n",
      "langcodes==3.4.0\n",
      "langdetect==1.0.9\n",
      "langsmith==0.1.57\n",
      "language_data==1.2.0\n",
      "lazy-object-proxy @ file:///home/builder/ci_310/lazy-object-proxy_1640791307593/work\n",
      "libarchive-c @ file:///tmp/build/80754af9/python-libarchive-c_1617780486945/work\n",
      "line_profiler==4.1.3\n",
      "llama-index==0.10.37\n",
      "llama-index-agent-openai==0.2.5\n",
      "llama-index-cli==0.1.12\n",
      "llama-index-core==0.10.36\n",
      "llama-index-embeddings-langchain==0.1.2\n",
      "llama-index-embeddings-openai==0.1.9\n",
      "llama-index-experimental==0.1.3\n",
      "llama-index-indices-managed-llama-cloud==0.1.6\n",
      "llama-index-legacy==0.9.48\n",
      "llama-index-llms-openai==0.1.19\n",
      "llama-index-multi-modal-llms-openai==0.1.6\n",
      "llama-index-program-openai==0.1.6\n",
      "llama-index-question-gen-openai==0.1.3\n",
      "llama-index-readers-file==0.1.22\n",
      "llama-index-readers-llama-parse==0.1.4\n",
      "llama-parse==0.4.3\n",
      "llama_cpp_python==0.2.24\n",
      "llamaindex-py-client==0.1.19\n",
      "llvmlite==0.39.1\n",
      "locket @ file:///opt/conda/conda-bld/locket_1652903118915/work\n",
      "lxml @ file:///opt/conda/conda-bld/lxml_1657545139709/work\n",
      "lz4 @ file:///home/builder/ci_310/lz4_1640887640513/work\n",
      "marisa-trie==1.1.1\n",
      "Markdown @ file:///croot/markdown_1671541909495/work\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe @ file:///opt/conda/conda-bld/markupsafe_1654597864307/work\n",
      "marshmallow==3.21.2\n",
      "marshmallow-enum==1.5.1\n",
      "matplotlib @ file:///croot/matplotlib-suite_1677674301264/work\n",
      "matplotlib-inline @ file:///opt/conda/conda-bld/matplotlib-inline_1662014470464/work\n",
      "mccabe @ file:///opt/conda/conda-bld/mccabe_1644221741721/work\n",
      "mdurl==0.1.2\n",
      "mistune @ file:///home/builder/ci_310/mistune_1640791641754/work\n",
      "mkl-fft==1.3.1\n",
      "mkl-random @ file:///home/builder/ci_310/mkl_random_1641843545607/work\n",
      "mkl-service==2.4.0\n",
      "mmh3==4.1.0\n",
      "mock @ file:///tmp/build/80754af9/mock_1607622725907/work\n",
      "monotonic==1.6\n",
      "mpmath==1.2.1\n",
      "msgpack @ file:///opt/conda/conda-bld/msgpack-python_1652362659880/work\n",
      "multidict==6.0.5\n",
      "multipledispatch @ file:///home/builder/ci_310/multipledispatch_1640791758119/work\n",
      "munkres==1.1.4\n",
      "murmurhash==1.0.10\n",
      "mygene==3.2.2\n",
      "mypy-extensions==0.4.3\n",
      "mysql-connector-python==8.0.29\n",
      "natsort==8.4.0\n",
      "navigator-updater==0.3.0\n",
      "nbclassic @ file:///croot/nbclassic_1676902904845/work\n",
      "nbclient @ file:///tmp/build/80754af9/nbclient_1650290238894/work\n",
      "nbconvert @ file:///croot/nbconvert_1668450669124/work\n",
      "nbformat @ file:///croot/nbformat_1670352325207/work\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.3\n",
      "nltk==3.8.1\n",
      "notebook @ file:///croot/notebook_1668179881751/work\n",
      "notebook_shim @ file:///croot/notebook-shim_1668160579331/work\n",
      "numba @ file:///croot/numba_1670258325998/work\n",
      "numexpr @ file:///croot/numexpr_1668713893690/work\n",
      "numpy==1.26.4\n",
      "numpydoc @ file:///croot/numpydoc_1668085905352/work\n",
      "oauthlib==3.2.2\n",
      "onnxruntime==1.17.3\n",
      "openai==1.30.1\n",
      "openpyxl==3.0.10\n",
      "opentelemetry-api==1.24.0\n",
      "opentelemetry-exporter-otlp-proto-common==1.24.0\n",
      "opentelemetry-exporter-otlp-proto-grpc==1.24.0\n",
      "opentelemetry-instrumentation==0.45b0\n",
      "opentelemetry-instrumentation-asgi==0.45b0\n",
      "opentelemetry-instrumentation-fastapi==0.45b0\n",
      "opentelemetry-proto==1.24.0\n",
      "opentelemetry-sdk==1.24.0\n",
      "opentelemetry-semantic-conventions==0.45b0\n",
      "opentelemetry-util-http==0.45b0\n",
      "orjson==3.10.3\n",
      "overrides==7.7.0\n",
      "packaging==23.2\n",
      "pandas==1.5.3\n",
      "pandocfilters @ file:///opt/conda/conda-bld/pandocfilters_1643405455980/work\n",
      "panel @ file:///croot/panel_1676379587510/work\n",
      "param @ file:///croot/param_1671697421807/work\n",
      "paramz==0.9.6\n",
      "parse==1.20.1\n",
      "parsel @ file:///tmp/build/80754af9/parsel_1646739959912/work\n",
      "parso @ file:///opt/conda/conda-bld/parso_1641458642106/work\n",
      "partd @ file:///opt/conda/conda-bld/partd_1647245470509/work\n",
      "pathlib @ file:///Users/ktietz/demo/mc3/conda-bld/pathlib_1629713961906/work\n",
      "pathspec @ file:///croot/pathspec_1674681560568/work\n",
      "patsy==0.5.3\n",
      "pdf2image==1.17.0\n",
      "pdfminer==20191125\n",
      "pep8==1.7.1\n",
      "pexpect==4.9.0\n",
      "pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\n",
      "Pillow==9.4.0\n",
      "pinecone==4.0.0\n",
      "pkginfo @ file:///croot/pkginfo_1666725041340/work\n",
      "platformdirs==4.2.1\n",
      "plotly @ file:///tmp/abs_7afcdfad-dbbb-49d2-adea-186abf525c45jbnd8p95/croots/recipe/plotly_1658160053621/work\n",
      "pluggy @ file:///tmp/build/80754af9/pluggy_1648024709248/work\n",
      "ply==3.11\n",
      "pooch @ file:///tmp/build/80754af9/pooch_1623324770023/work\n",
      "posthog==3.5.0\n",
      "poyo @ file:///tmp/build/80754af9/poyo_1617751526755/work\n",
      "preshed==3.0.9\n",
      "prometheus-client @ file:///tmp/abs_d3zeliano1/croots/recipe/prometheus_client_1659455100375/work\n",
      "prompt-toolkit @ file:///croot/prompt-toolkit_1672387306916/work\n",
      "Protego @ file:///tmp/build/80754af9/protego_1598657180827/work\n",
      "protobuf==4.25.3\n",
      "psutil @ file:///opt/conda/conda-bld/psutil_1656431268089/work\n",
      "ptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\n",
      "pure-eval @ file:///opt/conda/conda-bld/pure_eval_1646925070566/work\n",
      "py @ file:///opt/conda/conda-bld/py_1644396412707/work\n",
      "pyarrow==16.1.0\n",
      "pyasn1 @ file:///Users/ktietz/demo/mc3/conda-bld/pyasn1_1629708007385/work\n",
      "pyasn1-modules==0.2.8\n",
      "pycodestyle @ file:///croot/pycodestyle_1674267221883/work\n",
      "pycosat @ file:///croot/pycosat_1666805502580/work\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\n",
      "pycryptodome==3.20.0\n",
      "pyct @ file:///croot/pyct_1675441480074/work\n",
      "pycurl==7.45.1\n",
      "pydantic==2.7.1\n",
      "pydantic_core==2.18.2\n",
      "PyDispatcher==2.0.5\n",
      "pydocstyle @ file:///croot/pydocstyle_1675221668445/work\n",
      "pyee==11.1.0\n",
      "pyerfa @ file:///home/builder/ci_310/pyerfa_1640793199719/work\n",
      "pyflakes @ file:///croot/pyflakes_1674165128613/work\n",
      "Pygments==2.18.0\n",
      "PyHamcrest @ file:///tmp/build/80754af9/pyhamcrest_1615748656804/work\n",
      "PyJWT @ file:///opt/conda/conda-bld/pyjwt_1657544592787/work\n",
      "pylint @ file:///croot/pylint_1676919896363/work\n",
      "pylint-venv @ file:///croot/pylint-venv_1673990127282/work\n",
      "pyls-spyder==0.4.0\n",
      "pymatcher==0.0.0\n",
      "pyodbc @ file:///tmp/abs_d365zrcsdp/croots/recipe/pyodbc_1659513794382/work\n",
      "pyOpenSSL @ file:///croot/pyopenssl_1677607685877/work\n",
      "pyparsing @ file:///opt/conda/conda-bld/pyparsing_1661452539315/work\n",
      "pypdf==4.2.0\n",
      "PyPika==0.48.9\n",
      "pyppeteer==2.0.0\n",
      "pyproject_hooks==1.1.0\n",
      "PyQt5-sip==12.11.0\n",
      "pyquery==2.0.0\n",
      "pyrsistent @ file:///home/builder/ci_310/pyrsistent_1640807196327/work\n",
      "PySocks @ file:///home/builder/ci_310/pysocks_1640793678128/work\n",
      "pytest==7.1.2\n",
      "python-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\n",
      "python-dotenv==1.0.1\n",
      "python-iso639==2024.4.27\n",
      "python-lsp-black @ file:///opt/conda/conda-bld/python-lsp-black_1661852031497/work\n",
      "python-lsp-jsonrpc==1.0.0\n",
      "python-lsp-server @ file:///croot/python-lsp-server_1677296760843/work\n",
      "python-magic==0.4.27\n",
      "python-multipart==0.0.9\n",
      "python-slugify @ file:///tmp/build/80754af9/python-slugify_1620405669636/work\n",
      "python-snappy @ file:///croot/python-snappy_1670943913523/work\n",
      "pytoolconfig @ file:///croot/pytoolconfig_1676315026871/work\n",
      "pytz @ file:///croot/pytz_1671697431263/work\n",
      "pyviz-comms @ file:///tmp/build/80754af9/pyviz_comms_1623747165329/work\n",
      "PyWavelets @ file:///croot/pywavelets_1670425177960/work\n",
      "pyxdg @ file:///tmp/build/80754af9/pyxdg_1603822279816/work\n",
      "PyYAML==6.0.1\n",
      "pyzmq @ file:///opt/conda/conda-bld/pyzmq_1657724186960/work\n",
      "QDarkStyle @ file:///tmp/build/80754af9/qdarkstyle_1617386714626/work\n",
      "qstylizer @ file:///croot/qstylizer_1674008522553/work/dist/qstylizer-0.2.2-py2.py3-none-any.whl\n",
      "QtAwesome @ file:///croot/qtawesome_1674008680358/work\n",
      "qtconsole @ file:///croot/qtconsole_1674008428467/work\n",
      "QtPy @ file:///opt/conda/conda-bld/qtpy_1662014892439/work\n",
      "queuelib==1.5.0\n",
      "rapidfuzz==3.9.0\n",
      "regex==2023.12.25\n",
      "requests==2.31.0\n",
      "requests-cache==1.2.0\n",
      "requests-file @ file:///Users/ktietz/demo/mc3/conda-bld/requests-file_1629455781986/work\n",
      "requests-html==0.10.0\n",
      "requests-mock==1.12.1\n",
      "requests-oauthlib==2.0.0\n",
      "retrying==1.3.4\n",
      "rich==13.7.1\n",
      "rope @ file:///croot/rope_1676675015455/work\n",
      "rsa==4.9\n",
      "Rtree @ file:///croot/rtree_1675157851263/work\n",
      "ruamel-yaml-conda @ file:///croot/ruamel_yaml_1667489728852/work\n",
      "ruamel.yaml @ file:///croot/ruamel.yaml_1666304550667/work\n",
      "ruamel.yaml.clib @ file:///croot/ruamel.yaml.clib_1666302247304/work\n",
      "s3transfer==0.10.1\n",
      "safetensors==0.4.3\n",
      "scikit-image @ file:///croot/scikit-image_1669241743693/work\n",
      "scikit-learn @ file:///croot/scikit-learn_1676911643119/work\n",
      "scikit-learn-intelex==20230228.214242\n",
      "scipy==1.10.0\n",
      "Scrapy @ file:///croot/scrapy_1677738182270/work\n",
      "seaborn @ file:///croot/seaborn_1673479180098/work\n",
      "SecretStorage @ file:///home/builder/ci_310/secretstorage_1640812904213/work\n",
      "semantic-router==0.0.42\n",
      "Send2Trash @ file:///tmp/build/80754af9/send2trash_1632406701022/work\n",
      "sentence-transformers==2.7.0\n",
      "service-identity @ file:///Users/ktietz/demo/mc3/conda-bld/service_identity_1629460757137/work\n",
      "shellingham==1.5.4\n",
      "sip @ file:///tmp/abs_44cd77b_pu/croots/recipe/sip_1659012365470/work\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\n",
      "smart-open @ file:///opt/conda/conda-bld/smart_open_1651563547610/work\n",
      "sniffio @ file:///home/builder/ci_310/sniffio_1640794799774/work\n",
      "snowballstemmer @ file:///tmp/build/80754af9/snowballstemmer_1637937080595/work\n",
      "sortedcontainers @ file:///tmp/build/80754af9/sortedcontainers_1623949099177/work\n",
      "soupsieve @ file:///croot/soupsieve_1666296392845/work\n",
      "spacy==3.7.4\n",
      "spacy-legacy==3.0.12\n",
      "spacy-loggers==1.0.5\n",
      "Sphinx @ file:///opt/conda/conda-bld/sphinx_1657784123546/work\n",
      "sphinxcontrib-applehelp @ file:///home/ktietz/src/ci/sphinxcontrib-applehelp_1611920841464/work\n",
      "sphinxcontrib-devhelp @ file:///home/ktietz/src/ci/sphinxcontrib-devhelp_1611920923094/work\n",
      "sphinxcontrib-htmlhelp @ file:///tmp/build/80754af9/sphinxcontrib-htmlhelp_1623945626792/work\n",
      "sphinxcontrib-jsmath @ file:///home/ktietz/src/ci/sphinxcontrib-jsmath_1611920942228/work\n",
      "sphinxcontrib-qthelp @ file:///home/ktietz/src/ci/sphinxcontrib-qthelp_1611921055322/work\n",
      "sphinxcontrib-serializinghtml @ file:///tmp/build/80754af9/sphinxcontrib-serializinghtml_1624451540180/work\n",
      "spyder @ file:///croot/spyder_1677776123936/work\n",
      "spyder-kernels @ file:///croot/spyder-kernels_1673292235155/work\n",
      "SQLAlchemy==2.0.30\n",
      "srsly==2.4.8\n",
      "stack-data @ file:///opt/conda/conda-bld/stack_data_1646927590127/work\n",
      "starlette==0.37.2\n",
      "statsmodels @ file:///croot/statsmodels_1676643798791/work\n",
      "striprtf==0.0.26\n",
      "suds-community==1.1.2\n",
      "sympy @ file:///croot/sympy_1668202399572/work\n",
      "tables @ file:///croot/pytables_1673967643433/work\n",
      "tabulate @ file:///opt/conda/conda-bld/tabulate_1657784105888/work\n",
      "TBB==0.2\n",
      "tblib @ file:///Users/ktietz/demo/mc3/conda-bld/tblib_1629402031467/work\n",
      "tenacity==8.3.0\n",
      "terminado @ file:///croot/terminado_1671751832461/work\n",
      "text-unidecode @ file:///Users/ktietz/demo/mc3/conda-bld/text-unidecode_1629401354553/work\n",
      "textdistance @ file:///tmp/build/80754af9/textdistance_1612461398012/work\n",
      "thinc==8.2.3\n",
      "threadpoolctl @ file:///Users/ktietz/demo/mc3/conda-bld/threadpoolctl_1629802263681/work\n",
      "three-merge @ file:///tmp/build/80754af9/three-merge_1607553261110/work\n",
      "tifffile @ file:///tmp/build/80754af9/tifffile_1627275862826/work\n",
      "tiktoken==0.7.0\n",
      "tinycss2 @ file:///croot/tinycss2_1668168815555/work\n",
      "tldextract @ file:///opt/conda/conda-bld/tldextract_1646638314385/work\n",
      "tokenizers==0.19.1\n",
      "toml @ file:///tmp/build/80754af9/toml_1616166611790/work\n",
      "tomli @ file:///opt/conda/conda-bld/tomli_1657175507142/work\n",
      "tomlkit @ file:///tmp/abs_56_0lnnq5x/croots/recipe/tomlkit_1658946880479/work\n",
      "toolz @ file:///croot/toolz_1667464077321/work\n",
      "torch==1.12.1\n",
      "tornado @ file:///home/builder/ci_310/tornado_1640795392144/work\n",
      "tqdm==4.66.4\n",
      "traitlets @ file:///croot/traitlets_1671143879854/work\n",
      "transformers==4.40.2\n",
      "TwinCell @ file:///home/jpic/TwinCell\n",
      "Twisted @ file:///tmp/abs_82802zpkox/croots/recipe/twisted_1659592759417/work\n",
      "typer==0.9.4\n",
      "types-requests==2.31.0.20240406\n",
      "typing-inspect==0.9.0\n",
      "typing_extensions==4.11.0\n",
      "ujson @ file:///opt/conda/conda-bld/ujson_1657544923770/work\n",
      "Unidecode @ file:///tmp/build/80754af9/unidecode_1614712377438/work\n",
      "unstructured==0.13.7\n",
      "unstructured-client==0.8.1\n",
      "url-normalize==1.4.3\n",
      "urllib3==2.2.1\n",
      "uvicorn==0.29.0\n",
      "uvloop==0.19.0\n",
      "virtualenv==20.26.2\n",
      "w3lib @ file:///Users/ktietz/demo/mc3/conda-bld/w3lib_1629359764703/work\n",
      "wasabi==1.1.2\n",
      "watchdog @ file:///home/builder/ci_310/watchdog_1640811339391/work\n",
      "watchfiles==0.21.0\n",
      "wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work\n",
      "weasel==0.3.4\n",
      "webencodings==0.5.1\n",
      "websocket-client @ file:///home/builder/ci_310/websocket-client_1640795866898/work\n",
      "websockets==10.4\n",
      "Werkzeug @ file:///croot/werkzeug_1671215996852/work\n",
      "whatthepatch @ file:///opt/conda/conda-bld/whatthepatch_1661795988879/work\n",
      "widgetsnbextension==4.0.10\n",
      "wrapt @ file:///tmp/abs_c335821b-6e43-4504-9816-b1a52d3d3e1eel6uae8l/croots/recipe/wrapt_1657814400492/work\n",
      "wurlitzer @ file:///home/builder/ci_310/wurlitzer_1640796026694/work\n",
      "xarray @ file:///croot/xarray_1668776594578/work\n",
      "xmltodict==0.13.0\n",
      "yapf @ file:///tmp/build/80754af9/yapf_1615749224965/work\n",
      "yarl==1.9.4\n",
      "zict==2.1.0\n",
      "zipp @ file:///croot/zipp_1672387121353/work\n",
      "zope.event==5.0\n",
      "zope.interface @ file:///home/builder/ci_310/zope.interface_1640811449297/work\n",
      "zstandard @ file:///croot/zstandard_1677013143055/work\n"
     ]
    }
   ],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71b50c-5c8f-4c90-a269-2abe1e97ef01",
   "metadata": {},
   "source": [
    "# Didn't Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a9afef-e128-4d86-bade-96042a50dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM  \n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"llmware/bling-1.4b-0.1\")  \n",
    "#model = AutoModelForCausalLM.from_pretrained(\"llmware/bling-1.4b-0.1\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360b51f0-a380-4085-9324-9a8833a37023",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '/nfs/turbo/umms-indikar/shared/projects/RAG/models/pytorch_model.bin'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/nfs/turbo/umms-indikar/shared/projects/RAG/models/pytorch_model.bin'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/nfs/turbo/umms-indikar/shared/projects/RAG/models/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch_model.bin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43m   \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path \u001b[38;5;241m+\u001b[39m model_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:804\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 804\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    806\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:637\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    636\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 637\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    654\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:462\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/nfs/turbo/umms-indikar/shared/projects/RAG/models/pytorch_model.bin'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "model_path = '/nfs/turbo/umms-indikar/shared/projects/RAG/models/'\n",
    "model_name = 'pytorch_model.bin'\n",
    "tokenizer = AutoTokenizer.from_pretrained(   model_path + model_name)  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_path + model_name)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
