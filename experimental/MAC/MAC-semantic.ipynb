{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-__73jg2gQVL"
   },
   "source": [
    "# Advanced Chunking Methods for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1eOnr6z_zLoc",
    "outputId": "b43767c1-4b53-498c-c21e-922b7d3c4696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6UuTFepgakv"
   },
   "source": [
    "Semantic chunking takes the idea of chunking documents (usually for RAG) to optimize for their end state of _vector embeddings_. Vector embeddings are retrieved based on semantic similarity, and _semantic chunking_ focuses on building chunks using the exact same mechanism.\n",
    "\n",
    "That means that we optimize our chunks for ideal retrieval performance. In essence, we are doing this by identifying the optimal chunk size that maintains a concise semantic meaning. A concise semantic meaning is important because we are compressing our chunk into a _single_ vector embedding, so if the meaning of that chunk is not concise we would, in theory, produce suboptimal embeddings that are attempting to capture multiple meanings into a single vector, which just isn't possible â€” at best, we produce a type of _average_ over the multiple meanings.\n",
    "\n",
    "In this example, we'll explore semantic chunking and see the full pipeline from raw data through to chunking and embedding our data, ready for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jwmww484f8VW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-openai 0.1.13 requires tiktoken<1,>=0.7, but you have tiktoken 0.6.0 which is incompatible.\n",
      "pyppeteer 2.0.0 requires urllib3<2.0.0,>=1.25.8, but you have urllib3 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "    semantic-router==0.0.37 \\\n",
    "    pinecone-client==3.1.0 \\\n",
    "    datasets==2.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AdnBmNm8FlQs",
    "outputId": "83cb76f1-e899-443f-9fde-f5cca30e9a4b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1484457673494a59aeeb30bd6f49d118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb05ee9863f243e5b41c598d0851a2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/217M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dadbef61a14645bbb9591c6dfbd62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2673 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'content', 'references'],\n",
       "    num_rows: 2673\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"jamescalam/ai-arxiv2\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIruvzvuGJTp"
   },
   "source": [
    "Let's initialize our encoder which will be used to identify semantically concise splits in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FmYefuEmGT4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machoi/.local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/machoi/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\"OpenAI API key: \")\n",
    "\n",
    "encoder = OpenAIEncoder(name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Qnzvvog9Hh9U"
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for RollingWindowSplitter\nencoder -> name\n  field required (type=value_error.missing)\nencoder -> score_threshold\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msemantic_router\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m      4\u001b[0m logger\u001b[38;5;241m.\u001b[39msetLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# reduce logs from splitter\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m splitter \u001b[38;5;241m=\u001b[39m RollingWindowSplitter(\n\u001b[1;32m      7\u001b[0m     encoder\u001b[38;5;241m=\u001b[39mencoder,\n\u001b[1;32m      8\u001b[0m     dynamic_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m     min_split_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     10\u001b[0m     max_split_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     11\u001b[0m     window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     12\u001b[0m     plot_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# set this to true to visualize chunking\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     enable_statistics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# to print chunking stats\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/semantic_router/splitters/rolling_window.py:52\u001b[0m, in \u001b[0;36mRollingWindowSplitter.__init__\u001b[0;34m(self, encoder, name, threshold_adjustment, dynamic_threshold, window_size, min_split_tokens, max_split_tokens, split_tokens_tolerance, plot_splits, enable_statistics)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     41\u001b[0m     encoder: BaseEncoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     enable_statistics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     51\u001b[0m ):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name\u001b[38;5;241m=\u001b[39mname, encoder\u001b[38;5;241m=\u001b[39mencoder)\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculated_threshold: \u001b[38;5;28mfloat\u001b[39m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m encoder\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for RollingWindowSplitter\nencoder -> name\n  field required (type=value_error.missing)\nencoder -> score_threshold\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from semantic_router.splitters import RollingWindowSplitter\n",
    "from semantic_router.utils.logger import logger\n",
    "\n",
    "logger.setLevel(\"WARNING\")  # reduce logs from splitter\n",
    "\n",
    "splitter = RollingWindowSplitter(\n",
    "    encoder=encoder,\n",
    "    dynamic_threshold=True,\n",
    "    min_split_tokens=100,\n",
    "    max_split_tokens=500,\n",
    "    window_size=2,\n",
    "    plot_splits=True,  # set this to true to visualize chunking\n",
    "    enable_statistics=True  # to print chunking stats\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "whJG5gbgomiG",
    "outputId": "e5e9cad1-425d-449a-ca25-f62d147f6827"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m2024-07-05 13:27:16 WARNING semantic_router.utils.logger Retrying in 2 seconds...\u001b[0m\n",
      "\u001b[33m2024-07-05 13:27:23 WARNING semantic_router.utils.logger Retrying in 4 seconds...\u001b[0m\n",
      "\u001b[33m2024-07-05 13:27:33 WARNING semantic_router.utils.logger Retrying in 8 seconds...\u001b[0m\n",
      "\u001b[33m2024-07-05 13:27:53 WARNING semantic_router.utils.logger Retrying in 16 seconds...\u001b[0m\n",
      "\u001b[33m2024-07-05 13:28:28 WARNING semantic_router.utils.logger Retrying in 32 seconds...\u001b[0m\n",
      "\u001b[33m2024-07-05 13:29:35 WARNING semantic_router.utils.logger Retrying in 64 seconds...\u001b[0m\n",
      "\u001b[31m2024-07-05 13:29:35 ERROR semantic_router.utils.logger Error encoding documents ['4 2 0 2', 'n a J 8 ] G L . s c [', '1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a', '# Mixtral of Experts', 'Albert Q.', 'Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃƒÂ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃƒÂ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃƒÂ©e Lacroix, William El Sayed', 'Abstract', 'We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.', 'Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts).', 'For every token, at each layer, a router network selects two experts to process the current state and combine their outputs.', 'Even though each token only sees two experts, the selected experts can be different at each timestep.', 'As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference.', 'Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks.', 'In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks.', 'We also provide a model fine- tuned to follow instructions, Mixtral 8x7B Ã¢', 'Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B Ã¢', 'chat model on human bench- marks.', 'Both the base and instruct models are released under the Apache 2.0 license.', 'Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/', '# Introduction', 'In this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0.', 'Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks.', 'As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.', 'Mixtral is a sparse mixture-of-experts network.', 'It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters.', 'At every layer, for every token, a router network chooses two of these groups (the Ã¢', 'expertsÃ¢', ') to process the token and combine their output additively.', 'This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.', 'Mixtral is pretrained with multilingual data using a context size of 32k tokens.', 'It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks.', 'In particular,', 'Mixture of Experts Layer i gating inputs af outputs router expert', 'Figure 1:', 'Mixture of Experts Layer.', 'Each input vector is assigned to 2 of the 8 experts by a router.', 'The layerÃ¢', 's output is the weighted sum of the outputs of the two selected experts.', 'In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture.', 'Mixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains.', 'Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence.', 'We also present Mixtral 8x7B Ã¢', 'Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25].', 'Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B Ã¢', 'chat model on human evaluation benchmarks.', 'Mixtral Ã¢', 'Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD.', 'We release both Mixtral 8x7B and Mixtral 8x7B Ã¢', 'Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications.', 'To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference.', 'Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.', '# 2 Architectural details', 'Mixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1).', 'The model architecture parameters are summarized in Table 1.', 'Parameter Value', 'dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts', '# 2.1 Sparse Mixture of Experts', 'We present a brief overview of the Mixture of Experts layer (Figure 1).', 'For a more in-depth overview, see [12].', 'The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkÃ¢', 's output. i.e. given n expert networks {E0, Ei, ..., EnÃ¢', '1}, the output of the expert layer is given by:', 'Table 1:', 'Model architecture.', '# j nÃ¢', 'G(x)i Ã‚Â· Ei(x). i=0', 'Here, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network.', 'If the gating vector is sparse, we can avoid computing the outputs of experts whose gates are zero.', 'There are multiple alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the Top-K logits of a linear layer [28].', 'We use', 'G(x) := Softmax(TopK(x Ã‚Â· Wg)),', 'where (TopK(Ã¢', '))i := Ã¢', 'i if Ã¢', 'i is among the top-K coordinates of logits Ã¢', 'Ã¢', 'Rn and (TopK(Ã¢', '))i := Ã¢', 'Ã¢', 'otherwise.', 'The value of K Ã¢', 'the number of experts used per token Ã¢', 'is a hyper-parameter that modu- lates the amount of compute used to process each token.', 'If one increases n while keeping K fixed, one', '# 1https://mistral.ai/news/mixtral-of-experts/', '2', '4096 32 128 14336 32 8 32768 32000 8 2', 'can increase the modelÃ¢', 's parameter count while keeping its computational cost effectively constant.', 'This motivates a distinction between the modelÃ¢', 's total parameter count (commonly referenced as the sparse parameter count), which grows with n, and the number of parameters used for processing an individual token (called the active parameter count), which grows with K up to n.', 'MoE layers can be run efficiently on single GPUs with high performance specialized kernels.', 'For example, Megablocks [13] casts the feed-forward network (FFN) operations of the MoE layer as large sparse matrix multiplications, significantly enhancing the execution speed and naturally handling cases where different experts get a variable number of tokens assigned to them.', 'Moreover, the MoE layer can be distributed to multiple GPUs through standard Model Parallelism techniques, and through a particular kind of partitioning strategy called Expert Parallelism (EP) [28].', 'During the MoE layerÃ¢', 's execution, tokens meant to be processed by a specific expert are routed to the corresponding GPU for processing, and the expertÃ¢', 's output is returned to the original token location.', 'Note that EP introduces challenges in load balancing, as it is essential to distribute the workload evenly across the GPUs to prevent overloading individual GPUs or hitting computational bottlenecks.', 'In a Transformer model, the MoE layer is applied independently per token and replaces the feed-forward (FFN) sub-block of the transformer block.', 'For Mixtral we use the same SwiGLU architecture as the expert function Ei(x) and set K = 2.', 'This means each token is routed to two SwiGLU sub-blocks with different sets of weights.', 'Taking this all together, the output y for an input token x is computed as:', 'n-1 y= Ss Softmax(Top2(a - W,)); - SwiGLU;(a). i=0', 'This formulation is similar to the GShard architecture [21], with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token.', '# 3 Results', 'We compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison.', 'We measure performance on a wide variety of tasks categorized as follow:', 'Ã¢', 'Â¢ Commonsense Reasoning (0-shot):', 'Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]', 'World Knowledge (5-shot):', 'NaturalQuestions [20], TriviaQA [19] Ã¢', 'Â¢ Reading Comprehension (0-shot):', 'BoolQ [7], QuAC [5] Ã¢', 'Â¢ Math:', 'GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4 Ã¢', 'Â¢ Code:', 'Humaneval [4] (0-shot) and MBPP [1] (3-shot) Ã¢', 'Â¢ Popular aggregated results:', 'MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34]', '(3-5-shot, English multiple-choice questions only)', '80 SE Mistral 78 = LLaMA27B = Sl LLaMA134B, jam Mistral 78 = LlaMA27B Ss LLAMA 1348, cee Mixtral 8x78 Sm LLaMA213BÃ‚Â° mmm LLaMA2 70B je Mixtral 8x78 mm LlaMA2138 lm LLaMA2 708 70 50 60 50 20 40 10 BH Code MMU Knowledge Reasoning Ã¢', 'Comprehension AGI Eval Math Ã¢', 'Accuracy (%)', 'Figure 2:', 'Performance of Mixtral and different Llama models on a wide range of benchmarks.', 'All models were re-evaluated on all metrics with our evaluation pipeline for accurate comparison.', 'Mixtral outperforms or matches Llama 2 70B on all benchmarks.', 'In particular, it is vastly superior in mathematics and code generation.', '3', 'Active Params MMLU HellaS WinoG PIQA Arc-e Arc-c NQ TriQA HumanE MBPP Math GSM8K 7B 44.4% 77.1% 69.5% 77.9% 68.7% 43.2% 17.5% 56.6% 11.6% 26.1% 3.9% 16.0% 13B 55.6% 80.7% 72.9% 80.8% 75.2% 48.8% 16.7% 64.0% 18.9% 35.4% 6.0% 34.3% 33B 56.8% 83.7% 76.2% 82.2% 79.6% 54.4% 24.1% 68.5% 25.0% 40.9% 8.4% 44.1% 70B 69.9% 85.4% 80.4% 82.6% 79.9% 56.5% 25.4% 73.0% 29.3% 49.8% 13.8% 69.6% 7B 62.5% 81.0% 74.2% 82.2% 80.5% 54.9% 23.2% 62.5% 26.2% 50.2% 12.7% 50.0% 13B 70.6% 84.4% 77.2% 83.6% 83.1% 59.7% 30.6% 71.5% 40.2% 60.7% 28.4% 74.4%', 'Table 2:', 'Comparison of Mixtral with Llama.', 'Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.', '70 Mixtral 8x7B. Ã¢', 'Mixtral 8x7B Mixtral 8x7B 355 =o = Es & E60!', 'Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Ã‚Â§ 30 5 eo Ã¢', '= Mistral Ã‚Â° 20 Ã¢', 'e LlaMA2 78 (138 348 70B 7B (138 348 708 7B Ã‚Â«13B 34B 708 Active Params Active Params Active Params', 'Figure 3:', 'Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B).', 'Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters.', 'It is also vastly superior to Llama 2 70B on code and math.', 'Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2.', 'Figure 2 compares the performance of Mixtral with the Llama models in different categories.', 'Mixtral surpasses Llama 2 70B across most metrics.', 'In particular, Mixtral displays a superior performance in code and mathematics benchmarks.', 'Size and Efficiency.', 'We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsÃ¢', 'efficiency in the cost-performance spectrum (see Figure 3).', 'As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token.', 'With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.', 'Note that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization.', 'The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B.', 'As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device.', 'They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.', 'Comparison with Llama 2 70B and GPT-3.5.', 'In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5.', 'We observe that Mixtral performs similarly or above the two other models.', 'On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B).', 'For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.', '2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.', '4', 'LLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30', '# Mixtral 8x7B', 'Table 3:', 'Comparison of Mixtral with Llama 2 70B and GPT-3.5.', 'Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.', 'Evaluation Differences.', 'On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.', '# 3.1 Multilingual benchmarks', 'Compared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining.', 'The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English.', 'In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.', 'Active Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%', 'Table 4:', 'Comparison of Mixtral with Llama on Multilingual Benchmarks.', 'On ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages:', 'French, German, Spanish, and Italian.', '# 3.2 Long range performance', 'To assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a passkey inserted randomly in a long prompt.', 'Results in Figure 4 (Left) show that Mixtral achieves a 100% retrieval accuracy regardless of the context length or the position of passkey in the sequence.', 'Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the context increases.', 'Passkey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey Loc', '3.8 Ã¢', 'Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length', 'Passkey Performance ry 3.8 Ã¢', 'Mixtral_8x7B 3.5 0.8 32 > 0.6 $3.0 i] 228 04 fos 0.2 a 2.0 0.0 OK 4K 8K 12K 16K 20K 24K 28K 0 5k 10k 15k 20k 25k 30k Seq Len Context length', 'Figure 4:', 'Long range performance of Mixtral. (Left) Mixtral has 100% retrieval accuracy of the Passkey task regardless of the location of the passkey and length of the input sequence. (Right) The perplexity of Mixtral on the proof-pile dataset decreases monotonically as the context length increases.', '5', '# 3.3 Bias Benchmarks', 'To identify possible flaws to be corrected by fine-tuning / preference modeling, we measure the base model performance on Bias Benchmark for QA (BBQ) [24] and Bias in Open-Ended Language Generation Dataset (BOLD) [10].', 'BBQ is a dataset of hand-written question sets that target attested social biases against nine differ- ent socially-relevant categories: age, dis- ability status, gender identity, nationality, physical appearance, race/ethnicity, religion, socio-economic status, sexual orientation.', 'BOLD is a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains.', 'Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Ã‚Â± std) gender profession religious_ideology political_ideology race 0.293 Ã‚Â± 0.073 0.218 Ã‚Â± 0.073 0.188 Ã‚Â± 0.133 0.149 Ã‚Â± 0.140 0.232 Ã‚Â± 0.049 0.323 Ã‚Â±0.045 0.243 Ã‚Â± 0.087 0.144 Ã‚Â± 0.089 0.186 Ã‚Â± 0.146 0.232 Ã‚Â± 0.052', 'Figure 5:', 'Bias Benchmarks.', 'Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).', 'We benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5.', 'Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%).', 'For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group.', 'Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.', '# Instruction Fine-tuning', 'We train Mixtral Ã¢', 'Instruct using supervised fine-tuning (SFT) on an instruction dataset followed by Direct Preference Optimization (DPO) [25] on a paired feedback dataset.', 'Mixtral Ã¢', 'Instruct reaches a score of 8.30 on MT-Bench [33] (see Table 2), making it the best open-weights model as of December 2023.', 'Independent human evaluation conducted by LMSys is reported in Figure 63 and shows that Mixtral Ã¢', 'Instruct outperforms GPT-3.5-Turbo, Gemini Pro, Claude-2.1, and Llama 2 70B chat.', 'vs Arena Elo rating 1 MT-bench (score) License 1243 9.32 Proprietary 1192 8.96 Proprietary 1158 9.18 Proprietary Glaude-4 1149 7.9 Proprietary Claude-2.0 1131 8.06 Proprietary 1121 eS) Apache 2.0 Glaude-2.4 1117 8.18 Proprietary GPT-3..5-Turbo-9613 1117 8.39 Proprietary Gemini..Pro 1141 Proprietary Glas ta 1110 7.85 Proprietary Tulu-2-0P0-708 1110 7.89 AI2 ImpACT Low-risk Yi-34B-Chat 1110 Yi License GPT-3.5:Turbo-0314 1105 7.94 Proprietary Llama-2-79b-chat 1077 6.86 Llama 2 Community', 'Figure 6:', 'LMSys Leaderboard. (Screenshot from Dec 22, 2023) Mixtral 8x7B Instruct v0.1 achieves an Arena Elo rating of 1121 outperforming Claude-2.1 (1117), all versions of GPT-3.5-Turbo (1117 best), Gemini Pro (1111), and Llama-2-70b-chat (1077).', 'Mixtral is currently the best open-weights model by a large margin.', '3https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard', '6', '# 5 Routing analysis', 'In this section, we perform a small analysis on the expert selection by the router.', 'In particular, we are interested to see if during training some experts specialized to some specific domains (e.g. mathematics, biology, philosophy, etc.).', 'To investigate this, we measure the distribution of selected experts on different subsets of The Pile validation dataset [14].', 'Results are presented in Figure 7, for layers 0, 15, and 31 (layers 0 and 31 respectively being the first and the last layers of the model).', 'Surprisingly, we do not observe obvious patterns in the assignment of experts based on the topic.', 'For instance, at all layers, the distribution of expert assignment is very similar for ArXiv papers (written in Latex), for biology (PubMed Abstracts), and for Philosophy (PhilPapers) documents.', 'Only for DM Mathematics we note a marginally different distribution of experts.', 'This divergence is likely a consequence of the datasetÃ¢', 's synthetic nature and its limited coverage of the natural language spectrum, and is particularly noticeable at the first and last layers, where the hidden states are very correlated to the input and output embeddings respectively.', 'This suggests that the router does exhibit some structured syntactic behavior.', 'Figure 8 shows examples of text from different domains (Python code, mathematics, and English), where each token is highlighted with a background color corresponding to its selected expert.', 'The figure shows that words such as Ã¢', 'selfÃ¢', 'in Python and Ã¢', 'QuestionÃ¢', 'in English often get routed through the same expert even though they involve multiple tokens.', 'Similarly, in code, the indentation tokens are always assigned to the same experts, particularly at the first and last layers where the hidden states are more correlated to the input and output of the model.', 'We also note from Figure 8 that consecutive tokens are often assigned the same experts.', 'In fact, we observe some degree of positional locality in The Pile datasets.', 'Table 5 shows the proportion of con- secutive tokens that get the same expert assignments per domain and layer.', 'The proportion of repeated', '0.20 0.15 0.10 0.05 layer: 15 0.20 0.15 0.10 0.05 layer: 31 Selection proportion 0.20 0.15 0.10 0.05 Expert ID | | ArXiv | Github | | PhilPapers | StackExchange | | DM Mathematics | | Gutenberg | | PubMed Abstracts | | Wikipedia (en)', 'Figure 7:', 'Proportion of tokens assigned to each expert on different domains from The Pile dataset for layers 0, 15, and 31.', 'The gray dashed vertical line marks 1/8, i.e. the proportion expected with uniform sampling.', 'Here, we consider experts that are either selected as a first or second choice by the router.', 'A breakdown of the proportion of assignments done in each case cane be seen in Figure 9 in the Appendix.', '7', 'Layer 0 First choice Layer 15 Layer 31 Layer 0 First or second choice Layer 15 Layer 31 ArXiv DM Mathematics Github Gutenberg PhilPapers PubMed Abstracts StackExchange Wikipedia (en) 14.0% 14.1% 14.9% 13.9% 13.6% 14.2% 13.6% 14.4% 27.9% 28.4% 28.1% 26.1% 25.3% 24.6% 27.2% 23.6% 22.7% 19.7% 19.7% 26.3% 22.1% 22.0% 23.6% 25.3% 46.5% 44.9% 49.9% 49.5% 46.9% 48.6% 48.2% 49.8% 62.3% 67.0% 66.9% 63.1% 61.9% 61.6% 64.6% 62.1% 52.9% 44.5% 49.2% 52.2% 51.3% 51.8% 53.6% 51.8%', 'Table 5:', 'Percentage of expert assignment repetitions.', 'We evaluate the proportion of times the same expert is assigned to a token i and its following token i+1.', 'We report whether the first chosen expert is the same, or whether the same expert is observed as first or second choice in consecutive tokens.', 'For reference, the expected proportion of repetitions in the case of random assignments is 1 5 7 Ã¢', '46% for Ã¢', 'First and second choiceÃ¢', '.', 'Repetitions at the first layer are close to random, but are significantly higher at layers 15 and 31.', 'The high number of repetitions shows that expert choice exhibits high temporal locality at these layers.', 'consecutive assignments is significantly higher than random for higher layers.', 'This has implications in how one might optimize the model for fast training and inference.', 'For example, cases with high locality are more likely to cause over-subscription of certain experts when doing Expert Parallelism.', 'Conversely, this locality can be leveraged for caching, as is done in [11].', 'A more complete view of these same expert frequency is provided for all layers and across datasets in Figure 10 in the Appendix.', '# 6 Conclusion', 'In this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models.', 'Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks.', 'Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B).', 'We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license.', 'By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.', 'Layer 0 Layer 15 Layer 31 class MoeLayer(nn.', 'Module) : Ã¢', \"init__(self, experts//List [nn.Modutel,) | Super (V7 init assert len(experts) > 0 self. experts = nn.ModuleList((experts) self. gate = gate self.args = moe_args def forward(self, inputs: torch.Tensor): inputs _squashed = inputs. view(-1,_ inputs.| gate_logits = self.gatel inputs_squashed) weights, selected_experts = torch. topk( gate_logits, Self-args.nun_experts_ÃƒÂ© weights! = nri.|funct ional softinax'( weights, din=1, dtype=torch. float, ).type_as|(inputs) results| = torch. zeros_ ike! linputs_squashe for i, expert in enunerate(self. experts): batch_idx,! nth_expert = torch. wnere( results [batch_idx] += weights [batch_i input s_squashed [batch_idx] ) return resutts:.view las{(inputs) class NoeLayer (nn.\", 'Module) = def _ init__(self, experts!', \"List'{nri.Modulelly Super (Tz init_t assert len (experts) > 9) self.experts = nn.\", 'ModuleList((experits)) def forward(self, inputs: torch.', 'Tensor)?! inputs_squashed = inputs.View(-1) inputs) gate_logits = self.gatel inputs_squashed) weights, selected_experts = torch. topk( getellogits, self.argssnun_experts pe weightsÃ¢', '= nn. functionallsoftmax(Ã‚Â® Weights, dtypextorch. floaty ) type_as (inputs) results| = torch. zerdsillikel(input siiequashe| for i, expert in enumerate (self. experts): batch idx, nth_expert = torch.where(s results [batch_idx] += weights [batch_iÃ‚Â¢ inputs|_squashed[batch idx], y return resultsiiview jas (inputs) class| MoeLayer(nn.', 'Module): def init__(self, expertsÃ¢', 'List|fifi.Modulel) Super(Ve_init_O) assert len(experts) > 0 self, experts = nn.ModuleListl(@xperits)) self. gate = gate Self.args = moe_args def forward(self, inputs: torch.', 'Tensor): inputs_squashed = inputs.view(=1, inputs) gate_logits = self.gate( inputs_squashed) weights, selected_experts = torch. topk( gate_logits, self.argssfum_experts_pe weights) nni.unct iorial.isoftinax( YP Yiitype_as (inputs) results = torch. zerosillikel(inputslisquashe| for i, expert in enunerate(self.experts): batch_idx, nth_expert = torch.where(s results [batch_idx] += weights [batch_iÃ‚Â¢ inputs_squashed [batch_idx] ) return) results\\\\iviewilas|(inputs)) Tuestiond] Solve Ã¢', 'AINr 27K SLIT! and SORT, lanswers 4 Question?Ã¢', 'Calculate Baiasoazusaaly 4111270 iAnswer: -841469015.544 (Question!', 'LetÃ¢', 'x(gy = 94g # Hl Let! q(clJ= Zee #] IAnswer:', 'S4ea - 30 Ã¢', 'Question#!', 'Solve Azer Ã‚Â¥ 27HE = Ate and 1505 lanswer:) 4 Calculate ~eaieseiaz. saa Ã‚Â¥ 417127. ~841469015.544 Ã¢', 'Answer: (Questor Ã¢', 'Answer: etÃ¢', 'x(q) = 9*g Ã‚Â¥ Wl Let! ql)! = 2eele Sara Ã¢', '30 question Solve -42Ã‚Â¥e1E B7eC= Ã¢', 'Ad67 and 130%] answers \\\\questionÃ‚Â®| calculate savesona2.saq + auaz7.', 'Answer: -847469015.544 Ã¢', 'OÃ‚Â¥o)H A Let q(el = (questiond!', 'Let! x(a) = awed | Answers 54a ~ Ã¢', 'A model airplane flies stower when flying into tt jwind and faster with wind at its back. when Launcl Iright angles to the wind,Ã¢', 'cross wind,| its groun Icompared with! flying in still air is (A) the same (B) greater (C) less (0)! either! grea lor less dependingÃ¢', 'on wind speed i nodelaitp ane) URE slover when flying into eH lind and faster with wind at its back.', 'When) launch Tight angles to the wind, a cross wind,. its) grounc Compared with Ã¢', 'lying in stitt air is (A) the same (18) greater) (C) less (D)! either grea lor less depending on wind speed H model airplane flies slower! when flying inte th wind and faster with wind at its backÃ¢', '.', 'When Launcl [right angles to the wind, a cross wind, its grounc Icompared with flying in still air is (A) the sane (B) greater (C) less (0)! either gree jor less depending on wind speed', 'Figure 8:', 'Text samples where each token is colored with the first expert choice.', 'The selection of experts appears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.', '8', '# Acknowledgements', 'We thank the CoreWeave and Scaleway teams for technical support as we trained our models.', 'We are grateful to NVIDIA for supporting us in integrating TensorRT-LLM and Triton and working alongside us to make a sparse mixture of experts compatible with TensorRT-LLM.', '# References', '[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.', 'Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.', '[2] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.', 'Llemma:', 'An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023.', '[3] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.', 'Piqa:', 'Reasoning about phys- ical commonsense in natural language.', 'In Proceedings of the AAAI conference on artificial intelligence, pages 7432Ã¢', '7439, 2020.', '[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al.', 'Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.', '[5] Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer.', 'Quac:', 'Question answering in context. arXiv preprint arXiv:1808.07036, 2018.', '[6] Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al.', 'Unified scaling laws for routed language models.', 'In International Conference on Machine Learning, pages 4057Ã¢', '4086.', 'PMLR, 2022.', '[7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.', 'Boolq:', 'Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.', '[8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.', 'Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.', '[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.', 'Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.', '[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta.', 'Bold:', 'Dataset and metrics for measuring biases in open-ended language generation.', 'In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862Ã¢', '872, 2021.', '[11] Artyom Eliseev and Denis Mazur.', 'Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.', '[12] William Fedus, Jeff Dean, and Barret Zoph.', 'A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.', '[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia.', 'Megablocks:', 'Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.', '[14] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.', 'The pile:', 'An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.', '[15] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder, Lichan Hong, and Ed Chi.', 'Dselect-k:', 'Differentiable selection in the mixture of experts with applications to multi-task learning.', 'Advances in Neural Information Processing Systems, 34:29335Ã¢', '29347, 2021.', '9', '[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.', 'Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.', '[17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.', 'Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.', '[18] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.', 'Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.', '[19] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.', 'Triviaqa:', 'A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.', '[20] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al.', 'Natural questions: a benchmark for question answering research.', 'Transactions of the Association for Computational Linguistics, pages 453Ã¢', '466, 2019.', '[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.', 'Gshard:', 'Scaling giant models with condi- tional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.', '[22] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.', 'Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.', '[23] Amirkeivan Mohtashami and Martin Jaggi.', 'Landmark attention:', 'Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300, 2023.', '[24] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp- son, Phu Mon Htut, and Samuel R Bowman.', 'Bbq:', 'A hand-built bias benchmark for question answering. arXiv preprint arXiv:2110.08193, 2021.', '[25] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn.', 'Direct preference optimization:', 'Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.', '[26] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.', 'Winogrande:', 'An adversarial winograd schema challenge at scale.', 'Communications of the ACM, pages 99Ã¢', '106, 2021.', '[27] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.', 'Socialiqa:', 'Com- monsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019.', '[28] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.', 'Outrageously large neural networks:', 'The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.', '[29] Mirac Suzgun, Nathan Scales, Nathanael SchÃƒÂ¤rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei.', 'Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.', '[30] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.', 'Commonsenseqa:', 'A ques- tion answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.', '[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ã…', 'ukasz Kaiser, and Illia Polosukhin.', 'Attention is all you need.', 'Advances in neural information processing systems, 30, 2017.', '[32] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.', 'Hellaswag:', 'Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.', '[33] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.', 'Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.', '10', '[34] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.', 'Agieval:', 'A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.', '[35] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al.', 'Mixture-of-experts with expert choice routing.', 'Advances in Neural Information Processing Systems, 35:7103Ã¢', '7114, 2022.', '11', '# Either choice', '0', 'Layer -- 0.3 0.2 0 Layer 0 -- First choice 0.3 Layer 0 -- Second choice 0.3 < 2 t Layer 15 -- First choice fe} Q 0.3 Ã‚Â° a 0.2 el (el er rere! ie it len | ie} o 0 v Layer 15 -- Second choice 8 03 0.2 0 Layer 31 -- Either choice', '# Expert ID', 'ArXiv Github PhilPapers.', 'StackExchange |_| | |_| | | DM Mathematics | Gutenberg || PubMed Abstracts | Wikipedia (en)', 'Figure 9:', 'Proportion of tokens assigned to each expert on different subsets from The Pile dataset, separated by whether the expert was selected as first or second choice, or either.', 'The Ã¢', 'Either choiceÃ¢', 'case is equivalent to Figure 7.', 'The gray dashed vertical line marks 1', '12', 'First choice 9 w is) Ã‚Â° N a Ã‚Â° N is) Ã‚Â° An wu 0.7 0.6 Proportion of repeated assignments 0.5 Layer source Ã¢', 'e ArXiv Ã¢', 'eÃ¢', 'DM Mathematics Ã¢', 'e Github Ã¢', 'eÃ¢', 'Gutenberg Ã¢', 'eÃ¢', 'PhilPapers Ã¢', 'eÃ¢', 'PubMed Ã¢', 'e- StackExchange Ã¢', 'e-Ã¢', 'Wikipedia (en)', '# Abstracts', 'Figure 10:', 'Repeated consecutive assignments per MoE layer.', 'Repeated assignments occur a lot more often than they would with uniform assignments (materialized by the dashed lines).', 'Patterns are similar across datasets with less repetitions for DM Mathematics.', '13']: No embeddings returned. Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No embeddings returned. Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m splits \u001b[38;5;241m=\u001b[39m splitter([dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/semantic_router/splitters/rolling_window.py:85\u001b[0m, in \u001b[0;36mRollingWindowSplitter.__call__\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m     79\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingle document exceeds the maximum token limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_split_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplitting to sentences before semantically splitting.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m         )\n\u001b[1;32m     84\u001b[0m     docs \u001b[38;5;241m=\u001b[39m split_to_sentences(docs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 85\u001b[0m encoded_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_documents(docs)\n\u001b[1;32m     86\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_similarity_scores(encoded_docs)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_threshold:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/semantic_router/splitters/rolling_window.py:117\u001b[0m, in \u001b[0;36mRollingWindowSplitter._encode_documents\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    115\u001b[0m batch_docs \u001b[38;5;241m=\u001b[39m docs[i : i \u001b[38;5;241m+\u001b[39m max_docs_per_batch]\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     batch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(batch_docs)\n\u001b[1;32m    118\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mextend(batch_embeddings)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/semantic_router/encoders/openai.py:111\u001b[0m, in \u001b[0;36mOpenAIEncoder.__call__\u001b[0;34m(self, docs, truncate)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m embeds\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeds, CreateEmbeddingResponse)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m embeds\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    109\u001b[0m ):\n\u001b[1;32m    110\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturned embeddings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo embeddings returned. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m [embeds_obj\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m embeds_obj \u001b[38;5;129;01min\u001b[39;00m embeds\u001b[38;5;241m.\u001b[39mdata]\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[0;31mValueError\u001b[0m: No embeddings returned. Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "splits = splitter([dataset[\"content\"][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Z3uH4xtEdNLB",
    "outputId": "e22020db-0da6-4374-8d05-00b7aff06b2a"
   },
   "outputs": [],
   "source": [
    "splits = splitter([dataset[\"content\"][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G7WTu-tJdTy4",
    "outputId": "d8f8912f-b642-4208-8d55-2ecb99d24202"
   },
   "outputs": [],
   "source": [
    "splits = splitter([dataset[\"content\"][2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuXuYaESFvlK"
   },
   "source": [
    "We can view a few of our splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OlwiRB8DjSm",
    "outputId": "ae805297-88a7-47b8-beed-64d00d0e9dc5"
   },
   "outputs": [],
   "source": [
    "splitter.print(splits[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUNlTCrZGVU7"
   },
   "source": [
    "The actual structure of splits we get from this function is contained within a `DocumentSplit` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzTGXcrjGcZm",
    "outputId": "eb74e98a-08fc-418b-a40d-135a9d64dc63"
   },
   "outputs": [],
   "source": [
    "splits[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2fKyuvcGknI"
   },
   "source": [
    "In here, our chunks are separated into a list of _sentence-like_ strings. To output our chunk from this we use the `.content` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "oIkAZJ2THPTj",
    "outputId": "258a58ab-b397-4112-af6f-27cc17b673cd"
   },
   "outputs": [],
   "source": [
    "splits[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ICebxV2ED7B"
   },
   "source": [
    "When creating embeddings we can include additional contextual information to improve retrieval performance. One performant but simple method for this is to prefix titles or headers to our chunks â€” this works particularly well for more structured documents like PDFs.\n",
    "\n",
    "We can define a function that will do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AV9JyVyAEEOT",
    "outputId": "bd63a072-1531-4761-a013-de6c5a3bea1f"
   },
   "outputs": [],
   "source": [
    "def build_chunk(title: str, content: str):\n",
    "    return f\"# {title}\\n{content}\"\n",
    "\n",
    "# we use it like:\n",
    "title = dataset[2][\"title\"]\n",
    "for s in splits[:3]:\n",
    "    print(\"---\")\n",
    "    print(build_chunk(title=title, content=s.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYVu9HKlLOGy"
   },
   "source": [
    "These chunks are all we need to create our embeddings, but we don't necessarily want to feed the same information into our LLM that has been fed into our embedding model. In our example, we will add a little more structure to what the LLM sees, and also a little more context.\n",
    "\n",
    "To achieve this, we will keep the `title` and `content` fields separate in metadata so that during retrieval we can format them in a way that makes sense for us.\n",
    "\n",
    "Additionally, we may also want to pull in some context from surrounding chunks for the LLM â€” for that, we must track before and after chunks which we will place in two new metadata fields, `prechunk` and `postchunk`.\n",
    "\n",
    "Last, but not least â€” we may want to allow for connections between different documents. To support this we will add a `arxiv_id` field that identifies _this_ paper, and also a `references` field that includes other paper's `arxiv_id` that were referenced in _this_ paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmESimMfPAJm"
   },
   "outputs": [],
   "source": [
    "arxiv_id = dataset[2][\"id\"]\n",
    "refs = list(dataset[2][\"references\"].values())\n",
    "\n",
    "metadata = []\n",
    "for i, s in enumerate(splits[:3]):\n",
    "    prechunk = \"\" if i == 0 else splits[i-1].content\n",
    "    postchunk = \"\" if i-1 == len(splits) else splits[i+1].content\n",
    "    metadata.append({\n",
    "        \"title\": title,\n",
    "        \"content\": s.content,\n",
    "        \"prechunk\": prechunk,\n",
    "        \"postchunk\": postchunk,\n",
    "        \"arxiv_id\": arxiv_id,\n",
    "        \"references\": refs\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2i8GstS5iRPa",
    "outputId": "821f5022-0c71-42b6-8426-3e739da3f6a9"
   },
   "outputs": [],
   "source": [
    "metadata[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyHu9pzeplmF"
   },
   "outputs": [],
   "source": [
    "from semantic_router.schema import DocumentSplit\n",
    "\n",
    "\n",
    "def build_metadata(doc: dict, doc_splits: list[DocumentSplit]):\n",
    "    # get document level metadata first\n",
    "    arxiv_id = doc[\"id\"]\n",
    "    title = doc[\"title\"]\n",
    "    refs = list(doc[\"references\"].values())\n",
    "    # init split level metadata list\n",
    "    metadata = []\n",
    "    for i, split in enumerate(doc_splits):\n",
    "        # get neighboring chunks\n",
    "        prechunk_id = \"\" if i == 0 else f\"{arxiv_id}#{i-1}\"\n",
    "        postchunk_id = \"\" if i+1 == len(doc_splits) else f\"{arxiv_id}#{i+1}\"\n",
    "        # create dict and append to metadata list\n",
    "        metadata.append({\n",
    "            \"id\": f\"{arxiv_id}#{i}\",\n",
    "            \"title\": title,\n",
    "            \"content\": split.content,\n",
    "            \"prechunk_id\": prechunk_id,\n",
    "            \"postchunk_id\": postchunk_id,\n",
    "            \"arxiv_id\": arxiv_id,\n",
    "            \"references\": refs\n",
    "        })\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wjL0TPsssYe"
   },
   "outputs": [],
   "source": [
    "metadata = build_metadata(\n",
    "    doc=dataset[2],\n",
    "    doc_splits=splits[:3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5kNtNAFUg7U",
    "outputId": "36f32c80-5769-4771-f6f2-2c8fd716bc69"
   },
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C617bM9qS2xK"
   },
   "source": [
    "When feeding this structure into our LLM we will be able to tweak the exact format, how much info we provide, and how we handle connected documents â€” but by having this info here we can quickly iterate on the later generative steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWj-VTk_VyT-"
   },
   "source": [
    "## Implementation and Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNwdQEHLVyja"
   },
   "source": [
    "So far, we've seen how to process our data but we still haven't process our full dataset, nor have we begun embedding and storing our data. Now, we will do that.\n",
    "\n",
    "To begin, we will setup a Pinecone index where we'll be storing everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVPWnrtXWIiT"
   },
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\") or getpass(\"Pinecone API key: \")\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jth5pDgVnXiz"
   },
   "source": [
    "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/guides/projects/understanding-projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xb5dLaTJneZU"
   },
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-west-2\"  # us-east-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOQgPEM9ngHf"
   },
   "source": [
    "Before creating an index, we need the dimensionality of our OpenAI embedding model, we get this by embedding an example and measuring the vector dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eSV451gnknI",
    "outputId": "90717b3e-1c81-4c6f-d39f-4c69cee5cdfc"
   },
   "outputs": [],
   "source": [
    "dims = len(encoder([\"some random text\"])[0])\n",
    "dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64IXlHeopEJs"
   },
   "source": [
    "Now we create the index using our embedding dimensionality, and a metric also compatible with the model (this can be either cosine or dotproduct). We also pass our spec to index initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtGIlMlepGVv",
    "outputId": "57b8be0f-c226-43a2-adfa-5264a6d3e6e6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"better-rag-chunking\"\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=dims,  # dimensionality of embed 3\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1hiTcx1pSeX"
   },
   "source": [
    "# Populating our Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsAqEXd7pRUJ"
   },
   "source": [
    "Now our knowledge base is ready to be populated with our data. We will use the embed helper function to embed our documents and then add them to our index.\n",
    "\n",
    "We will also include metadata from each record in the format we developed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhrWkJPNKzS8"
   },
   "source": [
    "---\n",
    "\n",
    "_**Note**: You can find a prechunked version of the dataset [here](https://huggingface.co/datasets/jamescalam/ai-arxiv2-semantic-chunks) â€” you can use this to save time and money on chunking by loading from HF datasets with:_\n",
    "\n",
    "```python\n",
    "dataset = load_dataset(\n",
    "    \"jamescalam/ai-arxiv2-semantic-chunks\",\n",
    "    split=\"train[:10000]\"\n",
    ")\n",
    "```\n",
    "\n",
    "_You can also reduce the dataset size via the `split` parameter, to use only the first 10K rows we write `split=\"train[:10000]\"`._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388,
     "referenced_widgets": [
      "1b939a26a13048ec8de82b3fc683dc20",
      "81fccf2016cc46939a5deeab26e73b10",
      "27506fadebbe4299a1383036a97e08f4",
      "0f0f4675c4c64b8a815dd7e030d9f270",
      "3b3eebca882348abac9b09f54fee1479",
      "33b9b71edc724a0bbc3e6561e94ecb00",
      "d826a74734234f4ca44cc14a2e899d0e",
      "12a6c131a5f84d52853c29b138697e6d",
      "9721000803954878b2d9b5a585a33297",
      "97e38dbc89094de6991f9c09e28cb0e2",
      "eba9a22185df4ebbb1873a8a48b4c68d"
     ]
    },
    "id": "vjvdCEBgpaEe",
    "outputId": "022772b3-90e6-499d-80d6-33cae548f298"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# easier to work with dataset as pandas dataframe\n",
    "data = dataset.to_pandas().iloc[:10000]\n",
    "# store dataset *without* embeddings here\n",
    "full_dataset = []\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# adjust splitter to not display stats and visuals\n",
    "splitter.enable_statistics = False\n",
    "splitter.plot_splits = False\n",
    "\n",
    "for doc in tqdm(dataset):\n",
    "    # create splits\n",
    "    splits = splitter([doc[\"content\"]])\n",
    "    # create IDs and metadata for all splits in doc\n",
    "    metadata = build_metadata(doc=doc, doc_splits=splits)\n",
    "    for i in range(0, len(splits), batch_size):\n",
    "        i_end = min(len(splits), i+batch_size)\n",
    "        # get batch of data\n",
    "        metadata_batch = metadata[i:i_end]\n",
    "        full_dataset.extend(metadata_batch)\n",
    "        # generate unique ids for each chunk\n",
    "        ids = [m[\"id\"] for m in metadata_batch]\n",
    "        # get text content to embed\n",
    "        content = [\n",
    "            build_chunk(\n",
    "                title=x[\"title\"], content=x[\"content\"]\n",
    "            ) for x in metadata_batch\n",
    "        ]\n",
    "        # embed text\n",
    "        embeds = encoder(content)\n",
    "        # add to Pinecone\n",
    "        index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5lz-aDPKhS_"
   },
   "source": [
    "Now that we have our chunks stored we can go ahead and begin querying against them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGY0faNPKpkW"
   },
   "outputs": [],
   "source": [
    "def query(text: str):\n",
    "    xq = encoder([text])[0]\n",
    "    matches = index.query(\n",
    "        vector=xq,\n",
    "        top_k=3,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    chunks = []\n",
    "    for m in matches[\"matches\"]:\n",
    "        content = m[\"metadata\"][\"content\"]\n",
    "        title = m[\"metadata\"][\"title\"]\n",
    "        pre = m[\"metadata\"][\"prechunk_id\"]\n",
    "        post = m[\"metadata\"][\"postchunk_id\"]\n",
    "        other_chunks = index.fetch(ids=[pre, post])[\"vectors\"]\n",
    "        prechunk = other_chunks[pre][\"metadata\"][\"content\"]\n",
    "        postchunk = other_chunks[post][\"metadata\"][\"content\"]\n",
    "        chunk = f\"\"\"# {title}\n",
    "\n",
    "        {prechunk[-400:]}\n",
    "        {content}\n",
    "        {postchunk[:400]}\"\"\"\n",
    "        chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3sNHFgkoToo",
    "outputId": "edfc245b-630c-4471-bfdc-fb47189ff404"
   },
   "outputs": [],
   "source": [
    "query(\"what are large language models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZiQptSVe6FE"
   },
   "source": [
    "Once finished, you can delete your Pinecone index to save resources â€” _**but careful, this cannot be recovered without rerunning the index process!**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdH1hvDIfGe3",
    "outputId": "e84c6ec8-8d3e-4ffd-f254-000987344dae"
   },
   "outputs": [],
   "source": [
    "answer = input(\"Type 'y' to confirm deletion of the index...\\n>> \")\n",
    "if answer == \"y\":\n",
    "    pc.delete_index(index_name)\n",
    "    print(\"Index Deleted!\")\n",
    "else:\n",
    "    print(\"Deletion Cancelled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1xPPyqwKf5V"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAgoL90Yfjov"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f0f4675c4c64b8a815dd7e030d9f270": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_97e38dbc89094de6991f9c09e28cb0e2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_eba9a22185df4ebbb1873a8a48b4c68d",
      "value": "â€‡15/2673â€‡[00:58&lt;2:31:36,â€‡â€‡3.42s/it]"
     }
    },
    "12a6c131a5f84d52853c29b138697e6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b939a26a13048ec8de82b3fc683dc20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_81fccf2016cc46939a5deeab26e73b10",
       "IPY_MODEL_27506fadebbe4299a1383036a97e08f4",
       "IPY_MODEL_0f0f4675c4c64b8a815dd7e030d9f270"
      ],
      "layout": "IPY_MODEL_3b3eebca882348abac9b09f54fee1479"
     }
    },
    "27506fadebbe4299a1383036a97e08f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12a6c131a5f84d52853c29b138697e6d",
      "max": 2673,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9721000803954878b2d9b5a585a33297",
      "value": 15
     }
    },
    "33b9b71edc724a0bbc3e6561e94ecb00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b3eebca882348abac9b09f54fee1479": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81fccf2016cc46939a5deeab26e73b10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33b9b71edc724a0bbc3e6561e94ecb00",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d826a74734234f4ca44cc14a2e899d0e",
      "value": "â€‡â€‡1%"
     }
    },
    "9721000803954878b2d9b5a585a33297": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "97e38dbc89094de6991f9c09e28cb0e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d826a74734234f4ca44cc14a2e899d0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eba9a22185df4ebbb1873a8a48b4c68d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
