{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed940536-49f3-4269-8aca-77e1b0ac2625",
   "metadata": {},
   "source": [
    "# Learning Chroma Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57faf9cc-a7f1-46d3-9f70-f5f2a2d6d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BRAD import brad\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c33c6fe-a6db-4483-9f0a-e8dd067f48c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpic/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the database\n",
    "persist_directory = '/nfs/turbo/umms-indikar/shared/projects/RAG/databases/DigitalLibrary-10-June-2024/'\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')\n",
    "db_name = \"DigitalLibrary\"\n",
    "_client_settings = chromadb.PersistentClient(path=(persist_directory + db_name))\n",
    "vectordb = Chroma(persist_directory=persist_directory,\n",
    "                  embedding_function=embeddings_model,\n",
    "                  client=_client_settings,\n",
    "                  collection_name=db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0420ac3-0d5f-448c-84e4-81cd14ea600c",
   "metadata": {},
   "source": [
    "# Add new texts to a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80568368-0215-4d96-a2ab-4a5ea35ab9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a31a18-7156-42b0-9e99-780a4869ed8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ef2965-5172-409d-9754-b2e4c0079c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89ea276c-de3b-4315-abb9-d7708d2cfaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Chroma in module langchain_community.vectorstores.chroma object:\n",
      "\n",
      "class Chroma(langchain_core.vectorstores.VectorStore)\n",
      " |  Chroma(collection_name: 'str' = 'langchain', embedding_function: 'Optional[Embeddings]' = None, persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, collection_metadata: 'Optional[Dict]' = None, client: 'Optional[chromadb.Client]' = None, relevance_score_fn: 'Optional[Callable[[float], float]]' = None) -> 'None'\n",
      " |  \n",
      " |  `ChromaDB` vector store.\n",
      " |  \n",
      " |  To use, you should have the ``chromadb`` python package installed.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |              from langchain_community.vectorstores import Chroma\n",
      " |              from langchain_community.embeddings.openai import OpenAIEmbeddings\n",
      " |  \n",
      " |              embeddings = OpenAIEmbeddings()\n",
      " |              vectorstore = Chroma(\"langchain_store\", embeddings)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Chroma\n",
      " |      langchain_core.vectorstores.VectorStore\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, collection_name: 'str' = 'langchain', embedding_function: 'Optional[Embeddings]' = None, persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, collection_metadata: 'Optional[Dict]' = None, client: 'Optional[chromadb.Client]' = None, relevance_score_fn: 'Optional[Callable[[float], float]]' = None) -> 'None'\n",
      " |      Initialize with a Chroma client.\n",
      " |  \n",
      " |  __len__(self) -> 'int'\n",
      " |      Count the number of documents in the collection.\n",
      " |  \n",
      " |  add_images(self, uris: 'List[str]', metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more images through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          uris List[str]: File path to the image.\n",
      " |          metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n",
      " |          ids (Optional[List[str]], optional): Optional list of IDs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added images.\n",
      " |  \n",
      " |  add_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more texts through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts (Iterable[str]): Texts to add to the vectorstore.\n",
      " |          metadatas (Optional[List[dict]], optional): Optional list of metadatas.\n",
      " |          ids (Optional[List[str]], optional): Optional list of IDs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  delete(self, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'None'\n",
      " |      Delete by vector IDs.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: List of ids to delete.\n",
      " |  \n",
      " |  delete_collection(self) -> 'None'\n",
      " |      Delete the collection.\n",
      " |  \n",
      " |  encode_image(self, uri: 'str') -> 'str'\n",
      " |      Get base64 string from image URI.\n",
      " |  \n",
      " |  get(self, ids: 'Optional[OneOrMany[ID]]' = None, where: 'Optional[Where]' = None, limit: 'Optional[int]' = None, offset: 'Optional[int]' = None, where_document: 'Optional[WhereDocument]' = None, include: 'Optional[List[str]]' = None) -> 'Dict[str, Any]'\n",
      " |      Gets the collection.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: The ids of the embeddings to get. Optional.\n",
      " |          where: A Where type dict used to filter results by.\n",
      " |                 E.g. `{\"color\" : \"red\", \"price\": 4.20}`. Optional.\n",
      " |          limit: The number of documents to return. Optional.\n",
      " |          offset: The offset to start returning results from.\n",
      " |                  Useful for paging results with limit. Optional.\n",
      " |          where_document: A WhereDocument type dict used to filter by the documents.\n",
      " |                          E.g. `{$contains: \"hello\"}`. Optional.\n",
      " |          include: A list of what to include in the results.\n",
      " |                   Can contain `\"embeddings\"`, `\"metadatas\"`, `\"documents\"`.\n",
      " |                   Ids are always included.\n",
      " |                   Defaults to `[\"metadatas\", \"documents\"]`. Optional.\n",
      " |  \n",
      " |  max_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  max_marginal_relevance_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          embedding: Embedding to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  persist(self) -> 'None'\n",
      " |      [*Deprecated*] Persist the collection.\n",
      " |      \n",
      " |      This can be used to explicitly persist the data to disk.\n",
      " |      It will also be called automatically when the object is destroyed.\n",
      " |      \n",
      " |      Since Chroma 0.4.x the manual persistence method is no longer\n",
      " |      supported as docs are automatically persisted.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: langchain-community==0.1.17\n",
      " |         Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      " |  \n",
      " |  similarity_search(self, query: 'str', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Run similarity search with Chroma.\n",
      " |      \n",
      " |      Args:\n",
      " |          query (str): Query text to search for.\n",
      " |          k (int): Number of results to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[Document]: List of documents most similar to the query text.\n",
      " |  \n",
      " |  similarity_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to embedding vector.\n",
      " |      Args:\n",
      " |          embedding (List[float]): Embedding to look up documents similar to.\n",
      " |          k (int): Number of Documents to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      Returns:\n",
      " |          List of Documents most similar to the query vector.\n",
      " |  \n",
      " |  similarity_search_by_vector_with_relevance_scores(self, embedding: 'List[float]', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs most similar to embedding vector and similarity score.\n",
      " |      \n",
      " |      Args:\n",
      " |          embedding (List[float]): Embedding to look up documents similar to.\n",
      " |          k (int): Number of Documents to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[Tuple[Document, float]]: List of documents most similar to\n",
      " |          the query text and cosine distance in float for each.\n",
      " |          Lower score represents more similarity.\n",
      " |  \n",
      " |  similarity_search_with_score(self, query: 'str', k: 'int' = 4, filter: 'Optional[Dict[str, str]]' = None, where_document: 'Optional[Dict[str, str]]' = None, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Run similarity search with Chroma with distance.\n",
      " |      \n",
      " |      Args:\n",
      " |          query (str): Query text to search for.\n",
      " |          k (int): Number of results to return. Defaults to 4.\n",
      " |          filter (Optional[Dict[str, str]]): Filter by metadata. Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[Tuple[Document, float]]: List of documents most similar to\n",
      " |          the query text and cosine distance in float for each.\n",
      " |          Lower score represents more similarity.\n",
      " |  \n",
      " |  update_document(self, document_id: 'str', document: 'Document') -> 'None'\n",
      " |      Update a document in the collection.\n",
      " |      \n",
      " |      Args:\n",
      " |          document_id (str): ID of the document to update.\n",
      " |          document (Document): Document to update.\n",
      " |  \n",
      " |  update_documents(self, ids: 'List[str]', documents: 'List[Document]') -> 'None'\n",
      " |      Update a document in the collection.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids (List[str]): List of ids of the document to update.\n",
      " |          documents (List[Document]): List of documents to update.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_documents(documents: 'List[Document]', embedding: 'Optional[Embeddings]' = None, ids: 'Optional[List[str]]' = None, collection_name: 'str' = 'langchain', persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, client: 'Optional[chromadb.Client]' = None, collection_metadata: 'Optional[Dict]' = None, **kwargs: 'Any') -> 'Chroma' from abc.ABCMeta\n",
      " |      Create a Chroma vectorstore from a list of documents.\n",
      " |      \n",
      " |      If a persist_directory is specified, the collection will be persisted there.\n",
      " |      Otherwise, the data will be ephemeral in-memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          collection_name (str): Name of the collection to create.\n",
      " |          persist_directory (Optional[str]): Directory to persist the collection.\n",
      " |          ids (Optional[List[str]]): List of document IDs. Defaults to None.\n",
      " |          documents (List[Document]): List of documents to add to the vectorstore.\n",
      " |          embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n",
      " |          client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n",
      " |          collection_metadata (Optional[Dict]): Collection configurations.\n",
      " |                                                Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Chroma: Chroma vectorstore.\n",
      " |  \n",
      " |  from_texts(texts: 'List[str]', embedding: 'Optional[Embeddings]' = None, metadatas: 'Optional[List[dict]]' = None, ids: 'Optional[List[str]]' = None, collection_name: 'str' = 'langchain', persist_directory: 'Optional[str]' = None, client_settings: 'Optional[chromadb.config.Settings]' = None, client: 'Optional[chromadb.Client]' = None, collection_metadata: 'Optional[Dict]' = None, **kwargs: 'Any') -> 'Chroma' from abc.ABCMeta\n",
      " |      Create a Chroma vectorstore from a raw documents.\n",
      " |      \n",
      " |      If a persist_directory is specified, the collection will be persisted there.\n",
      " |      Otherwise, the data will be ephemeral in-memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          texts (List[str]): List of texts to add to the collection.\n",
      " |          collection_name (str): Name of the collection to create.\n",
      " |          persist_directory (Optional[str]): Directory to persist the collection.\n",
      " |          embedding (Optional[Embeddings]): Embedding function. Defaults to None.\n",
      " |          metadatas (Optional[List[dict]]): List of metadatas. Defaults to None.\n",
      " |          ids (Optional[List[str]]): List of document IDs. Defaults to None.\n",
      " |          client_settings (Optional[chromadb.config.Settings]): Chroma client settings\n",
      " |          collection_metadata (Optional[Dict]): Collection configurations.\n",
      " |                                                Defaults to None.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Chroma: Chroma vectorstore.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  embeddings\n",
      " |      Access the query embedding object if available.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  async aadd_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more documents through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (List[Document]: Documents to add to the vectorstore.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  async aadd_texts(self, texts: 'Iterable[str]', metadatas: 'Optional[List[dict]]' = None, **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more texts through the embeddings and add to the vectorstore.\n",
      " |  \n",
      " |  add_documents(self, documents: 'List[Document]', **kwargs: 'Any') -> 'List[str]'\n",
      " |      Run more documents through the embeddings and add to the vectorstore.\n",
      " |      \n",
      " |      Args:\n",
      " |          documents (List[Document]: Documents to add to the vectorstore.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List[str]: List of IDs of the added texts.\n",
      " |  \n",
      " |  async adelete(self, ids: 'Optional[List[str]]' = None, **kwargs: 'Any') -> 'Optional[bool]'\n",
      " |      Delete by vector ID or other criteria.\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: List of ids to delete.\n",
      " |          **kwargs: Other keyword arguments that subclasses might use.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Optional[bool]: True if deletion is successful,\n",
      " |          False otherwise, None if not implemented.\n",
      " |  \n",
      " |  async amax_marginal_relevance_search(self, query: 'str', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |      \n",
      " |      Maximal marginal relevance optimizes for similarity to query AND diversity\n",
      " |      among selected documents.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: Text to look up documents similar to.\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          fetch_k: Number of Documents to fetch to pass to MMR algorithm.\n",
      " |          lambda_mult: Number between 0 and 1 that determines the degree\n",
      " |                      of diversity among the results with 0 corresponding\n",
      " |                      to maximum diversity and 1 to minimum diversity.\n",
      " |                      Defaults to 0.5.\n",
      " |      Returns:\n",
      " |          List of Documents selected by maximal marginal relevance.\n",
      " |  \n",
      " |  async amax_marginal_relevance_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, fetch_k: 'int' = 20, lambda_mult: 'float' = 0.5, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs selected using the maximal marginal relevance.\n",
      " |  \n",
      " |  as_retriever(self, **kwargs: 'Any') -> 'VectorStoreRetriever'\n",
      " |      Return VectorStoreRetriever initialized from this VectorStore.\n",
      " |      \n",
      " |      Args:\n",
      " |          search_type (Optional[str]): Defines the type of search that\n",
      " |              the Retriever should perform.\n",
      " |              Can be \"similarity\" (default), \"mmr\", or\n",
      " |              \"similarity_score_threshold\".\n",
      " |          search_kwargs (Optional[Dict]): Keyword arguments to pass to the\n",
      " |              search function. Can include things like:\n",
      " |                  k: Amount of documents to return (Default: 4)\n",
      " |                  score_threshold: Minimum relevance threshold\n",
      " |                      for similarity_score_threshold\n",
      " |                  fetch_k: Amount of documents to pass to MMR algorithm (Default: 20)\n",
      " |                  lambda_mult: Diversity of results returned by MMR;\n",
      " |                      1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
      " |                  filter: Filter by document metadata\n",
      " |      \n",
      " |      Returns:\n",
      " |          VectorStoreRetriever: Retriever class for VectorStore.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          # Retrieve more documents with higher diversity\n",
      " |          # Useful if your dataset has many similar documents\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"mmr\",\n",
      " |              search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
      " |          )\n",
      " |      \n",
      " |          # Fetch more documents for the MMR algorithm to consider\n",
      " |          # But only return the top 5\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"mmr\",\n",
      " |              search_kwargs={'k': 5, 'fetch_k': 50}\n",
      " |          )\n",
      " |      \n",
      " |          # Only retrieve documents that have a relevance score\n",
      " |          # Above a certain threshold\n",
      " |          docsearch.as_retriever(\n",
      " |              search_type=\"similarity_score_threshold\",\n",
      " |              search_kwargs={'score_threshold': 0.8}\n",
      " |          )\n",
      " |      \n",
      " |          # Only get the single most similar document from the dataset\n",
      " |          docsearch.as_retriever(search_kwargs={'k': 1})\n",
      " |      \n",
      " |          # Use a filter to only retrieve documents from a specific paper\n",
      " |          docsearch.as_retriever(\n",
      " |              search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
      " |          )\n",
      " |  \n",
      " |  async asearch(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query using specified search type.\n",
      " |  \n",
      " |  async asimilarity_search(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query.\n",
      " |  \n",
      " |  async asimilarity_search_by_vector(self, embedding: 'List[float]', k: 'int' = 4, **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to embedding vector.\n",
      " |  \n",
      " |  async asimilarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs and relevance scores in the range [0, 1], asynchronously.\n",
      " |      \n",
      " |      0 is dissimilar, 1 is most similar.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: input text\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
      " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
      " |                  filter the resulting set of retrieved docs\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Tuples of (doc, similarity_score)\n",
      " |  \n",
      " |  async asimilarity_search_with_score(self, *args: 'Any', **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Run similarity search with distance asynchronously.\n",
      " |  \n",
      " |  search(self, query: 'str', search_type: 'str', **kwargs: 'Any') -> 'List[Document]'\n",
      " |      Return docs most similar to query using specified search type.\n",
      " |  \n",
      " |  similarity_search_with_relevance_scores(self, query: 'str', k: 'int' = 4, **kwargs: 'Any') -> 'List[Tuple[Document, float]]'\n",
      " |      Return docs and relevance scores in the range [0, 1].\n",
      " |      \n",
      " |      0 is dissimilar, 1 is most similar.\n",
      " |      \n",
      " |      Args:\n",
      " |          query: input text\n",
      " |          k: Number of Documents to return. Defaults to 4.\n",
      " |          **kwargs: kwargs to be passed to similarity search. Should include:\n",
      " |              score_threshold: Optional, a floating point value between 0 to 1 to\n",
      " |                  filter the resulting set of retrieved docs\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of Tuples of (doc, similarity_score)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  async afrom_documents(documents: 'List[Document]', embedding: 'Embeddings', **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from documents and embeddings.\n",
      " |  \n",
      " |  async afrom_texts(texts: 'List[str]', embedding: 'Embeddings', metadatas: 'Optional[List[dict]]' = None, **kwargs: 'Any') -> 'VST' from abc.ABCMeta\n",
      " |      Return VectorStore initialized from texts and embeddings.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.vectorstores.VectorStore:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(vectordb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
